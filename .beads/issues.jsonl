{"id":"sensei-07q","title":"Implement llms.txt parser","description":"Create parse_llms_txt() function in sensei/tome/parser.py. Extract URLs from llms.txt markdown format (links in H2 sections). Return list of URLs.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:03.962606-05:00","updated_at":"2025-11-27T11:26:20.975411-05:00","closed_at":"2025-11-27T11:26:20.975411-05:00","dependencies":[{"issue_id":"sensei-07q","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:03.964273-05:00","created_by":"daemon"},{"issue_id":"sensei-07q","depends_on_id":"sensei-au8","type":"blocks","created_at":"2025-11-27T11:16:18.327806-05:00","created_by":"daemon"}]}
{"id":"sensei-08s","title":"Tome: llms.txt-focused documentation retrieval for Sensei","description":"Add llms.txt-focused documentation retrieval to Sensei, complementing Context7 for general library lookups.\n\n## Overview\n\nTome is Sensei's llms.txt specialist. While Context7 handles general documentation from any source with its complex pipeline (parse→enrich→vectorize→rerank→cache), Tome focuses exclusively on llms.txt/llms-full.txt sources with a simpler, fresher approach.\n\n## Value Proposition\n\n- **Real-time**: No 10-15 day sync lag like Context7\n- **Authoritative**: llms.txt files are curated by library authors\n- **Simple**: No vector DB, just PostgreSQL FTS\n- **llms.txt-first**: Respects the standard's structure (INDEX/FULL/linked docs)\n\n## API Design\n\n### `tome_get(domain: str, path: str) -\u003e str`\nGet document content with sentinel values:\n- `\"INDEX\"` → `/llms.txt` (table of contents)\n- `\"FULL\"` → `/llms-full.txt` (complete docs)\n- Any path → specific document\n\n### `tome_search(domain: str, query: str, paths: list[str]) -\u003e list[SearchResult]`\nFull-text search within a domain, optionally scoped to path prefixes.\n\n## Implementation Components\n\n1. **Database**: Add PostgreSQL FTS (tsvector + GIN index)\n2. **Crawler**: Also fetch llms-full.txt\n3. **Storage**: Add search_documents_fts()\n4. **Service**: tome_get(), tome_search()\n5. **MCP Server**: Expose tools to agent\n6. **Integration**: Wire into Sensei agent\n\n## When to Use\n\n- **Tome**: Domains you've ingested, want fresh/authoritative llms.txt content\n- **Context7**: General library lookups, unknown domains, semantic search","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-01T08:39:31.779528-05:00","updated_at":"2025-12-01T09:03:15.06944-05:00","closed_at":"2025-12-01T09:03:15.06944-05:00","labels":["epic","tome"]}
{"id":"sensei-0ih","title":"Tome: Create SaveResult enum for storage return type","description":"Replace bool return from save_tome_document() with SaveResult enum (INSERTED, UPDATED, SKIPPED). This upstream fix enables proper tracking of documents_updated in IngestResult.","design":"```python\nclass SaveResult(Enum):\n    INSERTED = \"inserted\"\n    UPDATED = \"updated\"\n    SKIPPED = \"skipped\"  # content unchanged\n```\n\nCrawler can then:\n```python\nmatch await save_tome_document(...):\n    case SaveResult.INSERTED: result.documents_added += 1\n    case SaveResult.UPDATED: result.documents_updated += 1\n    case SaveResult.SKIPPED: result.documents_skipped += 1\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T13:28:47.061712-05:00","updated_at":"2025-11-27T13:42:03.542461-05:00","closed_at":"2025-11-27T13:42:03.542461-05:00","labels":["architecture","important","tome"]}
{"id":"sensei-0nt","title":"Make database init conditional (SQLite FTS5 vs Postgres ILIKE)","description":"Detect database type from URL. Skip FTS5 virtual tables on Postgres, use ILIKE for search instead.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.676313-05:00","updated_at":"2025-11-27T09:16:22.149689-05:00","closed_at":"2025-11-27T09:16:22.149689-05:00","dependencies":[{"issue_id":"sensei-0nt","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.6772-05:00","created_by":"daemon"}]}
{"id":"sensei-0uo","title":"Add kura script entry point to pyproject.toml","description":"Add kura = \"sensei.kura:main\" to [project.scripts] so users can run `uvx --from sensei kura` for stdio MCP.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.142133-05:00","updated_at":"2025-11-27T08:58:06.351994-05:00","closed_at":"2025-11-27T08:58:06.351994-05:00","dependencies":[{"issue_id":"sensei-0uo","depends_on_id":"sensei-fnh","type":"blocks","created_at":"2025-11-27T08:55:36.527213-05:00","created_by":"daemon"}]}
{"id":"sensei-0y5","title":"Create fly.toml configuration","description":"Create fly.toml with: app name, region (iad for US East near Neon), internal port 8080, volume mount at /data, health check endpoint, memory/CPU settings.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.791005-05:00","updated_at":"2025-11-30T05:57:26.643762-05:00","closed_at":"2025-11-30T05:57:26.643762-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-0y5","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.929291-05:00","created_by":"daemon"}]}
{"id":"sensei-10i","title":"Move flatten/save_tree from storage.py to crawler.py","description":"**Current state (storage.py:264-319):**\n`flatten` and `save_tree` are nested functions inside `save_sections` that do tree traversal:\n- Convert SectionData tree → flat Section models\n- Maintain parent_section_id relationships\n- Track position counters\n\n**Problem:** Tree traversal is business logic, not storage logic. Storage should receive flat data and store it.\n\n**Additional issue:** `flatten` function (lines 266-290) is defined but **never called** - only `save_tree` is used. This is dead code.\n\n**Fix:**\n1. Delete the dead `flatten` function\n2. Move `save_tree` logic to crawler.py as:\n```python\ndef flatten_section_tree(\n    root: SectionData, \n    document_id: UUID\n) -\u003e list[Section]:\n    \"\"\"Convert SectionData tree to flat list of Section models with parent relationships.\"\"\"\n    sections = []\n    position = [0]\n    \n    def walk(node: SectionData, parent_id: UUID | None):\n        if node.content or node.children:\n            section = Section(\n                document_id=document_id,\n                parent_section_id=parent_id,\n                heading=node.heading,\n                level=node.level,\n                content=node.content,\n                position=position[0],\n            )\n            position[0] += 1\n            sections.append(section)\n            for child in node.children:\n                walk(child, section.id)\n    \n    walk(root, None)\n    return sections\n```\n3. Storage just does bulk insert of the flat list","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.268775-05:00","updated_at":"2025-12-01T15:17:32.450562-05:00","closed_at":"2025-12-01T15:17:32.450562-05:00","dependencies":[{"issue_id":"sensei-10i","depends_on_id":"sensei-7af","type":"related","created_at":"2025-12-01T13:01:31.231896-05:00","created_by":"daemon"}]}
{"id":"sensei-11w","title":"Remove depth column from Document model","description":"The `depth` column on Document tracks crawl depth from llms.txt (0 = llms.txt itself, 1 = directly linked, etc.). \n\nThis is crawl metadata that doesn't belong on the Document model - it's an artifact of how we discovered the document, not a property of the document itself.\n\n**Changes:**\n- Remove `depth` column from Document model in models.py\n- Remove `depth` from DocumentContent type in types.py  \n- Update migration (sensei-gwq) to not include depth\n- Update crawler to not track/save depth\n\n**Note:** If we need crawl metadata in the future, it belongs in a separate crawl_history or document_source table, not on the document itself.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T10:01:02.296358-05:00","updated_at":"2025-12-01T13:01:45.601173-05:00","closed_at":"2025-12-01T13:01:45.601173-05:00","labels":["cleanup","database","tome"],"dependencies":[{"issue_id":"sensei-11w","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T10:01:02.298897-05:00","created_by":"daemon"}]}
{"id":"sensei-16ef","title":"Remove lazy import in `sensei/eval/metrics.py`","description":"`get_eval_model()` performs a function-level `import os`, violating the repo rule to never use lazy imports. This should be moved to module scope.","acceptance_criteria":"- No function-level imports remain in `sensei/eval/metrics.py`.\n- Import ordering/style matches repo conventions.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-11T20:42:48.878296-05:00","updated_at":"2025-12-12T11:41:39.141078-05:00","closed_at":"2025-12-12T11:41:39.141078-05:00"}
{"id":"sensei-198","title":"Add asyncpg dependency to pyproject.toml","description":"Add asyncpg for async PostgreSQL driver. Also add alembic as dependency.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.48923-05:00","updated_at":"2025-11-27T16:01:50.231857-05:00","closed_at":"2025-11-27T16:01:50.231857-05:00","labels":["database","dependencies"],"dependencies":[{"issue_id":"sensei-198","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:08.065958-05:00","created_by":"daemon"}]}
{"id":"sensei-1c5","title":"Remove init_db() and SQLite-specific code from storage.py","description":"Remove: init_db(), is_postgres(), FTS5 virtual table, SQLite triggers, SQLite search logic in search_queries(). Keep engine/session factory and all query functions.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.42918-05:00","updated_at":"2025-11-27T16:01:50.456783-05:00","closed_at":"2025-11-27T16:01:50.456783-05:00","labels":["cleanup","database"],"dependencies":[{"issue_id":"sensei-1c5","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.982994-05:00","created_by":"daemon"}]}
{"id":"sensei-1ha","title":"Create alembic.ini and alembic/env.py with async support","description":"Initialize Alembic with async engine config. Read DATABASE_URL from env. Import Base.metadata from sensei.database.models. Enable compare_type and compare_server_default.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.259893-05:00","updated_at":"2025-11-27T16:01:50.34949-05:00","closed_at":"2025-11-27T16:01:50.34949-05:00","labels":["database","migrations"],"dependencies":[{"issue_id":"sensei-1ha","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.851521-05:00","created_by":"daemon"}]}
{"id":"sensei-1j9","title":"Create docker-compose.yml for local PostgreSQL","description":"PostgreSQL 16 container with volume persistence. Port 5432, credentials sensei/sensei/sensei.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.375073-05:00","updated_at":"2025-11-27T16:01:50.298951-05:00","closed_at":"2025-11-27T16:01:50.298951-05:00","labels":["database","docker"],"dependencies":[{"issue_id":"sensei-1j9","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.937163-05:00","created_by":"daemon"}]}
{"id":"sensei-1lmy","title":"Simplify unified.py lifespan","description":"Remove ensure_db_ready() call from unified lifespan. Keep only dispose_engine().\n\n**File:** `sensei/unified.py`\n\n**Remove:**\n- Import of `ensure_db_ready` from database.local\n- Call to `await ensure_db_ready()` in lifespan\n\n**Keep:**\n- Import of `dispose_engine` from database.engine\n- Call to `await dispose_engine()` in finally block","design":"```python\n# Before\nfrom sensei.database.engine import dispose_engine\nfrom sensei.database.local import ensure_db_ready\n\n@asynccontextmanager\nasync def lifespan(server):\n    await ensure_db_ready()\n    try:\n        yield\n    finally:\n        await dispose_engine()\n\n# After\nfrom sensei.database.engine import dispose_engine\n\n@asynccontextmanager\nasync def lifespan(server):\n    try:\n        yield\n    finally:\n        await dispose_engine()\n```","acceptance_criteria":"- No import of ensure_db_ready\n- No call to ensure_db_ready\n- dispose_engine still called in finally\n- Unified server starts without DB initialization","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.907123-05:00","updated_at":"2025-12-21T09:57:21.758004-05:00","closed_at":"2025-12-21T09:57:21.758004-05:00","dependencies":[{"issue_id":"sensei-1lmy","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.910764-05:00","created_by":"daemon"},{"issue_id":"sensei-1lmy","depends_on_id":"sensei-zujl","type":"blocks","created_at":"2025-12-21T09:46:23.048605-05:00","created_by":"daemon"}]}
{"id":"sensei-1ntj","title":"Deepeval Integration for Quality Evaluation","description":"Integrate deepeval framework for automated quality evaluation of Sensei responses.\n\n**Goals:**\n1. Regression testing - Catch quality degradation when changing prompts, models, or tools\n2. Quality monitoring - Ongoing measurement of response quality\n\n**Approach: Tracing + Agentic Metrics**\n\nUse DeepEval's tracing system to automatically capture agent execution, then evaluate with agentic metrics designed for tool-using agents.\n\n**Key Design Decisions:**\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Test cases | Minimal Goldens (input only) | Tracing auto-populates actual_output, tools_called, retrieval_context |\n| Evaluation pattern | `evals_iterator()` | DeepEval's native pattern for traced agents |\n| Metrics | Agentic + G-Eval | Task Completion, Argument Correctness, Step Efficiency, custom criteria |\n| Agent instrumentation | `@observe` + `update_current_trace()` | Non-intrusive, only active during evaluation |\n\n**Metrics:**\n\n| Metric | What It Measures | Threshold |\n|--------|------------------|-----------|\n| Task Completion | Did agent accomplish the research task? | 0.7 |\n| Argument Correctness | Were tool arguments appropriate for the query? | 0.7 |\n| Step Efficiency | Was execution efficient (no unnecessary steps)? | 0.7 |\n| G-Eval: Doc Quality | Custom criteria for documentation responses | 0.7 |\n\n**NOT using:**\n- Tool Correctness (requires expected_tools in goldens)\n- Plan Adherence/Quality (requires explicit planning in agent)\n- RAG metrics (Faithfulness, AnswerRelevancy, ContextualRelevancy) - replaced by agentic metrics\n\n**Architecture:**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  1. Load Goldens from YAML (input only)                         │\n│     tests/eval/datasets/*.yaml → List[Golden]                   │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  2. evals_iterator() loops through Goldens                      │\n│     for golden in dataset.evals_iterator(metrics=[...]):        │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  3. Run Traced Agent                                            │\n│     @observe decorated handle_query()                           │\n│     update_current_trace(input, output, tools_called, context)  │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│  4. DeepEval Evaluates Trace                                    │\n│     Metrics analyze full execution trace automatically          │\n└─────────────────────────────────────────────────────────────────┘\n```","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-06T10:34:26.999439-05:00","updated_at":"2025-12-06T13:05:59.228617-05:00","closed_at":"2025-12-06T13:05:59.228617-05:00","labels":["eval","quality","testing"]}
{"id":"sensei-1qn","title":"Tome: Add rate limiting to crawler","description":"HttpCrawler configured with max_requests but no rate limiting. Could overwhelm target servers. Add max_requests_per_minute or min_request_delay.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-11-27T11:44:48.829289-05:00","updated_at":"2025-11-27T16:08:51.073521-05:00","closed_at":"2025-11-27T16:08:51.073521-05:00","labels":["minor","tome"]}
{"id":"sensei-225","title":"Tome: Validate domain parameter in ingest_domain()","description":"crawler.py:49 has no validation that domain parameter is actually a domain (not a full URL). User might pass https://react.dev instead of react.dev.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, validation happens automatically when constructing Domain(user_input).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:32.747283-05:00","updated_at":"2025-11-27T13:42:03.743659-05:00","closed_at":"2025-11-27T13:42:03.743659-05:00","labels":["important","tome"],"dependencies":[{"issue_id":"sensei-225","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.941895-05:00","created_by":"daemon"}]}
{"id":"sensei-271","title":"Review and clean up chunker.py code quality issues","description":"**Issues found in chunker.py:**\n\n1. **_start_line hack (line 262):**\n   ```python\n   section._start_line = start_line  # type: ignore[attr-defined]\n   ```\n   Setting an attribute on a dataclass that doesn't have it. This is a code smell - the type ignore is a red flag.\n\n2. **Confusing function relationships:**\n   - `_split_by_top_level_headings` calls `_split_by_headings_at_level(lines, min_level=None)`\n   - `_split_by_subheadings` calls `_split_by_headings_at_level(lines, min_level=parent_level + 1)`\n   - These wrapper functions add indirection without much clarity\n\n3. **Token counting is rough (line 52-54):**\n   ```python\n   return len(content.split())  # Just counts words\n   ```\n   Comment says \"roughly 1.3 tokens per word on average but we use 1:1 for simplicity\". This could under-estimate by 30%.\n\n4. **Algorithm complexity:**\n   The interplay between `chunk_markdown`, `_chunk_section`, `_split_by_top_level_headings`, `_split_by_subheadings`, and `_extract_intro` is hard to follow. Consider simplifying.\n\n5. **Potential issue with intro extraction:**\n   `_extract_intro` relies on `_start_line` attribute which is set via the hack in point 1.\n\n**Recommendations:**\n- Add `start_line: int | None = None` to SectionData dataclass properly\n- Consider flattening the function hierarchy\n- Document the algorithm more clearly\n- Consider if token counting accuracy matters for the use case","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T13:01:21.687116-05:00","updated_at":"2025-12-01T14:45:23.231728-05:00","closed_at":"2025-12-01T14:45:23.231728-05:00","dependencies":[{"issue_id":"sensei-271","depends_on_id":"sensei-7af","type":"related","created_at":"2025-12-01T13:01:31.273264-05:00","created_by":"daemon"}]}
{"id":"sensei-28m","title":"Deps.http_client typed as Any instead of httpx.AsyncClient","description":"deps.py:16 types http_client as Optional[Any] instead of proper type.\n\nProblems:\n1. Loses type safety\n2. tools/common.py:44-49 get_client() uses getattr patterns to work around it\n3. Caller must track whether client was created to close it\n\nFix:\n1. Type properly: `http_client: httpx.AsyncClient | None = None`\n2. Consider using httpx context manager pattern\n3. Or inject client at request start in middleware","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.599112-05:00","updated_at":"2025-11-30T09:22:39.625143-05:00","closed_at":"2025-11-30T09:22:39.625143-05:00","labels":["cleanup","types"]}
{"id":"sensei-28w","title":"Remove depth column from Document model - web is a graph, not a tree","description":"**Current state:**\n- `Document` model has `depth = Column(Integer, nullable=False, server_default=\"0\")` (models.py:101)\n- `save_document_metadata` accepts and saves `depth` (storage.py:191, 208, 223, 234)\n- Crawler passes depth (crawler.py:120)\n\n**Problem:** The web is a graph, not a tree. The same document could be reached at different depths depending on the crawl path. Depth is only relevant for controlling crawl behavior (how many links to follow), not for storage.\n\n**Fix:**\n1. Remove `depth` column from `Document` model (models.py:101)\n2. Remove `depth` parameter from `save_document_metadata` (storage.py)\n3. Create migration to drop the column\n4. Keep depth only in crawler.py for crawl control (already does this correctly)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:20.826228-05:00","updated_at":"2025-12-01T13:34:48.08205-05:00","closed_at":"2025-12-01T13:34:48.08205-05:00"}
{"id":"sensei-2fb","title":"Create sensei/paths.py - centralized path management","description":"Create a new module for centralized path detection and management.\n\n**File:** `sensei/paths.py`\n\n**Design decision:** Use `SENSEI_HOME` environment variable with `~/.sensei` default. No fragile cwd detection.\n\n**Functions to implement:**\n```python\nimport os\nfrom pathlib import Path\n\n\ndef get_sensei_home() -\u003e Path:\n    \"\"\"Get the sensei home directory.\n    \n    Priority:\n    1. SENSEI_HOME env var (explicit override)\n    2. ~/.sensei (default)\n    \n    For development, set SENSEI_HOME=.sensei in .env\n    \"\"\"\n    if env_home := os.environ.get(\"SENSEI_HOME\"):\n        return Path(env_home)\n    return Path.home() / \".sensei\"\n\n\ndef get_pgdata() -\u003e Path:\n    \"\"\"Get PostgreSQL data directory path.\"\"\"\n    return get_sensei_home() / \"pgdata\"\n\n\ndef get_pg_log() -\u003e Path:\n    \"\"\"Get PostgreSQL log file path.\"\"\"\n    return get_sensei_home() / \"pg.log\"\n\n\ndef get_scout_repos() -\u003e Path:\n    \"\"\"Get scout repository cache directory.\"\"\"\n    return get_sensei_home() / \"scout\" / \"repos\"\n\n\ndef get_local_database_url() -\u003e str:\n    \"\"\"Get connection URL for local PostgreSQL using Unix socket.\n    \n    This is the default database URL when DATABASE_URL is not set.\n    Uses Unix socket to avoid port conflicts with system PostgreSQL.\n    \"\"\"\n    return f\"postgresql+asyncpg:///sensei?host={get_pgdata()}\"\n```\n\n**Why this approach:**\n- Simple and predictable\n- No fragile cwd detection\n- Devs set `SENSEI_HOME=.sensei` in `.env` for local data\n- Production uses `~/.sensei` by default\n- Follows \"explicit over implicit\" principle\n\n**Also update:**\n1. `sensei/config.py` - add `sensei_home` field (for documentation/visibility, won't be used by paths.py to avoid import cycle)\n2. `.env` - add `SENSEI_HOME=.sensei` for development\n\n**Data flow:**\n```\npaths.py (reads SENSEI_HOME directly from os.environ)\n    │\n    └──► config.py (has sensei_home field for visibility)\n             │\n             └──► storage.py (uses settings.database_url)\n```","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:05.63198-05:00","updated_at":"2025-12-02T07:41:39.451537-05:00","closed_at":"2025-12-02T07:41:39.451537-05:00","dependencies":[{"issue_id":"sensei-2fb","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:05.633609-05:00","created_by":"daemon"}]}
{"id":"sensei-2fya","title":"Update test fixtures for new engine pattern","description":"After sensei-rcrf refactors to module-level engine with testing hooks, update the test fixtures in `conftest.py`.\n\n**Current** (monkey-patching globals):\n```python\nstorage._engine = test_engine\nstorage._async_session_local = async_sessionmaker(...)\n```\n\n**Target** (using testing hook):\n```python\n# Set DATABASE_URL before engine module loads (pytest early fixture or conftest.py top)\nos.environ[\"DATABASE_URL\"] = TEST_DATABASE_URL\n\n# Then in fixture, use the testing hook for session factory override\nfrom sensei.database.engine import set_test_session_factory, get_session_factory\n```\n\nThis sets up the foundation for transaction rollback isolation (sensei-ucwq).","acceptance_criteria":"- [ ] Set `DATABASE_URL` env var before engine module import (pytest conftest.py)\n- [ ] Remove monkey-patching of `storage._engine` and `storage._async_session_local`\n- [ ] Use `set_test_session_factory()` hook from engine.py\n- [ ] Verify all tests pass with new pattern\n- [ ] Keep existing isolation method (migrations + cleanup) for now; transaction rollback in sensei-ucwq","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T06:28:10.842389-05:00","updated_at":"2025-12-20T13:07:05.103858-05:00","closed_at":"2025-12-20T13:07:05.103858-05:00","dependencies":[{"issue_id":"sensei-2fya","depends_on_id":"sensei-rcrf","type":"blocks","created_at":"2025-12-20T06:28:10.844691-05:00","created_by":"daemon"}]}
{"id":"sensei-2i5c","title":"Add VercelAIAdapter endpoint to Python sensei backend","description":"Add /api/chat endpoint to the unified API server using PydanticAI's VercelAIAdapter. Streams all events (text, reasoning, tool calls, tool results) in AI SDK compatible format.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T18:30:56.678844-05:00","updated_at":"2025-12-17T18:31:11.782908-05:00","closed_at":"2025-12-17T18:31:11.782908-05:00","dependencies":[{"issue_id":"sensei-2i5c","depends_on_id":"sensei-5t3e","type":"parent-child","created_at":"2025-12-17T18:30:56.681871-05:00","created_by":"daemon"}]}
{"id":"sensei-2of2","title":"Update sensei/cli.py docstring example","description":"Update the usage example in cli.py docstring to reflect the new idiomatic pattern where main() is defined in server.py and __main__.py just imports it.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-02T12:34:44.952655-05:00","updated_at":"2025-12-02T12:38:34.962001-05:00","closed_at":"2025-12-02T12:38:34.962001-05:00"}
{"id":"sensei-2oup","title":"Update paths.py for SENSEI_SCOUT_CACHE_DIR","description":"Update get_scout_repos() to check SENSEI_SCOUT_CACHE_DIR env var first, falling back to ~/.sensei/scout/repos","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T16:06:44.338905-05:00","updated_at":"2025-12-17T16:09:37.837078-05:00","closed_at":"2025-12-17T16:09:37.837078-05:00","dependencies":[{"issue_id":"sensei-2oup","depends_on_id":"sensei-xbo9","type":"parent-child","created_at":"2025-12-17T16:06:44.340921-05:00","created_by":"daemon"}]}
{"id":"sensei-2p9l","title":"Standardize naming: output + messages consistently","description":"Rename `markdown` → `output` for consistency across codebase.\n\n**Changes:**\n- `QueryResult.markdown` → `QueryResult.output`\n- `QueryResponse.markdown` → `QueryResponse.output`\n- Update core.py, api/__init__.py, server.py references\n\n**Breaking API change:** `/query` response field changes from `markdown` to `output`.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-06T10:51:38.293412-05:00","updated_at":"2025-12-12T11:53:45.614835-05:00","closed_at":"2025-12-12T11:53:45.614835-05:00","labels":["naming","refactor"]}
{"id":"sensei-2q3g","title":"Standardize naming: output and history","description":"Inconsistent naming across codebase for agent results:\n\n**Current state:**\n- LLM response called `output` in some places, `markdown` in others\n- Message history called `messages` everywhere\n\n**Target state:**\n- `output` - The LLM's final response (consistently, not `markdown`)\n- `history` - The list of messages/tool calls (not `messages`)\n\n**Scope:**\n- QueryResult fields\n- core.py variables\n- storage layer\n- Database column (if worth the migration)\n- Related types (CacheHit, etc.)\n\n**Why:** Consistency reduces cognitive load. `history` better conveys \"sequence of events\" vs `messages` which sounds like chat messages.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-06T10:50:53.258027-05:00","updated_at":"2025-12-12T11:50:10.23664-05:00","closed_at":"2025-12-12T11:50:10.23664-05:00","labels":["naming","refactor"]}
{"id":"sensei-2r4","title":"Fix module-level database initialization in storage.py","description":"storage.py:24-28 creates engine at module import time, causing test failures when DATABASE_URL is empty. Use lazy initialization or context-based engine creation.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:29.394024-05:00","updated_at":"2025-11-27T16:01:55.594075-05:00","closed_at":"2025-11-27T16:01:55.594075-05:00","labels":["database","important"]}
{"id":"sensei-2vw","title":"Remove tools/cache.py backwards-compat re-export module","description":"sensei/tools/cache.py is just a backwards-compatibility re-export from kura.tools:\n\n```python\nfrom sensei.kura.tools import _compute_age_days, get_cached_response, search_cache\n__all__ = [\"search_cache\", \"get_cached_response\", \"_compute_age_days\"]\n```\n\nOnly used by tests:\n- tests/test_cache_integration.py:11\n- tests/test_cache.py:66, 85, 109, 127\n\nChanges needed:\n1. Update test imports from `sensei.tools.cache` → `sensei.kura.tools`\n2. Delete sensei/tools/cache.py","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:16:36.907406-05:00","updated_at":"2025-11-30T09:17:09.110672-05:00","closed_at":"2025-11-30T09:17:09.110672-05:00","labels":["cleanup"]}
{"id":"sensei-2ym","title":"Timezone coercion logic duplicated across modules","description":"Multiple places handle timezone coercion differently:\n- kura/tools.py:19-30 (_compute_age_days)\n- storage.py:173-179 (search_queries)\n\nBoth do:\n```python\nif created_at.tzinfo is None:\n    created_at = created_at.replace(tzinfo=UTC)\n```\n\nFix options:\n1. Centralize in _compute_age_days() and use everywhere\n2. Ensure DB always returns timezone-aware datetimes (server_default with timezone)\n3. Add utility function in types.py for timezone normalization","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.5016-05:00","updated_at":"2025-11-30T09:19:19.825069-05:00","closed_at":"2025-11-30T09:19:19.825069-05:00","labels":["cleanup","database"]}
{"id":"sensei-32xv","title":"Add main() function to sensei/tome/server.py","description":"Move the main() entry point from __main__.py to server.py where the tome FastMCP server is defined. This follows the idiomatic pattern used by pytest/uvicorn.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:33:57.875007-05:00","updated_at":"2025-12-02T12:36:37.235312-05:00","closed_at":"2025-12-02T12:36:37.235312-05:00"}
{"id":"sensei-35k","title":"Improve MCP tool descriptions in server files","description":"Improve the docstrings/descriptions for each MCP tool in their respective server files (scout/server.py, kura/server.py, tools/tavily.py, tools/context7.py). These descriptions are what Sensei sees via MCP, so they should be high-quality and guide proper usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T22:15:09.963837-05:00","updated_at":"2025-12-01T07:09:47.855866-05:00","closed_at":"2025-12-01T07:09:47.855866-05:00"}
{"id":"sensei-37t","title":"Consolidate get_section_subtree_by_heading to single query with JOIN","description":"**Current state (storage.py:360-420):**\nTwo separate queries:\n```python\n# Query 1: Find active document\ndoc_result = await session.execute(\n    select(Document).where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n)\ndoc = doc_result.scalar_one_or_none()\nif not doc:\n    return []\n\n# Query 2: Recursive CTE with doc.id\nsql = \"\"\"\n    WITH RECURSIVE subtree AS (\n        SELECT * FROM sections\n        WHERE document_id = :doc_id AND heading = :heading\n        ...\n    )\n\"\"\"\n```\n\n**Problem:** Two round-trips to database when one suffices.\n\n**Fix:** Inline document lookup into the CTE's base case:\n```sql\nWITH RECURSIVE subtree AS (\n    -- Base case: find section by heading in active document\n    SELECT s.* FROM sections s\n    JOIN documents d ON s.document_id = d.id\n    WHERE d.domain = :domain\n      AND d.path = :path\n      AND d.generation_active = true\n      AND s.heading = :heading\n    UNION ALL\n    -- Recursive case: get children\n    SELECT s.* FROM sections s\n    JOIN subtree t ON s.parent_section_id = t.id\n)\nSELECT * FROM subtree ORDER BY position\n```\n\n**Benefits:**\n- Single query instead of two\n- Reduces latency\n- Same pattern as sensei-sp6\n\n**Related:** sensei-sp6 (same pattern for get_sections_by_document)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:23:59.684392-05:00","updated_at":"2025-12-01T15:35:56.996079-05:00","closed_at":"2025-12-01T15:35:56.996079-05:00","labels":["performance","query-optimization","storage"],"dependencies":[{"issue_id":"sensei-37t","depends_on_id":"sensei-sp6","type":"related","created_at":"2025-12-01T15:24:11.729151-05:00","created_by":"daemon"}]}
{"id":"sensei-3ao","title":"Expose tome module via MCP tool","description":"The tome module (ingest_domain) exists but nothing exposes it. No MCP tool, no API endpoint, no CLI command. It's dead code from a system perspective.\n\nCreate `sensei/tools/tome.py` that wraps `ingest_domain` as a tool callable via MCP.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-01T07:30:35.674535-05:00","updated_at":"2025-12-01T08:43:58.31512-05:00","closed_at":"2025-12-01T08:43:58.31512-05:00","labels":["mcp","p0","tome"]}
{"id":"sensei-3c9","title":"Add TomeDocument model to database","description":"Add SQLAlchemy model for TomeDocument with fields: id, domain, url, path, content, content_hash, depth, fetched_at. Add to sensei/database/models.py","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:02.272288-05:00","updated_at":"2025-11-27T11:26:10.678093-05:00","closed_at":"2025-11-27T11:26:10.678093-05:00","dependencies":[{"issue_id":"sensei-3c9","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:02.275146-05:00","created_by":"daemon"}]}
{"id":"sensei-3f9","title":"Add stateless_http=True to MCP server mounts","description":"Enable stateless mode for all three MCP servers (mcp, scout, kura) in __main__.py for horizontal scaling.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.731053-05:00","updated_at":"2025-11-27T09:16:32.361168-05:00","closed_at":"2025-11-27T09:16:32.361168-05:00","dependencies":[{"issue_id":"sensei-3f9","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.731773-05:00","created_by":"daemon"}]}
{"id":"sensei-3na","title":"Add scout script entry point to pyproject.toml","description":"Add [project.scripts] with scout = \"sensei.scout:main\" and create main() function","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.291703-05:00","updated_at":"2025-11-27T06:18:27.225929-05:00","closed_at":"2025-11-27T06:18:27.225929-05:00","dependencies":[{"issue_id":"sensei-3na","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.292305-05:00","created_by":"daemon"}]}
{"id":"sensei-3oe","title":"Fix test fixture asyncio.run() conflict with Alembic migrations","description":"The test_db fixture called run_migrations() which used command.upgrade() synchronously. Since the fixture is async and pytest-asyncio runs it in an event loop, calling asyncio.run() inside Alembic failed with \"cannot be called from a running event loop\". Fixed by running migrations in a ThreadPoolExecutor via run_in_executor().","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-27T16:12:07.24009-05:00","updated_at":"2025-11-27T16:12:11.275195-05:00","closed_at":"2025-11-27T16:12:11.275195-05:00","labels":["async","testing"]}
{"id":"sensei-3u52","title":"Investigate returning structured JSON from MCP tools (Kura)","description":"MCP spec (2025-11-25) supports `structuredContent` in tool results alongside text. Currently Kura tools return strings.\n\nQuestions to investigate:\n1. Does FastMCP support returning dicts/Pydantic models that serialize to `structuredContent`?\n2. Does PydanticAI's `FastMCPToolset` pass `structuredContent` to the model or just text?\n3. What's the benefit for discoverability in logs/traces?\n\nCurrent architecture: rich types in core → strings at MCP edge (per CLAUDE.md layering)\n\nReference: https://modelcontextprotocol.io/specification/2025-11-25/server/tools#structured-content","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T17:11:07.026945-05:00","updated_at":"2025-12-12T17:11:07.026945-05:00","labels":["investigation","kura","mcp"]}
{"id":"sensei-3zm","title":"Verify dev server runs and builds successfully","description":"Test that npm run dev and npm run build work in the www package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.339278-05:00","updated_at":"2025-11-27T16:14:31.592256-05:00","closed_at":"2025-11-27T16:14:31.592256-05:00","dependencies":[{"issue_id":"sensei-3zm","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.340618-05:00","created_by":"daemon"},{"issue_id":"sensei-3zm","depends_on_id":"sensei-dbo","type":"blocks","created_at":"2025-11-27T13:22:35.607426-05:00","created_by":"daemon"}]}
{"id":"sensei-4d2l","title":"Move migrations from app startup to CI/CD","description":"Currently migrations run at app startup via `ensure_db_ready()` → `ensure_migrated()`. This causes race conditions with multiple replicas.\n\nMove to running `alembic upgrade head` in CI/CD before deploy instead.\n\n**Current flow**:\n```\nDeploy → App starts → ensure_db_ready() → alembic upgrade head → Ready\n```\n\n**Target flow**:\n```\nCI/CD: alembic upgrade head → Deploy → App starts → Ready\n```\n\nFor external databases (`is_external_database=True`), the app should just connect - never migrate. For sensei-managed local databases, migrations still make sense at startup (single instance by definition).","acceptance_criteria":"- [ ] Remove `ensure_migrated()` call for external databases in `ensure_db_ready()`\n- [ ] Keep auto-migrate for sensei-managed mode (local PG, single instance)\n- [ ] Add deploy script or CI step that runs `alembic upgrade head`\n- [ ] Update CLAUDE.md or docs with deploy instructions\n- [ ] Consider adding startup check that warns if migrations are pending (optional)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T06:26:37.02149-05:00","updated_at":"2025-12-20T13:03:30.956261-05:00","closed_at":"2025-12-20T13:03:30.956261-05:00"}
{"id":"sensei-4mj0","title":"Create sensei/eval/evaluators.py placeholder","description":"Create placeholder module for custom evaluators with SpanTree access. Actual evaluators to be designed in separate brainstorm session. Include example structure:\n```python\n@dataclass\nclass ExampleEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -\u003e bool:\n        span_tree = ctx.span_tree\n        # Access tool calls, reasoning tokens, etc.\n        return True\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T09:42:23.364656-05:00","updated_at":"2025-12-12T11:41:51.516837-05:00","closed_at":"2025-12-12T11:41:51.516837-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-4mj0","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.329793-05:00","created_by":"daemon"}]}
{"id":"sensei-4mz","title":"Consolidate get_sections_for_toc to single query with JOIN","description":"**Current state (storage.py:575-615):**\nTwo separate queries:\n```python\n# Query 1: Find active document\ndoc_result = await session.execute(\n    select(Document).where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n)\ndoc = doc_result.scalar_one_or_none()\nif not doc:\n    return []\n\n# Query 2: Get section hierarchy data\nresult = await session.execute(\n    select(Section.id, Section.parent_section_id, Section.heading, Section.level)\n    .where(Section.document_id == doc.id)\n    .order_by(Section.position)\n)\n```\n\n**Problem:** Two round-trips to database when one suffices.\n\n**Fix:** Single query with JOIN:\n```python\nresult = await session.execute(\n    select(Section.id, Section.parent_section_id, Section.heading, Section.level)\n    .join(Document, Document.id == Section.document_id)\n    .where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n    .order_by(Section.position)\n)\nreturn [(row.id, row.parent_section_id, row.heading, row.level) for row in result]\n```\n\n**Benefits:**\n- Single query instead of two\n- Reduces latency\n- Cleaner code\n\n**Related:** sensei-sp6 (same pattern for get_sections_by_document)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:24:00.032606-05:00","updated_at":"2025-12-01T15:35:56.932049-05:00","closed_at":"2025-12-01T15:35:56.932049-05:00","labels":["performance","query-optimization","storage"],"dependencies":[{"issue_id":"sensei-4mz","depends_on_id":"sensei-sp6","type":"related","created_at":"2025-12-01T15:24:11.768197-05:00","created_by":"daemon"}]}
{"id":"sensei-4ok","title":"Deploy Sensei to Cloud Run with Neon Postgres","description":"Deploy full Sensei HTTP server to Google Cloud Run with Neon Postgres database. Stateless MCP for horizontal scaling.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T09:00:25.904011-05:00","updated_at":"2025-11-27T15:59:45.972035-05:00","closed_at":"2025-11-27T15:59:45.972035-05:00"}
{"id":"sensei-4pt","title":"Create sensei/database/local.py - PostgreSQL lifecycle","description":"Create the PostgreSQL lifecycle management module.\n\n**File:** `sensei/database/local.py`\n\n**This module handles:**\n- PostgreSQL lifecycle (init/start/stop) for sensei-managed mode\n- Migration running for local mode only (thin wrapper on alembic)\n\n**Functions to implement:**\n```python\nimport logging\nimport shutil\nimport subprocess\n\nfrom sensei.paths import get_pgdata, get_pg_log, get_sensei_home\nfrom sensei.types import BrokenInvariant\n\nlogger = logging.getLogger(__name__)\n\n# Idempotency flag - ensure_db_ready() may be called multiple times\n# (e.g., from combined_lifespan AND individual service lifespans)\n_db_ready = False\n\n# ─────────────────────────────────────────────────────────────────────────────\n# PostgreSQL Lifecycle (sensei-managed mode only)\n# ─────────────────────────────────────────────────────────────────────────────\n\ndef check_postgres_installed() -\u003e bool:\n    \"\"\"Check if PostgreSQL binaries are available on PATH.\"\"\"\n    return shutil.which(\"pg_ctl\") is not None\n\n\ndef is_initialized() -\u003e bool:\n    \"\"\"Check if data directory exists and is initialized.\"\"\"\n    return (get_pgdata() / \"PG_VERSION\").exists()\n\n\ndef is_running() -\u003e bool:\n    \"\"\"Check if PostgreSQL is running.\"\"\"\n    result = subprocess.run(\n        [\"pg_ctl\", \"-D\", str(get_pgdata()), \"status\"],\n        capture_output=True,\n    )\n    return result.returncode == 0\n\n\ndef init_db() -\u003e None:\n    \"\"\"Initialize the data directory + create sensei database.\"\"\"\n    logger.info(\"Initializing PostgreSQL data directory...\")\n    get_sensei_home().mkdir(parents=True, exist_ok=True)\n    \n    subprocess.run(\n        [\"initdb\", \"-D\", str(get_pgdata()), \"--auth=trust\", \"--no-instructions\"],\n        check=True,\n    )\n    \n    start()\n    \n    # Create database (uses Unix socket)\n    subprocess.run(\n        [\"createdb\", \"-h\", str(get_pgdata()), \"sensei\"],\n        check=True,\n    )\n    logger.info(\"PostgreSQL initialized and sensei database created\")\n\n\ndef start() -\u003e None:\n    \"\"\"Start PostgreSQL (blocking until ready).\"\"\"\n    logger.info(\"Starting PostgreSQL...\")\n    subprocess.run(\n        [\"pg_ctl\", \"-D\", str(get_pgdata()), \n         \"-l\", str(get_pg_log()), \n         \"-o\", f\"-k {get_pgdata()}\",  # Unix socket in pgdata\n         \"start\", \"-w\"],\n        check=True,\n    )\n    logger.info(\"PostgreSQL started\")\n\n\ndef stop() -\u003e None:\n    \"\"\"Stop PostgreSQL.\"\"\"\n    if is_running():\n        logger.info(\"Stopping PostgreSQL...\")\n        subprocess.run(\n            [\"pg_ctl\", \"-D\", str(get_pgdata()), \"stop\", \"-m\", \"fast\"],\n            check=True,\n        )\n        logger.info(\"PostgreSQL stopped\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# Migrations (local mode only) - thin wrapper on alembic\n# ─────────────────────────────────────────────────────────────────────────────\n\nasync def ensure_migrated() -\u003e None:\n    \"\"\"Run alembic migrations.\"\"\"\n    import asyncio\n    from functools import partial\n    from alembic import command\n    from alembic.config import Config\n    from sensei.config import settings\n    \n    # Build alembic config programmatically (works when installed as package)\n    config = Config()\n    config.set_main_option(\"script_location\", \"sensei:migrations\")\n    # Alembic needs sync URL (no +asyncpg)\n    sync_url = settings.database_url.replace(\"+asyncpg\", \"\")\n    config.set_main_option(\"sqlalchemy.url\", sync_url)\n    \n    # Run migrations (sync operation, run in thread pool)\n    logger.info(\"Running database migrations...\")\n    loop = asyncio.get_event_loop()\n    await loop.run_in_executor(None, partial(command.upgrade, config, \"head\"))\n    logger.info(\"Database migrations complete\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# Main Entry Point\n# ─────────────────────────────────────────────────────────────────────────────\n\nasync def ensure_db_ready() -\u003e None:\n    \"\"\"Ensure database is ready for use.\n    \n    **Idempotent** - safe to call multiple times. Uses module-level flag\n    to skip work on subsequent calls (e.g., when called from both\n    combined_lifespan and individual service lifespans).\n    \n    Behavior:\n    - External DB (DATABASE_URL set): Do nothing (user's responsibility)\n    - Local DB (DATABASE_URL not set): Start PG if needed, run migrations\n    \"\"\"\n    global _db_ready\n    \n    if _db_ready:\n        logger.debug(\"ensure_db_ready() already completed, skipping\")\n        return\n    \n    from sensei.config import settings\n    \n    if settings.is_external_database:\n        # External DB - user is responsible for setup and migrations\n        _db_ready = True\n        return\n    \n    # Sensei-managed mode: ensure PostgreSQL is running\n    if not check_postgres_installed():\n        raise BrokenInvariant(\n            \"PostgreSQL not found. Please install PostgreSQL 17+:\\n\"\n            \"  macOS:   brew install postgresql@17\\n\"\n            \"  Ubuntu:  sudo apt install postgresql-17\\n\"\n            \"  Windows: https://www.postgresql.org/download/windows/\"\n        )\n    \n    if not is_initialized():\n        try:\n            init_db()\n        except subprocess.CalledProcessError:\n            # Another process may have initialized - check again\n            if not is_initialized():\n                raise  # Real failure\n    elif not is_running():\n        start()  # pg_ctl start is safe to call concurrently\n    \n    # Run migrations for local DB\n    await ensure_migrated()\n    \n    _db_ready = True\n    logger.info(\"Database ready\")\n```\n\n**Idempotency:**\n- Module-level `_db_ready` flag ensures work is done only once per process\n- Safe to call from multiple lifespans (combined + individual services)\n- Logs skip on subsequent calls for visibility\n\n**Simplified logic:**\n- External DB → do nothing (user handles everything)\n- Local DB → start PG + migrate (always)\n\n**Mode matrix:**\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No |","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:05.722136-05:00","updated_at":"2025-12-02T07:58:35.199754-05:00","closed_at":"2025-12-02T07:58:35.199754-05:00","dependencies":[{"issue_id":"sensei-4pt","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:05.722999-05:00","created_by":"daemon"},{"issue_id":"sensei-4pt","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:10:20.55718-05:00","created_by":"daemon"}]}
{"id":"sensei-4r2r","title":"Remove dead `cache_ttl_days` setting or implement cache cleanup","description":"`cache_ttl_days` is defined in `settings.py:135` and tested, but never actually used anywhere. Either:\n- Remove the dead code (setting + test)\n- Implement actual cache cleanup that uses the TTL to delete old cached queries\n\nGrep shows only 2 references: definition and test assertion.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-12-20T15:02:15.134054-05:00","updated_at":"2025-12-20T15:02:15.134054-05:00","labels":["lifetime-audit","tech-debt"]}
{"id":"sensei-4wa","title":"Local PostgreSQL lifecycle management (epic)","description":"Enable sensei to manage a local PostgreSQL instance automatically, eliminating the need for users to manually set up and run PostgreSQL.\n\n## User-facing requirements\n\n1. User installs PostgreSQL 17+ (via Homebrew, apt, etc.) - we document how\n2. Sensei automatically initializes, starts, and migrates the database\n3. Works identically for:\n   - Claude Code plugins: `uvx --from sensei-ai scout` (stdio MCP)\n   - Full server: `uvicorn sensei:app` (HTTP server)\n\n## Key design decisions\n\n- **SENSEI_HOME env var** - defaults to `~/.sensei`, set `SENSEI_HOME=.sensei` in `.env` for development\n- **Unix socket connection** - no port conflicts with system PostgreSQL\n- **`trust` auth** for local socket (it's a local dev DB)\n- **Auto-start on first DB access** (lazy, via `ensure_db_ready()`)\n- **No filelock** - race conditions handled gracefully (initdb failure → retry, pg_ctl is idempotent)\n- **No CLI** - fully automatic, no manual `sensei db *` commands\n- **Idempotent `ensure_db_ready()`** - safe to call multiple times (module-level flag)\n\n## Folder structure\n\n```\n~/.sensei/                      # or SENSEI_HOME if set\n├── pgdata/                     # PostgreSQL data directory\n├── pg.log                      # PostgreSQL log\n└── scout/\n    └── repos/                  # Cloned GitHub repos\n```\n\n## Mode matrix\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No (user's responsibility) |\n\n## Subtasks\n\n1. `sensei-2fb` - Create sensei/paths.py (SENSEI_HOME, get_pgdata, etc.)\n2. `sensei-4pt` - Create sensei/database/local.py (lifecycle + migrations)\n3. `sensei-gvh` - Update sensei/config.py (dynamic database_url)\n4. `sensei-6ul` - Update pyproject.toml (add tome entry point)\n5. `sensei-7t4` - Restructure alembic for package distribution\n6. `sensei-agi` - Add ensure_db_ready() to MCP server lifespans\n7. `sensei-6iu` - Update scout/manager.py to use centralized paths\n8. `sensei-dnj` - Verify storage.py uses settings.database_url\n9. `sensei-asv` - Add unit tests (no integration tests)\n10. `sensei-sop` - Update README with PostgreSQL instructions\n\n## Closed subtasks\n\n- `sensei-czf` - CLI not needed (auto-managed)","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-02T06:09:40.48943-05:00","updated_at":"2025-12-02T08:06:21.13878-05:00","closed_at":"2025-12-02T08:06:21.13878-05:00","labels":["database","dx","infrastructure"]}
{"id":"sensei-4wt","title":"Tome: Fix misleading link count logging","description":"crawler.py:90 logs \"Found {len(links)} links\" but links is already filtered at that point. Should extract total before filtering for accurate logging.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:44.144582-05:00","updated_at":"2025-11-27T16:08:50.891579-05:00","closed_at":"2025-11-27T16:08:50.891579-05:00","labels":["minor","tome"]}
{"id":"sensei-505","title":"Refactor import cycles and lazy imports in sensei package","description":"Multiple lazy imports inside functions (e.g., crawler.py importing save_tome_document inside ingest_domain) suggest poor layering. Cycles are a code smell indicating unclear boundaries. Need to analyze import graph, identify cycles, and restructure modules for clean dependency flow.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:44:08.3768-05:00","updated_at":"2025-11-27T13:52:52.722924-05:00","closed_at":"2025-11-27T13:52:52.722924-05:00","labels":["architecture","important","tech-debt"]}
{"id":"sensei-51u","title":"Create Fly volume and set secrets","description":"Run fly commands to: 1) fly launch (or fly apps create), 2) fly volumes create sensei_data --size 10, 3) fly secrets set for DATABASE_URL, ANTHROPIC_API_KEY, etc.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.914964-05:00","updated_at":"2025-11-30T05:57:26.737998-05:00","closed_at":"2025-11-30T05:57:26.737998-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-51u","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:18.011101-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-0y5","type":"blocks","created_at":"2025-11-27T16:00:24.494254-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-iid","type":"blocks","created_at":"2025-11-27T16:00:24.53964-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-n82","type":"blocks","created_at":"2025-11-27T16:00:24.57999-05:00","created_by":"daemon"}]}
{"id":"sensei-543","title":"Tome: Normalize subdomains in is_same_domain()","description":"parser.py:38-50 treats www.example.com and example.com as different domains. Should strip www. prefix for comparison to avoid filtering legitimate docs.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, this becomes trivial - just use Domain() for comparison.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:44:01.949301-05:00","updated_at":"2025-11-27T13:42:03.611821-05:00","closed_at":"2025-11-27T13:42:03.611821-05:00","labels":["critical","tome"],"dependencies":[{"issue_id":"sensei-543","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.845224-05:00","created_by":"daemon"}]}
{"id":"sensei-56t","title":"Decide on UUID generation strategy for primary keys","description":"TomeDocument uses String primary key with UUID generated in Python (storage.py). Need to decide: (1) Generate UUID in Python at storage layer (current), (2) Use PostgreSQL gen_random_uuid() as server_default, (3) Use SQLAlchemy's uuid type with default. Also affects: should queries table use UUID instead of caller-provided query_id?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T16:12:15.997504-05:00","updated_at":"2025-11-30T08:36:02.969798-05:00","closed_at":"2025-11-30T08:36:02.969798-05:00","labels":["database","migrations"]}
{"id":"sensei-5a1","title":"SQL injection vulnerability in document search","description":"storage.py:268 uses direct string interpolation in ILIKE query:\n```python\nstmt = stmt.where(Document.content.ilike(f\"%{query}%\"))\n```\nUse SQLAlchemy's parameterized binding instead.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-30T10:05:21.45224-05:00","updated_at":"2025-11-30T10:12:40.232206-05:00","closed_at":"2025-11-30T10:12:40.232206-05:00"}
{"id":"sensei-5g0x","title":"Remove lifespan from tome/server.py","description":"Remove the lifespan function from tome server. It only existed to call ensure_db_ready().\n\n**File:** `sensei/tome/server.py`\n\n**Remove:**\n- The entire `lifespan` async context manager function\n- The `asynccontextmanager` import from contextlib\n- The `lifespan=lifespan` parameter from FastMCP constructor","design":"```python\n# Before\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\nmcp = FastMCP(name=\"tome\", lifespan=lifespan)\n\n# After\nmcp = FastMCP(name=\"tome\")\n```\n\nThis enables FastMCP direct mounting (faster than proxy mounting).","acceptance_criteria":"- No lifespan function\n- No asynccontextmanager import\n- FastMCP created without lifespan parameter\n- Server still works when run standalone","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.732715-05:00","updated_at":"2025-12-21T09:57:31.277995-05:00","closed_at":"2025-12-21T09:57:31.277995-05:00","dependencies":[{"issue_id":"sensei-5g0x","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.738816-05:00","created_by":"daemon"},{"issue_id":"sensei-5g0x","depends_on_id":"sensei-zujl","type":"blocks","created_at":"2025-12-21T09:46:23.128998-05:00","created_by":"daemon"}]}
{"id":"sensei-5i5","title":"Separate warnings from failures in IngestResult","description":"Currently all issues go into a single `errors` list. There's no distinction between:\n- Content we deliberately skipped (wrong content-type) - expected behavior\n- Actual failures (decode error, network error) - unexpected behavior\n\nFix: Split into `warnings: list[str]` (expected skips) and `failures: list[str]` (unexpected errors) for better observability.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T07:30:35.95418-05:00","updated_at":"2025-12-02T08:35:49.723467-05:00","closed_at":"2025-12-02T08:35:49.723467-05:00","labels":["observability","p2","tome"],"dependencies":[{"issue_id":"sensei-5i5","depends_on_id":"sensei-qm8","type":"blocks","created_at":"2025-12-01T07:30:44.021966-05:00","created_by":"daemon"}]}
{"id":"sensei-5r2g","title":"Add main() function to sensei/kura/server.py","description":"Move the main() entry point from __main__.py to server.py where the kura FastMCP server is defined. This follows the idiomatic pattern used by pytest/uvicorn.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:33:57.781167-05:00","updated_at":"2025-12-02T12:36:37.115882-05:00","closed_at":"2025-12-02T12:36:37.115882-05:00"}
{"id":"sensei-5t3e","title":"Chat interface for sensei-web (backend API work)","description":"Backend API work for the sensei chat interface. The frontend lives in 803/web (packages/sensei) and tracks its own beads there (epic: web-xrc).\n\nThis epic tracks the Python backend work: unified API server, VercelAIAdapter endpoint, rate limiting, and related improvements.","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-17T18:30:43.457388-05:00","updated_at":"2025-12-17T18:30:43.457388-05:00"}
{"id":"sensei-5wd","title":"Add asyncpg dependency to pyproject.toml","description":"Add asyncpg for Postgres async support. SQLAlchemy auto-selects driver based on DATABASE_URL prefix.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.619244-05:00","updated_at":"2025-11-27T09:10:01.260401-05:00","closed_at":"2025-11-27T09:10:01.260401-05:00","dependencies":[{"issue_id":"sensei-5wd","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.620306-05:00","created_by":"daemon"}]}
{"id":"sensei-635q","title":"Improve prompt to include citations with exact passages from sources","description":"Update system prompt to require the agent to include references and citations with exact quoted passages from source material. This enables calling agents to dig deeper into the same sources if needed.","design":"## Current State\n\nThe `REPORTING_RESULTS` section in `sensei/prompts.py` asks for debugging context but doesn't mandate citations:\n\n```python\n### Provide Debugging Context\n\nWhen you do find an answer, include enough context that the caller can troubleshoot...\n- Explain the underlying model or concept, not just the solution\n- Note any assumptions or edge cases\n- Mention related functionality\n```\n\n**Problems:**\n1. No requirement for source URLs/references\n2. No exact quotes from documentation\n3. Calling agents can't easily follow up on sources\n4. No traceability back to original material\n\n## Why This Matters\n\nSensei is often called by other agents (Claude Code, etc.). When Sensei returns a response:\n- The calling agent may want to dig deeper into a topic\n- Exact passages let the caller verify accuracy\n- URLs/references enable follow-up research\n- Quoted text is more trustworthy than paraphrasing\n\n## Design Principles\n\n1. **Cite sources explicitly** - URL or tool + query that produced the result\n2. **Quote exact passages** - Use blockquotes for key text from sources\n3. **Structured format** - Make citations parseable by calling agents\n4. **Don't bloat responses** - Cite key claims, not every sentence\n\n## Proposed Addition to REPORTING_RESULTS\n\n```markdown\n### Citations and References\n\nWhen reporting findings, include explicit citations so callers can verify and dig deeper:\n\n**For each key claim or code example:**\n- Quote the exact passage from the source using blockquotes\n- Include the source URL or reference (tool name + query)\n- Note the confidence level (official docs vs community)\n\nExample format:\n\u003e \"useEffect cleanup functions run before the component unmounts and before every re-render\"\n\u003e — React docs (https://react.dev/reference/react/useEffect)\n\nThis enables calling agents to:\n- Verify your findings against the original source\n- Dig deeper into the same documentation\n- Follow related links from the source\n```\n\n## Acceptance Criteria\n\n- [ ] `REPORTING_RESULTS` updated with citation requirements\n- [ ] Agent includes source URLs in responses\n- [ ] Agent uses blockquotes for exact passages from docs\n- [ ] Citations are parseable (consistent format)\n- [ ] Response length increase is reasonable (\u003c30% for typical queries)\n- [ ] Example responses reviewed for citation quality\n\n## Implementation Considerations\n\n- May increase token usage (quotes add length)\n- Need to balance thoroughness vs brevity\n- Consider structured citation format for machine parsing\n- Could add a `--verbose` flag for full citations vs summary mode","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-18T07:45:24.325925-05:00","updated_at":"2025-12-18T09:08:02.305466-05:00","closed_at":"2025-12-18T09:08:02.305466-05:00","labels":["agent","prompt"]}
{"id":"sensei-656","title":"Re-evaluate str vs UUID type for query_id across layers","description":"Currently query_id uses different types at different layers:\n\n**Database layer (UUID):**\n- `storage.save_query()` returns `UUID`\n- `storage.get_query()` takes `UUID`\n- Models use `UUID(as_uuid=True)`\n\n**Application layer (str):**\n- `Deps.query_id: Optional[str]`\n- `Deps.parent_id: Optional[str]`\n- `types.QueryResult.query_id: str`\n- `types.Rating.query_id: str`\n- `types.CacheHit.query_id: str`\n- `types.SubSenseiResult.query_id: str`\n\n**Conversion points:**\n- `str(query_id)` - storage.py:167, core.py:119, 175, 178, sub_agent.py:86\n- `UUID(query_id)` - kura/tools.py:103, sub_agent.py:81, 158\n\n**Options to evaluate:**\n1. Keep current (str at edges, UUID in DB) - simpler JSON serialization\n2. Use UUID throughout - better type safety, Pydantic handles serialization\n3. Create a QueryId newtype that wraps UUID with str serialization\n\nDiscovered from sensei-56t review.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T08:37:42.887713-05:00","updated_at":"2025-11-30T09:31:38.454461-05:00","closed_at":"2025-11-30T09:31:38.454461-05:00","labels":["architecture","types"]}
{"id":"sensei-67v","title":"Implement generation-based crawls with atomic swap","description":"**Problem:**\nDocuments removed from source are never detected/deleted. Queries may see partial state during crawl.\n\n**Solution:** Generation-based crawls with atomic visibility swap.\n\n**Schema changes:**\n```sql\nALTER TABLE documents ADD COLUMN generation_id UUID NOT NULL;\nALTER TABLE documents ADD COLUMN generation_active BOOLEAN NOT NULL DEFAULT false;\nCREATE INDEX idx_documents_domain_active ON documents (domain) WHERE generation_active = true;\nCREATE VIEW documents_active AS SELECT * FROM documents WHERE generation_active = true;\n```\n\n**Crawl flow:**\n1. `gen_id = uuid4()` at crawl start\n2. Insert all docs with `generation_id=gen_id, generation_active=false`\n3. Insert all sections (FK to docs)\n4. Atomic swap: `UPDATE documents SET generation_active = (generation_id = $gen_id) WHERE domain = $domain`\n5. Cleanup: `DELETE FROM documents WHERE domain = $domain AND NOT generation_active` (cascades to sections)\n\n**Query changes:**\n- Use `documents_active` view instead of `documents` table\n- Or add `WHERE generation_active = true` to queries\n\n**Benefits:**\n- Queries always see complete set (old OR new, never partial)\n- Failed crawl = orphan generation, no impact on queries\n- Swap is instant (single UPDATE)\n- Can rollback by flipping generation_active back\n- Cleanup is separate, non-blocking\n\n**Note:** This replaces the skip-if-unchanged logic. Every crawl is a full re-process.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-01T13:22:59.958866-05:00","updated_at":"2025-12-01T13:34:48.025928-05:00","closed_at":"2025-12-01T13:34:48.025928-05:00"}
{"id":"sensei-6br","title":"Configure Vite and React Router for Framework Mode","description":"Create vite.config.ts and react-router.config.ts for Framework Mode setup","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.092242-05:00","updated_at":"2025-11-27T13:27:48.421694-05:00","closed_at":"2025-11-27T13:27:48.421694-05:00","dependencies":[{"issue_id":"sensei-6br","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.094102-05:00","created_by":"daemon"},{"issue_id":"sensei-6br","depends_on_id":"sensei-apn","type":"blocks","created_at":"2025-11-27T13:22:35.39033-05:00","created_by":"daemon"}]}
{"id":"sensei-6btu","title":"Standardize on SENSEI_DATABASE_URL naming in settings/docs/tests","description":"Update settings, docs, and tests to use SENSEI_DATABASE_URL only (no DATABASE_URL alias).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:05:13.755776-05:00","updated_at":"2025-12-21T10:05:18.317181-05:00","closed_at":"2025-12-21T10:05:18.317181-05:00"}
{"id":"sensei-6iu","title":"Update sensei/scout/manager.py - use centralized paths","description":"Update the scout RepoManager to use centralized paths instead of hardcoded `.scout/repos`.\n\n**File:** `sensei/scout/manager.py`\n\n**Current (line 41):**\n```python\nself.cache_dir = cache_dir or Path(\".\") / \".scout\" / \"repos\"\n```\n\n**Change to:**\n```python\nfrom sensei.paths import get_scout_repos\n\nself.cache_dir = cache_dir or get_scout_repos()\n```\n\n**Why:**\n- Currently uses cwd, which is wrong for `uvx --from sensei-ai scout`\n- With centralized paths, repos cache goes to:\n  - Development: `./.sensei/scout/repos/`\n  - Production: `~/.sensei/scout/repos/`\n- Repos are shared across all invocations (not per-cwd)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:32.667055-05:00","updated_at":"2025-12-02T07:58:35.218766-05:00","closed_at":"2025-12-02T07:58:35.218766-05:00","dependencies":[{"issue_id":"sensei-6iu","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:10:32.668714-05:00","created_by":"daemon"},{"issue_id":"sensei-6iu","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:32.669535-05:00","created_by":"daemon"}]}
{"id":"sensei-6sm","title":"Remove dead code from tools/common.py","description":"tools/common.py has unused functions that should be removed after exec_plan refactor:\n\nDead code:\n- format_entries (line 15) - never imported or used\n- get_client (line 44) - never imported or used  \n- DEFAULT_TIMEOUT (line 12) - only used by get_client\n\nKeep:\n- wrap_tool (line 52) - will be used after exec_plan refactor\n\nAfter exec_plan refactor is complete, audit again and remove any remaining dead code.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:14:46.657517-05:00","updated_at":"2025-11-30T06:17:58.148726-05:00","closed_at":"2025-11-30T06:17:58.148726-05:00","labels":["cleanup"]}
{"id":"sensei-6ul","title":"Update pyproject.toml - add tome entry point","description":"Update pyproject.toml with missing entry point.\n\n**File:** `pyproject.toml`\n\n**Update [project.scripts]:**\n```toml\n[project.scripts]\nscout = \"sensei.scout:scout.run\"        # existing\nkura = \"sensei.kura:kura.run\"           # existing  \ntome = \"sensei.tome.server:tome.run\"    # NEW: tome was missing!\n```\n\n**Why:**\n- `tome` entry point was missing (only scout and kura existed)\n- Enables `uvx --from sensei-ai tome` to work\n\n**No CLI needed** - sensei-czf was closed. HTTP server can be started via uvicorn directly if needed.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:55.930428-05:00","updated_at":"2025-12-02T07:41:39.475717-05:00","closed_at":"2025-12-02T07:41:39.475717-05:00","dependencies":[{"issue_id":"sensei-6ul","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:55.931283-05:00","created_by":"daemon"},{"issue_id":"sensei-6ul","depends_on_id":"sensei-czf","type":"blocks","created_at":"2025-12-02T06:11:08.411426-05:00","created_by":"daemon"}]}
{"id":"sensei-6wo","title":"Add new types for section-based storage","description":"Add new domain types to `sensei/types.py` for section-based storage.\n\n**New types:**\n\n```python\n@dataclass\nclass SectionData:\n    \"\"\"Intermediate type for chunking algorithm output.\"\"\"\n    heading: str | None\n    level: int\n    content: str\n    children: list[SectionData]\n\n@dataclass  \nclass TOCEntry:\n    \"\"\"Table of contents entry for tome_toc().\"\"\"\n    heading: str\n    level: int\n    children: list[TOCEntry]\n```\n\n**Update existing:**\n\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n    heading_path: str  # NEW: breadcrumb like \"API \u003e Hooks \u003e useState\"\n```\n\n**Remove:**\n- `DocumentContent.depth` field (per sensei-cij)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T10:00:36.794866-05:00","updated_at":"2025-12-01T10:10:33.327875-05:00","closed_at":"2025-12-01T10:10:33.327875-05:00","labels":["tome","types"],"dependencies":[{"issue_id":"sensei-6wo","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T10:00:36.797037-05:00","created_by":"daemon"}]}
{"id":"sensei-718","title":"Move search_queries transformation logic to service layer","description":"**Current state (storage.py:131-183):**\n`search_queries` builds `CacheHit` objects with calculated/derived fields:\n\n```python\nnow = datetime.now(UTC)\nhits = []\nfor row in rows:\n    query_id, query_text, lib, ver, inserted_at = row\n    age_days = (now - inserted_at).days if inserted_at else 0\n    hits.append(\n        CacheHit(\n            query_id=query_id,\n            query_truncated=query_text[:100] if query_text else \"\",\n            age_days=age_days,\n            library=lib,\n            version=ver,\n        )\n    )\nreturn hits\n```\n\n**Problems:**\n1. `query_truncated=query_text[:100]` - presentation logic (truncation)\n2. `age_days` calculation - derived field computation\n3. Building `CacheHit` domain objects - transformation logic\n\n**Storage should:** Return raw `Query` objects (or minimal row data)\n\n**Service layer should:** Handle truncation, age calculation, CacheHit construction\n\n**Fix:**\n1. Storage returns `list[Query]` from simple SELECT\n2. Create/use service function that transforms to `list[CacheHit]`\n\n**Note:** This function is used by cache tools. Check `sensei/tools/cache.py` for the caller.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:19:45.597114-05:00","updated_at":"2025-12-01T15:40:19.385005-05:00","closed_at":"2025-12-01T15:40:19.385005-05:00","labels":["refactoring","service-layer","storage"]}
{"id":"sensei-73d","title":"Add edge case tests for Domain value object","description":"The `Domain` value object in `sensei/types.py` handles normalization but edge cases aren't tested with tome.\n\n**Edge cases to test:**\n- Domain with trailing slash: `example.com/`\n- Domain with path: `example.com/docs` (should extract just domain)\n- Domain with port: `example.com:8080`\n- Domain with protocol: `https://example.com`\n- Domain with www: `www.example.com`\n- Mixed case: `Example.COM`\n\n**Files:**\n- `sensei/types.py` - Domain class\n- `tests/test_tome_parser.py` - could add tests here or new file","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T09:04:15.877467-05:00","updated_at":"2025-12-02T12:44:51.196795-05:00","closed_at":"2025-12-02T12:44:51.196795-05:00","labels":["testing","tome"]}
{"id":"sensei-775","title":"Precompute heading_path in sections table to eliminate recursive CTE in FTS","description":"**Current state (storage.py:488-558):**\n`search_sections_fts` uses a recursive CTE on every search to build heading breadcrumbs:\n```sql\nWITH RECURSIVE ancestors AS (\n    SELECT s.id, s.parent_section_id, s.heading, ...\n           ARRAY[s.heading] as path_array, 1 as depth\n    FROM sections s\n    JOIN documents d ON s.document_id = d.id\n    WHERE ... AND s.search_vector @@ websearch_to_tsquery(...)\n    UNION ALL\n    SELECT a.id, p.parent_section_id, p.heading, ...\n           p.heading || a.path_array, a.depth + 1\n    FROM ancestors a\n    JOIN sections p ON a.parent_section_id = p.id\n    WHERE a.depth \u003c 10\n),\nfull_paths AS (\n    SELECT DISTINCT ON (id) id, path_array, ...\n    FROM ancestors ORDER BY id, depth DESC\n)\nSELECT array_to_string(array_remove(fp.path_array, NULL), ' \u003e ') as heading_path\n...\n```\n\n**Problem:** Recursive CTE runs on every search query. For deep heading hierarchies, this is expensive. The heading path doesn't change after crawl.\n\n**Fix:** Precompute `heading_path` at crawl time:\n\n1. **Add column to Section model:**\n```python\n# sensei/database/models.py\nheading_path = Column(String, nullable=True)  # e.g., \"API \u003e Hooks \u003e useState\"\n```\n\n2. **Compute in flatten_section_tree() (crawler.py):**\n```python\ndef flatten_section_tree(root: SectionData, document_id: UUID) -\u003e list[Section]:\n    sections: list[Section] = []\n    position = [0]\n\n    def walk(node: SectionData, parent_id: UUID | None, path_parts: list[str]) -\u003e None:\n        if node.content or node.children:\n            # Build heading path from ancestors\n            current_path = path_parts + ([node.heading] if node.heading else [])\n            heading_path = \" \u003e \".join(p for p in current_path if p) or None\n            \n            section = Section(\n                document_id=document_id,\n                parent_section_id=parent_id,\n                heading=node.heading,\n                level=node.level,\n                content=node.content or \"\",\n                position=position[0],\n                heading_path=heading_path,  # NEW\n            )\n            position[0] += 1\n            sections.append(section)\n\n            for child in node.children:\n                walk(child, section.id, current_path)\n\n    walk(root, None, [])\n    return sections\n```\n\n3. **Simplify FTS query:**\n```sql\nSELECT d.url, d.path, \n       ts_headline(...) as snippet,\n       ts_rank(...) as rank,\n       s.heading_path  -- Direct column read, no CTE!\nFROM sections s\nJOIN documents d ON s.document_id = d.id\nWHERE d.domain = :domain AND d.generation_active = true\n  AND s.search_vector @@ websearch_to_tsquery('english', :query)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Migration:**\n```python\ndef upgrade():\n    op.add_column(\"sections\", sa.Column(\"heading_path\", sa.String(), nullable=True))\n\ndef downgrade():\n    op.drop_column(\"sections\", \"heading_path\")\n```\n\n**Benefits:**\n- Eliminates expensive recursive CTE on every search\n- O(1) lookup instead of O(depth) tree traversal\n- Search queries become much simpler and faster\n\n**Trade-off:** Small storage increase (~50 bytes per section avg). Worth it for search performance.\n\n**Note:** Existing sections will have NULL heading_path until re-crawled. FTS query should fall back to recursive CTE or return empty string for NULL.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T15:24:00.783924-05:00","updated_at":"2025-12-01T15:33:09.945079-05:00","closed_at":"2025-12-01T15:33:09.945079-05:00","labels":["database","denormalization","fts","performance"]}
{"id":"sensei-7af","title":"Chunker should always parse structure, not just when content is large","description":"**Current state (chunker.py:74-76):**\n```python\nif count_tokens(content) \u003c= max_tokens:\n    # Content fits - return as single section\n    return SectionData(heading=None, level=0, content=content, children=[])\n```\n\n**Problem:** If content is small, it returns a single flat SectionData with no structure. This loses the heading hierarchy even when it exists.\n\n**Current behavior:**\n- Small doc with h1, h2, h3 → single flat SectionData (structure lost)\n- Large doc → parsed into tree structure\n\n**Desired behavior:**\n- All docs → parsed into tree structure by headings\n- Token limit only affects whether we need to recursively split further\n\n**Fix:**\nRemove the early return. Always parse headings and build the structure. The max_tokens limit should only trigger additional splitting when a section is too large, not skip structure parsing entirely.\n\n```python\ndef chunk_markdown(content: str, max_tokens: int = DEFAULT_MAX_TOKENS) -\u003e SectionData:\n    lines = content.split(\"\\n\")\n    sections = _split_by_top_level_headings(lines)\n    \n    if not sections:\n        # No headings - return as single root section\n        return SectionData(heading=None, level=0, content=content, children=[])\n    \n    # Build tree from sections, recursively splitting if any are too large\n    # ...\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.550148-05:00","updated_at":"2025-12-01T14:40:07.466015-05:00","closed_at":"2025-12-01T14:40:07.466015-05:00"}
{"id":"sensei-7ao","title":"Update tests to run migrations instead of create_all","description":"Tests run alembic upgrade head against test database. No create_all() shortcut. Ensures migrations are always tested.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.547898-05:00","updated_at":"2025-11-27T16:01:50.511515-05:00","closed_at":"2025-11-27T16:01:50.511515-05:00","labels":["database","testing"],"dependencies":[{"issue_id":"sensei-7ao","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:08.142222-05:00","created_by":"daemon"},{"issue_id":"sensei-7ao","depends_on_id":"sensei-ke1","type":"blocks","created_at":"2025-11-27T13:39:08.322769-05:00","created_by":"daemon"}]}
{"id":"sensei-7cm9","title":"Simplify /api/chat endpoint - remove _extract_last_user_prompt","description":"The `/api/chat` endpoint has unnecessary complexity. `_extract_last_user_prompt()` exists solely to extract the user's text for cache lookup (`storage.search_queries(prompt)`). The pydantic-ai docs show a much simpler pattern.\n\n**Current complexity:**\n- Manual prompt extraction with janky `getattr` calls\n- Pre-fetching cache hits before agent runs\n- Custom deps just to pass cache_hits\n\n**Root cause:** Cache lookup needs the user's prompt, but VercelAIAdapter doesn't expose it directly.","design":"**Option 1: Drop cache lookup for now**\n- Remove `_extract_last_user_prompt()` entirely\n- Remove deps/cache_hits logic\n- Endpoint becomes ~15 lines, matches pydantic-ai docs exactly\n- Can add cache back later as a tool\n\n**Option 2: Make cache lookup an agent tool**\n- Agent decides when/if to search cache\n- Tool can access conversation history directly\n- More flexible, agent-driven approach\n- No pre-fetch needed in endpoint\n\n**Option 3: Pass run_input into deps**\n- Let the agent/tool extract prompt as needed\n- Keeps cache as pre-fetched context\n- Moves complexity from endpoint into deps/agent layer\n\n**Recommendation:** Start with Option 1 (simplest), then consider Option 2 if cache is valuable.","acceptance_criteria":"- [ ] `_extract_last_user_prompt()` removed from api/__init__.py\n- [ ] Endpoint matches simplified pydantic-ai pattern\n- [ ] Rate limiting preserved\n- [ ] Streaming still works with frontend","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T18:28:32.594816-05:00","updated_at":"2025-12-17T19:35:04.803656-05:00","closed_at":"2025-12-17T19:35:04.803656-05:00","dependencies":[{"issue_id":"sensei-7cm9","depends_on_id":"sensei-5t3e","type":"parent-child","created_at":"2025-12-17T18:31:04.413379-05:00","created_by":"daemon"}]}
{"id":"sensei-7d0","title":"Tome: Fix depth semantics documentation","description":"crawler.py:40,86 comment says \"0 = llms.txt only\" but code with max_depth=0 still crawls llms.txt. Documentation doesn't match behavior.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:36.082348-05:00","updated_at":"2025-11-27T16:08:50.831049-05:00","closed_at":"2025-11-27T16:08:50.831049-05:00","labels":["minor","tome"]}
{"id":"sensei-7f4","title":"Tome: Track documents_updated correctly in IngestResult","description":"crawler.py:71-83 only tracks documents_added and documents_skipped. documents_updated always stays at 0. Modify save_tome_document() to distinguish new/updated/skipped.","notes":"System thinking review: This is a symptom of bool return type from storage. Once sensei-0ih (SaveResult enum) is implemented, tracking becomes trivial with pattern matching.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:21.836762-05:00","updated_at":"2025-11-27T13:42:03.811644-05:00","closed_at":"2025-11-27T13:42:03.811644-05:00","labels":["important","tome"],"dependencies":[{"issue_id":"sensei-7f4","depends_on_id":"sensei-0ih","type":"blocks","created_at":"2025-11-27T13:28:54.991289-05:00","created_by":"daemon"}]}
{"id":"sensei-7go","title":"Add kura to plugin mcpServers config","description":"Update packages/marketplace/sensei/.claude-plugin/plugin.json to include kura MCP server alongside scout.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.252771-05:00","updated_at":"2025-11-27T08:58:06.459188-05:00","closed_at":"2025-11-27T08:58:06.459188-05:00","dependencies":[{"issue_id":"sensei-7go","depends_on_id":"sensei-k1i","type":"blocks","created_at":"2025-11-27T08:55:36.594533-05:00","created_by":"daemon"}]}
{"id":"sensei-7gz","title":"Implement adaptive recursive markdown chunker","description":"Create the chunking algorithm that splits markdown by headings based on token count.\n\n**Location:** `sensei/tome/chunker.py` (new file)\n\n**Algorithm:**\n```python\ndef chunk_markdown(content: str, max_tokens: int = 8000) -\u003e list[SectionData]:\n    if count_tokens(content) \u003c= max_tokens:\n        return [SectionData(content=content, level=0, heading=None)]\n    \n    children = split_by_top_level_headings(content)\n    if not children:\n        raise BrokenInvariant(\"Content exceeds limit with no heading boundaries\")\n    \n    results = []\n    for child in children:\n        if count_tokens(child.content) \u003c= max_tokens:\n            results.append(child)\n        else:\n            results.extend(chunk_markdown(child.content, max_tokens))\n    return results\n```\n\n**Functions needed:**\n- `count_tokens(content: str) -\u003e int` - use tiktoken or simple word count\n- `split_by_top_level_headings(content: str) -\u003e list[SectionData]` - parse markdown AST, extract sections\n- `chunk_markdown(content: str, max_tokens: int) -\u003e list[SectionData]` - main recursive function\n\n**SectionData type:**\n```python\n@dataclass\nclass SectionData:\n    heading: str | None\n    level: int\n    content: str\n    children: list[SectionData]  # For building parent relationships\n```\n\n**Uses existing:** marko parser from parser.py","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:37.763826-05:00","updated_at":"2025-12-01T10:10:33.275686-05:00","closed_at":"2025-12-01T10:10:33.275686-05:00","labels":["parser","tome"],"dependencies":[{"issue_id":"sensei-7gz","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T09:56:37.766473-05:00","created_by":"daemon"}]}
{"id":"sensei-7jw","title":"Add search_documents_fts() to storage layer","description":"Add PostgreSQL full-text search function to storage layer for tome_search.\n\n**Function signature:**\n```python\nasync def search_documents_fts(\n    domain: str,\n    query: str,\n    paths: list[str] | None = None,\n    limit: int = 10,\n) -\u003e list[SearchResult]\n```\n\n**Behavior:**\n- Use `websearch_to_tsquery()` for natural language query parsing\n- Filter by domain (required)\n- Optionally filter by path prefixes (e.g., paths=[\"/hooks\"] matches \"/hooks/useState\")\n- Return ranked results with:\n  - url, path\n  - snippet (ts_headline for context)\n  - relevance score\n\n**SQL approach:**\n```sql\nSELECT url, path, \n       ts_headline('english', content, query, 'MaxWords=50, MinWords=20') as snippet,\n       ts_rank(search_vector, query) as rank\nFROM documents\nWHERE domain = :domain\n  AND search_vector @@ websearch_to_tsquery('english', :query)\n  AND (path LIKE ANY(:path_patterns) OR :path_patterns IS NULL)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Domain model:**\nAdd `SearchResult` to `sensei/types.py`:\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n```\n\n**Depends on:** sensei-ip4 (FTS migration must run first)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:50.4462-05:00","updated_at":"2025-12-01T08:50:29.453276-05:00","closed_at":"2025-12-01T08:50:29.453276-05:00","labels":["fts","storage","tome"],"dependencies":[{"issue_id":"sensei-7jw","depends_on_id":"sensei-ip4","type":"blocks","created_at":"2025-12-01T08:38:58.748207-05:00","created_by":"daemon"},{"issue_id":"sensei-7jw","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.678492-05:00","created_by":"daemon"}]}
{"id":"sensei-7q6x","title":"Add capture_exception() to agent.py ToolError handler","description":"Add `sentry_sdk.capture_exception()` to the ToolError handler in `sensei/agent.py`.\n\n## Location\nLine 92 - ToolError catch block\n\n## Implementation\n\n```python\nexcept ToolError as e:\n    sentry_sdk.capture_exception(e)  # ADD THIS\n    return str(e)  # Return error message for agent to see\n```\n\n## Context\nThis catches tool errors and returns them as strings for the LLM agent to see. We still want to track these in Sentry for monitoring.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-18T10:16:55.002685-05:00","updated_at":"2025-12-18T10:21:02.792221-05:00","closed_at":"2025-12-18T10:21:02.792221-05:00","dependencies":[{"issue_id":"sensei-7q6x","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:55.003753-05:00","created_by":"daemon"}]}
{"id":"sensei-7t4","title":"Restructure alembic for package distribution","description":"Restructure alembic migrations to work when sensei is installed as a package.\n\n**Problem:**\nWhen running via `uvx --from sensei tome`, the cwd is NOT the sensei repo. The `alembic.ini` and `alembic/` directory won't be found.\n\n**Solution:** Move migrations into the package and update alembic.ini.\n\n**Steps:**\n\n### 1. Create sensei/migrations/ directory structure\n```\nsensei/\n├── migrations/\n│   ├── __init__.py\n│   ├── env.py\n│   ├── script.py.mako\n│   └── versions/\n│       ├── __init__.py\n│       └── 40de6c7e554f_initial_schema.py\n```\n\n### 2. Move files\n- `alembic/versions/40de6c7e554f_initial_schema.py` → `sensei/migrations/versions/`\n- `alembic/env.py` → `sensei/migrations/env.py` (adapt as needed)\n- `alembic/script.py.mako` → `sensei/migrations/script.py.mako`\n\n### 3. Update alembic.ini\n```ini\n[alembic]\nscript_location = sensei/migrations\n```\n\nThis keeps the CLI working for development (`alembic revision --autogenerate`).\n\n### 4. Update sensei/migrations/env.py\n```python\nfrom alembic import context\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import create_engine\n\nfrom sensei.database.models import Base\nfrom sensei.config import settings\n\ntarget_metadata = Base.metadata\n\ndef run_migrations_online():\n    # Use sync URL (no +asyncpg)\n    sync_url = settings.database_url.replace(\"+asyncpg\", \"\")\n    \n    connectable = create_engine(sync_url, poolclass=pool.NullPool)\n    \n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n        )\n        with context.begin_transaction():\n            context.run_migrations()\n\nrun_migrations_online()\n```\n\n### 5. Update pyproject.toml\n```toml\n[tool.setuptools.package-data]\nsensei = [\"migrations/*.py\", \"migrations/*.mako\", \"migrations/versions/*.py\"]\n```\n\n### 6. Delete old alembic/ directory\nAfter verifying everything works, remove the old `alembic/` directory (keep `alembic.ini` at root for dev CLI).\n\n**Why this works:**\n- `alembic.ini` at root → CLI works for development\n- Migrations in package → works when installed via `uvx`\n- Programmatic config in `local.py` uses `script_location = \"sensei:migrations\"` (colon notation for package)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:24:35.441879-05:00","updated_at":"2025-12-02T08:01:38.632441-05:00","closed_at":"2025-12-02T08:01:38.632441-05:00","dependencies":[{"issue_id":"sensei-7t4","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:24:35.443717-05:00","created_by":"daemon"},{"issue_id":"sensei-7t4","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:24:35.444464-05:00","created_by":"daemon"}]}
{"id":"sensei-7uw","title":"Update sub_agent.py to use Kura via MCP toolset","description":"Add create_kura_server() to sub-agent toolsets for consistency with main agent. Sub-agents should have same tool access patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.732217-05:00","updated_at":"2025-11-27T09:06:08.523371-05:00","closed_at":"2025-11-27T09:06:08.523371-05:00","dependencies":[{"issue_id":"sensei-7uw","depends_on_id":"sensei-x4g","type":"blocks","created_at":"2025-11-27T09:03:51.34974-05:00","created_by":"daemon"}]}
{"id":"sensei-7za","title":"Add FTS index on queries table for cache search","description":"**Current state (storage.py:131-183):**\n`search_queries` uses ILIKE with wildcards:\n```python\nconditions = \" AND \".join([f\"query ILIKE :term{i}\" for i in range(len(terms))])\nparams = {f\"term{i}\": f\"%{term}%\" for i, term in enumerate(terms)}\n\nsql = f\"\"\"\n    SELECT id, query, library, version, inserted_at\n    FROM queries\n    WHERE {conditions}\n    LIMIT :limit\n\"\"\"\n```\n\n**Problem:** `ILIKE '%term%'` cannot use indexes - it's a full table scan. As the queries table grows, cache search gets slower.\n\n**Fix:** Add tsvector column with GIN index for proper FTS:\n\n1. **Add to Query model:**\n```python\n# sensei/database/models.py\nclass Query(TimestampMixin, Base):\n    ...\n    search_vector = Column(\n        TSVECTOR,\n        Computed(\"to_tsvector('english', query)\", persisted=True),\n        nullable=True,\n    )\n```\n\n2. **Migration:**\n```python\ndef upgrade():\n    op.execute(\"\"\"\n        ALTER TABLE queries\n        ADD COLUMN search_vector tsvector\n        GENERATED ALWAYS AS (to_tsvector('english', query)) STORED\n    \"\"\")\n    op.execute(\"\"\"\n        CREATE INDEX idx_queries_search_vector\n        ON queries USING GIN(search_vector)\n    \"\"\")\n\ndef downgrade():\n    op.execute(\"DROP INDEX IF EXISTS idx_queries_search_vector\")\n    op.drop_column(\"queries\", \"search_vector\")\n```\n\n3. **Update search_queries:**\n```python\nasync def search_queries(terms: list[str], limit: int = 10) -\u003e list[Query]:\n    if not terms:\n        return []\n    \n    # Convert terms to tsquery\n    query_str = \" \u0026 \".join(terms)  # AND semantics\n    \n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(Query)\n            .where(Query.search_vector.op(\"@@\")(func.plainto_tsquery(\"english\", query_str)))\n            .limit(limit)\n        )\n        return list(result.scalars().all())\n```\n\n**Benefits:**\n- Index-backed search instead of table scan\n- Scales with table size\n- Supports stemming, ranking, etc.\n\n**Trade-off:**\n- Small storage overhead for tsvector column\n- Slightly different search semantics (FTS vs substring match)\n\n**Priority:** P3 - Cache search is not the hot path. Only implement if cache usage grows significantly.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T15:24:01.136428-05:00","updated_at":"2025-12-01T15:42:17.345074-05:00","closed_at":"2025-12-01T15:42:17.345074-05:00","labels":["cache","database","fts","performance"]}
{"id":"sensei-8ec","title":"Tome: Document migration strategy for TomeDocument schema","description":"models.py:64-76 uses Base.metadata.create_all() which doesn't handle schema evolution. Document that schema changes require manual migration or Alembic setup.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T11:44:12.648564-05:00","updated_at":"2025-11-27T13:39:08.377067-05:00","closed_at":"2025-11-27T13:39:08.377067-05:00","labels":["important","tome"]}
{"id":"sensei-8g4a","title":"Test was checking for removed 'subagent' keyword in prompt","description":"test_cache_prompt_includes_cache_instructions was asserting \"subagent\" in QUERY_DECOMPOSITION but the prompt was updated and no longer uses that term. Fixed by removing the assertion.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-20T07:24:13.96494-05:00","updated_at":"2025-12-20T07:28:41.922355-05:00","closed_at":"2025-12-20T07:28:41.922355-05:00","dependencies":[{"issue_id":"sensei-8g4a","depends_on_id":"sensei-lw13","type":"discovered-from","created_at":"2025-12-20T07:24:13.97162-05:00","created_by":"daemon"}]}
{"id":"sensei-8gqb","title":"Create initial YAML test datasets (input only)","description":"Create initial test case datasets in YAML format with minimal Goldens (input only).\n\n**Create:** `tests/eval/datasets/`\n- `fastapi.yaml` - 5-10 FastAPI test cases\n- `react.yaml` - 5-10 React test cases\n- `general.yaml` - 5-10 general programming questions\n\n**Example fastapi.yaml:**\n```yaml\nmetadata:\n  library: fastapi\n\ncases:\n  - id: fastapi-dependency-injection-01\n    query: \"How do I use dependency injection in FastAPI?\"\n    category: api-usage\n\n  - id: fastapi-background-tasks-01\n    query: \"How do I run background tasks in FastAPI?\"\n    category: api-usage\n\n  - id: fastapi-websockets-01\n    query: \"How do I implement WebSocket endpoints in FastAPI?\"\n    category: api-usage\n\n  - id: fastapi-testing-01\n    query: \"How do I write tests for FastAPI endpoints?\"\n    category: testing\n\n  - id: fastapi-auth-jwt-01\n    query: \"How do I implement JWT authentication in FastAPI?\"\n    category: authentication\n\n  - id: fastapi-middleware-01\n    query: \"How do I create custom middleware in FastAPI?\"\n    category: api-usage\n\n  - id: fastapi-validation-01\n    query: \"How do I add custom validation to Pydantic models in FastAPI?\"\n    category: validation\n```\n\n**Example react.yaml:**\n```yaml\nmetadata:\n  library: react\n\ncases:\n  - id: react-useeffect-01\n    query: \"How do I use useEffect for data fetching in React?\"\n    category: hooks\n\n  - id: react-context-01\n    query: \"How do I share state between components using React Context?\"\n    category: state-management\n\n  - id: react-suspense-01\n    query: \"How do I implement lazy loading with React Suspense?\"\n    category: performance\n\n  - id: react-forms-01\n    query: \"What's the best way to handle form state in React?\"\n    category: forms\n\n  - id: react-testing-01\n    query: \"How do I test React components with React Testing Library?\"\n    category: testing\n```\n\n**What changed from original design:**\n- No `expected_answer` field - not needed\n- No `difficulty` field - not used\n- Only `id`, `query`, and optional `category` for filtering\n- Tracing populates all runtime data (actual_output, tools_called, etc.)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:57.533283-05:00","updated_at":"2025-12-06T13:00:58.182124-05:00","closed_at":"2025-12-06T13:00:58.182124-05:00","labels":["eval","testing"],"dependencies":[{"issue_id":"sensei-8gqb","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.215807-05:00","created_by":"daemon"},{"issue_id":"sensei-8gqb","depends_on_id":"sensei-i8d4","type":"blocks","created_at":"2025-12-06T10:36:31.155181-05:00","created_by":"daemon"}]}
{"id":"sensei-8mm","title":"Remove Cloud Run artifacts","description":"Remove .gcloudignore and Procfile since we're switching to Fly.io. Keep Dockerfile but update it for Fly.io.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.62305-05:00","updated_at":"2025-11-30T05:57:26.548242-05:00","closed_at":"2025-11-30T05:57:26.548242-05:00","labels":["cleanup","deployment"],"dependencies":[{"issue_id":"sensei-8mm","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.861005-05:00","created_by":"daemon"}]}
{"id":"sensei-8mqm","title":"Implement Sentry error monitoring for production API","description":"Add Sentry error monitoring to capture exceptions in production. Sentry should only be active when SENTRY_DSN is set (Fly.io deployment), and be a no-op for local development or when others run our server.\n\n## Key Decisions\n\n1. **Init location**: Module-level in `sensei/api/__init__.py`, BEFORE FastAPI app creation\n2. **No manual integrations needed**: SDK auto-enables FastAPI, Starlette, httpx, asyncpg, SQLAlchemy, PydanticAI, MCP integrations\n3. **Async note**: For FastAPI/uvicorn, module-level init is correct (patches must happen before app creation)\n4. **MCP integration**: Already captures exceptions in tool handlers automatically via `sentry_sdk.integrations.mcp`\n\n## What Sentry captures automatically\n- Unhandled exceptions (500s)\n- Request context (URL, method, headers, body)\n- MCP tool/prompt/resource handler exceptions\n- Distributed traces across httpx, asyncpg, SQLAlchemy\n\n## What needs manual capture_exception()\nCatch blocks that handle errors gracefully (convert to HTTP responses) but should still be tracked:\n- API endpoints converting SenseiError → HTTPException\n- Tool wrapper converting errors → string responses\n- Crawler recording failures without re-raising","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-18T10:16:05.882909-05:00","updated_at":"2025-12-18T10:21:02.92448-05:00","closed_at":"2025-12-18T10:21:02.92448-05:00","labels":["infrastructure","observability","sentry"]}
{"id":"sensei-8no","title":"Remove debug logging from crawler.py","description":"Debug logging was added for link analysis during a debugging session:\n\n```python\n# Debug logging for link analysis\nlogger.debug(f\"=== Link analysis for {url} ===\")\nlogger.debug(f\"Same-domain links ({len(same_domain_links)}):\")\nfor link in same_domain_links:\n    logger.debug(f\"  ✓ {link}\")\nlogger.debug(f\"Other-domain links ({len(other_domain_links)}):\")\nfor link in other_domain_links:\n    logger.debug(f\"  ✗ {link}\")\n```\n\nThis should be removed or converted to a proper verbose/trace level option if needed for future debugging.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-01T13:01:31.137042-05:00","updated_at":"2025-12-02T12:43:34.48236-05:00","closed_at":"2025-12-02T12:43:34.48236-05:00"}
{"id":"sensei-8qn","title":"Create tome service layer with tome_get and tome_search","description":"Create the middle layer that exposes tome functionality to tools.\n\n**New file:** `sensei/tome/service.py`\n\n**Functions:**\n\n### `tome_get(domain: str, path: str) -\u003e str | None`\nGet document content by domain and path.\n\nSentinel values:\n- `\"INDEX\"` → `/llms.txt`\n- `\"FULL\"` → `/llms-full.txt`\n- Any other path → literal path lookup\n\n```python\nasync def tome_get(domain: str, path: str) -\u003e Success[str] | NoResults:\n    actual_path = {\n        \"INDEX\": \"/llms.txt\",\n        \"FULL\": \"/llms-full.txt\",\n    }.get(path, path)\n    \n    url = f\"https://{domain}{actual_path}\"\n    doc = await storage.get_document_by_url(url)\n    if doc:\n        return Success(doc.content)\n    return NoResults()\n```\n\n### `tome_search(domain: str, query: str, paths: list[str]) -\u003e list[SearchResult]`\nFull-text search within a domain, optionally filtered by path prefixes.\n\n```python\nasync def tome_search(\n    domain: str, \n    query: str, \n    paths: list[str],\n) -\u003e Success[list[SearchResult]] | NoResults:\n    results = await storage.search_documents_fts(domain, query, paths)\n    if results:\n        return Success(results)\n    return NoResults()\n```\n\n**Error handling:**\n- Domain not ingested → return NoResults (agent can try Context7)\n- Empty query → raise ToolError","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.00595-05:00","updated_at":"2025-12-01T08:53:41.332262-05:00","closed_at":"2025-12-01T08:53:41.332262-05:00","labels":["service","tome"],"dependencies":[{"issue_id":"sensei-8qn","depends_on_id":"sensei-7jw","type":"blocks","created_at":"2025-12-01T08:38:58.794598-05:00","created_by":"daemon"},{"issue_id":"sensei-8qn","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.720479-05:00","created_by":"daemon"}]}
{"id":"sensei-8t6h","title":"Simplified Database Lifecycle","description":"Remove sensei-managed PostgreSQL. Make all database operations explicit.\n\nSee design doc: docs/plans/2025-01-21-simplified-database-lifecycle.md","acceptance_criteria":"- DATABASE_URL is required (no default)\n- No PostgreSQL lifecycle management (no init_db, start, stop)\n- No auto-migration on startup\n- Sub-servers (kura, tome) have no lifespans\n- Unified lifespan only does dispose_engine()\n- All tests pass\n- Documentation updated","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T09:44:30.619939-05:00","updated_at":"2025-12-21T09:57:58.420707-05:00","closed_at":"2025-12-21T09:57:58.420707-05:00"}
{"id":"sensei-97ch","title":"Research beautiful portable shell script frameworks","description":"The OpenCode plugin install script is plain POSIX sh. Research frameworks/patterns for making it beautiful while staying portable.\n\nIdeas to explore:\n- Colored output (tput, ANSI codes)\n- Progress indicators\n- Fancy banners (figlet-style but inline)\n- Error handling with nice messages\n- Frameworks like charm.sh/gum (but adds dependency)\n\nGoal: Improve UX without sacrificing single-file portability or requiring dependencies beyond curl/sh.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-17T16:48:15.972163-05:00","updated_at":"2025-12-17T16:48:15.972163-05:00","dependencies":[{"issue_id":"sensei-97ch","depends_on_id":"sensei-gy0d","type":"parent-child","created_at":"2025-12-17T16:48:23.099951-05:00","created_by":"daemon"}]}
{"id":"sensei-9bui","title":"Message parser for trace data (OPTIONAL - may not be needed)","description":"**UPDATE:** With DeepEval's native PydanticAI integration via `ConfidentInstrumentationSettings`, this module **may not be needed**. DeepEval auto-captures `input`, `output`, and `tools_called`.\n\nKeep this bead **on hold** until we verify what the native integration captures. If it captures everything we need, close this bead.\n\n---\n\n## Original Purpose (if still needed)\n\nCreate message_parser.py to extract trace data from agent messages.\n\n**Create:** `sensei/eval/message_parser.py`\n\n```python\n\"\"\"Extract trace data from PydanticAI agent messages.\n\nNOTE: This may not be needed if DeepEval's ConfidentInstrumentationSettings\nauto-captures tools_called from PydanticAI agents.\n\"\"\"\n\nfrom deepeval.test_case import ToolCall\n\n\ndef extract_trace_data(messages: list[dict] | None) -\u003e dict:\n    \"\"\"Extract tools_called and retrieval_context from agent messages.\"\"\"\n    if not messages:\n        return {\"tools_called\": [], \"retrieval_context\": []}\n    \n    tools_called = []\n    retrieval_context = []\n    \n    for msg in messages:\n        parts = msg.get(\"parts\", [])\n        for part in parts:\n            part_kind = part.get(\"part_kind\")\n            \n            if part_kind == \"tool-call\":\n                tools_called.append(ToolCall(\n                    name=part.get(\"tool_name\", \"\"),\n                    input_parameters=part.get(\"args\", {}),\n                ))\n            elif part_kind == \"tool-return\":\n                content = part.get(\"content\")\n                if content and isinstance(content, str):\n                    retrieval_context.append(content)\n    \n    return {\n        \"tools_called\": tools_called,\n        \"retrieval_context\": retrieval_context,\n    }\n```\n\n## Status\n\n**ON HOLD** - Verify native integration first","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:13.29676-05:00","updated_at":"2025-12-06T12:25:46.8469-05:00","closed_at":"2025-12-06T12:25:46.8469-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-9bui","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:15.971436-05:00","created_by":"daemon"},{"issue_id":"sensei-9bui","depends_on_id":"sensei-ip47","type":"blocks","created_at":"2025-12-06T10:36:30.845759-05:00","created_by":"daemon"}]}
{"id":"sensei-9hb","title":"Fix search_cache_tool test mock - expects string but gets list","description":"**Test:** `test_search_cache_tool` in `tests/test_cache.py:78`\n\n**Issue:** Test mocks `search_queries` expecting a string argument:\n```python\nmock_search.assert_called_once_with(\"React hooks\", limit=5)\n```\n\nBut actual implementation passes a list:\n```python\nsearch_queries(['React', 'hooks'], limit=5)\n```\n\n**Fix:** Update the mock expectation to match the actual behavior.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-01T13:34:12.904674-05:00","updated_at":"2025-12-01T13:35:02.80021-05:00","closed_at":"2025-12-01T13:35:02.80021-05:00"}
{"id":"sensei-9lu","title":"Update tome MCP server with new tools","description":"Update server.py to expose new API and update existing tools.\n\n**Update existing:**\n- `tome_get` - add optional `heading` parameter\n- `tome_search` - return heading_path in results\n\n**Add new:**\n- `tome_toc` - expose table of contents\n\n**Tool definitions:**\n\n```python\n@server.tool()\nasync def tome_toc(domain: str, path: str) -\u003e str:\n    \\\"\\\"\\\"Get table of contents for a document.\n    \n    Returns the heading structure so you can navigate to specific sections.\n    Use with tome_get(domain, path, heading) to retrieve specific sections.\n    \\\"\\\"\\\"\n\n@server.tool()\nasync def tome_get(\n    domain: str, \n    path: str, \n    heading: str | None = None\n) -\u003e str:\n    \\\"\\\"\\\"Get documentation content.\n    \n    Args:\n        domain: The domain (e.g., \"react.dev\")\n        path: Document path, or \"INDEX\"/\"FULL\" sentinels\n        heading: Optional - get only this section and its children\n    \n    Returns:\n        Full document content, or specific section if heading provided.\n    \\\"\\\"\\\"\n```","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-01T09:57:15.005994-05:00","updated_at":"2025-12-01T09:57:15.005994-05:00","labels":["mcp","tome"],"dependencies":[{"issue_id":"sensei-9lu","depends_on_id":"sensei-m7g","type":"blocks","created_at":"2025-12-01T09:57:15.008075-05:00","created_by":"daemon"},{"issue_id":"sensei-9lu","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.88693-05:00","created_by":"daemon"}]}
{"id":"sensei-9miy","title":"Add rate limiting via slowapi to Python API","description":"Add slowapi for IP-based rate limiting on REST endpoints. Currently 3 req/min on /api/chat. In-memory storage for MVP, can upgrade to Redis later.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T18:30:56.737745-05:00","updated_at":"2025-12-17T18:31:11.826189-05:00","closed_at":"2025-12-17T18:31:11.826189-05:00","dependencies":[{"issue_id":"sensei-9miy","depends_on_id":"sensei-5t3e","type":"parent-child","created_at":"2025-12-17T18:30:56.738781-05:00","created_by":"daemon"}]}
{"id":"sensei-9pf","title":"Build Dojo: DSPy prompt optimization from feedback","description":"DSPy plugin that optimizes Sensei's prompts using collected user feedback ratings. Uses MIPROv2 optimizer with 4-5 star rated examples as training data.","design":"## Stack\n- DSPy framework\n- MIPROv2 optimizer (joint instruction + few-shot optimization)\n\n## Input\n- User feedback ratings from existing ratings table\n- Filter to 4-5 star examples\n- Need 50-100 diverse examples\n\n## Output\n- Optimized system prompts\n- Curated few-shot examples\n\n## Integration\n- Periodic retraining when new feedback accumulates\n- Save optimized prompts for production use\n\n## Status: Stub for now\nCreate folder with README describing objective. Build after Tome.","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-11-27T11:12:59.177268-05:00","updated_at":"2025-11-27T16:00:28.806615-05:00","closed_at":"2025-11-27T16:00:28.806615-05:00"}
{"id":"sensei-9v0","title":"Add crawlee dependency to pyproject.toml","description":"Add crawlee[httpx] to dependencies in pyproject.toml","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:06.395559-05:00","updated_at":"2025-11-27T11:26:10.763828-05:00","closed_at":"2025-11-27T11:26:10.763828-05:00","dependencies":[{"issue_id":"sensei-9v0","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:06.39726-05:00","created_by":"daemon"}]}
{"id":"sensei-9xe","title":"Tome: Add timeout configuration to HttpCrawler","description":"crawler.py:51-53 HttpCrawler created without timeout settings. Could hang indefinitely on slow/dead servers. Add request_handler_timeout_secs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:26.124788-05:00","updated_at":"2025-11-27T13:42:05.769835-05:00","closed_at":"2025-11-27T13:42:05.769835-05:00","labels":["important","tome"]}
{"id":"sensei-afa","title":"Prepare pyproject.toml for PyPI publishing","description":"Add required metadata: authors, license, classifiers, urls, etc.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.330399-05:00","updated_at":"2025-11-27T06:20:07.826302-05:00","closed_at":"2025-11-27T06:20:07.826302-05:00","dependencies":[{"issue_id":"sensei-afa","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.331062-05:00","created_by":"daemon"}]}
{"id":"sensei-agi","title":"Add ensure_db_ready() to MCP server lifespans","description":"Add database initialization to MCP server startup paths.\n\n**Key insight:** Services that USE the database should call `ensure_db_ready()` in their lifespan. The function is idempotent, so multiple calls are safe.\n\n**Service matrix:**\n| Service | Needs DB? | Add lifespan? |\n|---------|-----------|---------------|\n| Scout | No | No |\n| Kura | Yes | Yes |\n| Tome | Yes | Yes |\n| Combined HTTP | Yes | Yes (calls before nesting) |\n\n**Files to update:**\n\n### 1. sensei/scout/server.py\n**NO CHANGES** - Scout doesn't use the database.\n\n### 2. sensei/kura/server.py\n```python\nfrom contextlib import asynccontextmanager\nfrom fastmcp import FastMCP\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\nkura = FastMCP(name=\"kura\", lifespan=lifespan)\n```\n\n### 3. sensei/tome/server.py\n```python\nfrom contextlib import asynccontextmanager\nfrom fastmcp import FastMCP\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\ntome = FastMCP(name=\"tome\", lifespan=lifespan)\n```\n\n### 4. sensei/__main__.py\n```python\n@asynccontextmanager\nasync def combined_lifespan(app: FastAPI):\n    # Ensure database is ready (idempotent - will be called again in kura/tome lifespans but will no-op)\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    \n    async with mcp_app.lifespan(app):\n        async with scout_mcp_app.lifespan(app):\n            async with kura_mcp_app.lifespan(app):\n                async with tome_mcp_app.lifespan(app):\n                    yield\n```\n\n**Why call in both combined AND individual lifespans?**\n- Standalone stdio (`uvx --from sensei kura`): Only individual lifespan runs\n- Combined HTTP (`uvx sensei`): Combined lifespan runs first, individual lifespans no-op\n\n**Idempotency guarantee:** `ensure_db_ready()` uses a module-level flag to skip work on subsequent calls within the same process.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:11:08.460139-05:00","updated_at":"2025-12-02T08:04:14.992287-05:00","closed_at":"2025-12-02T08:04:14.992287-05:00","dependencies":[{"issue_id":"sensei-agi","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:08.461179-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:11:21.240209-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-gvh","type":"blocks","created_at":"2025-12-02T06:11:21.283815-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-7t4","type":"blocks","created_at":"2025-12-02T06:24:41.868741-05:00","created_by":"daemon"}]}
{"id":"sensei-aky","title":"Refactor exec_plan tools to use wrap_tool and proper result types","description":"tools/exec_plan.py returns error strings directly instead of using the established pattern:\n- Core functions should raise ToolError for errors, return Success[str] for success\n- wrap_tool decorator converts rich types to strings at PydanticAI boundary\n\nCurrent (lines 83-113):\n```python\nasync def add_exec_plan(ctx: RunContext[Deps]) -\u003e str:\n    if not ctx.deps or not ctx.deps.query_id:\n        return \"Error: missing query_id; cannot create ExecPlan.\"  # BAD\n    ...\n    return \"ExecPlan template added...\"\n```\n\nShould be:\n```python\nasync def _add_exec_plan(ctx: RunContext[Deps]) -\u003e Success[str]:\n    if not ctx.deps or not ctx.deps.query_id:\n        raise ToolError(\"Missing query_id; cannot create ExecPlan.\")\n    ...\n    return Success(\"ExecPlan template added...\")\n\nadd_exec_plan = wrap_tool(_add_exec_plan)\n```\n\nFiles affected:\n- sensei/tools/exec_plan.py\n- sensei/tools/common.py (wrap_tool will be used)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:14:46.602674-05:00","updated_at":"2025-11-30T07:55:27.570545-05:00","closed_at":"2025-11-30T07:55:27.570545-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-anj","title":"Create Procfile for Cloud Run","description":"Add Procfile with: web: uvicorn sensei.__main__:app --host 0.0.0.0 --port $PORT","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.789268-05:00","updated_at":"2025-11-27T09:16:41.222625-05:00","closed_at":"2025-11-27T09:16:41.222625-05:00","dependencies":[{"issue_id":"sensei-anj","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.78984-05:00","created_by":"daemon"}]}
{"id":"sensei-apn","title":"Initialize packages/www directory and package.json","description":"Create the www package directory and configure package.json with React Router v7 Framework Mode dependencies","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.022327-05:00","updated_at":"2025-11-27T13:27:48.360399-05:00","closed_at":"2025-11-27T13:27:48.360399-05:00","dependencies":[{"issue_id":"sensei-apn","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.024215-05:00","created_by":"daemon"}]}
{"id":"sensei-aqok","title":"Add capture_exception() to API endpoint catch blocks","description":"Add `sentry_sdk.capture_exception()` to catch blocks in `sensei/api/__init__.py` that convert exceptions to HTTP responses.\n\n## Locations to instrument\n\n1. **POST /query** (lines 103-114) - catch blocks for BrokenInvariant, TransientError, ToolError, ModelHTTPError\n2. **POST /query/stream** (line 251) - catch block for ModelHTTPError  \n3. **POST /rate** (line 293) - catch block for generic Exception\n\n## Pattern\n\n```python\nexcept SomeError as e:\n    sentry_sdk.capture_exception(e)  # ADD THIS\n    logger.error(...)\n    raise HTTPException(...)\n```\n\n## Do NOT instrument\n- `ValidationError` (line 148) - user input error, not our bug\n- `json.JSONDecodeError` in `_extract_prompt_from_vercel_body` (line 133) - expected parsing failure","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-18T10:16:54.702805-05:00","updated_at":"2025-12-18T10:21:02.542519-05:00","closed_at":"2025-12-18T10:21:02.542519-05:00","dependencies":[{"issue_id":"sensei-aqok","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:54.704339-05:00","created_by":"daemon"}]}
{"id":"sensei-asv","title":"Add tests for database lifecycle management","description":"Add unit tests for the new database lifecycle code. **No integration tests** - only unit tests with mocking.\n\n**Files to create:** `tests/test_paths.py`, `tests/test_database_local.py`\n\n### test_paths.py\n```python\nimport os\nfrom pathlib import Path\nfrom unittest.mock import patch\n\ndef test_get_sensei_home_default():\n    \"\"\"Returns ~/.sensei when SENSEI_HOME not set.\"\"\"\n    with patch.dict(os.environ, {}, clear=True):\n        # Need to reload module to pick up env change\n        from sensei import paths\n        import importlib\n        importlib.reload(paths)\n        assert paths.get_sensei_home() == Path.home() / \".sensei\"\n\ndef test_get_sensei_home_from_env(tmp_path):\n    \"\"\"Respects SENSEI_HOME env var.\"\"\"\n    with patch.dict(os.environ, {\"SENSEI_HOME\": str(tmp_path)}):\n        from sensei import paths\n        import importlib\n        importlib.reload(paths)\n        assert paths.get_sensei_home() == tmp_path\n\ndef test_get_pgdata():\n    \"\"\"Returns sensei_home/pgdata.\"\"\"\n    from sensei.paths import get_sensei_home, get_pgdata\n    assert get_pgdata() == get_sensei_home() / \"pgdata\"\n\ndef test_get_scout_repos():\n    \"\"\"Returns sensei_home/scout/repos.\"\"\"\n    from sensei.paths import get_sensei_home, get_scout_repos\n    assert get_scout_repos() == get_sensei_home() / \"scout\" / \"repos\"\n\ndef test_get_local_database_url():\n    \"\"\"Returns Unix socket URL pointing to pgdata.\"\"\"\n    from sensei.paths import get_pgdata, get_local_database_url\n    url = get_local_database_url()\n    assert url.startswith(\"postgresql+asyncpg:///sensei?host=\")\n    assert str(get_pgdata()) in url\n```\n\n### test_database_local.py\n```python\nimport shutil\nfrom unittest.mock import patch, MagicMock\n\ndef test_check_postgres_installed_true():\n    \"\"\"Returns True when pg_ctl found.\"\"\"\n    with patch.object(shutil, \"which\", return_value=\"/usr/bin/pg_ctl\"):\n        from sensei.database.local import check_postgres_installed\n        assert check_postgres_installed() is True\n\ndef test_check_postgres_not_installed():\n    \"\"\"Returns False when pg_ctl not found.\"\"\"\n    with patch.object(shutil, \"which\", return_value=None):\n        from sensei.database.local import check_postgres_installed\n        assert check_postgres_installed() is False\n\ndef test_is_initialized_false_when_no_pgdata(tmp_path):\n    \"\"\"Returns False when PG_VERSION doesn't exist.\"\"\"\n    with patch(\"sensei.database.local.get_pgdata\", return_value=tmp_path / \"pgdata\"):\n        from sensei.database.local import is_initialized\n        assert is_initialized() is False\n\ndef test_is_initialized_true_when_pg_version_exists(tmp_path):\n    \"\"\"Returns True when PG_VERSION exists.\"\"\"\n    pgdata = tmp_path / \"pgdata\"\n    pgdata.mkdir()\n    (pgdata / \"PG_VERSION\").write_text(\"17\")\n    \n    with patch(\"sensei.database.local.get_pgdata\", return_value=pgdata):\n        from sensei.database.local import is_initialized\n        assert is_initialized() is True\n\n@pytest.mark.asyncio\nasync def test_ensure_db_ready_idempotent():\n    \"\"\"Second call is a no-op due to _db_ready flag.\"\"\"\n    import sensei.database.local as local\n    \n    # Reset the flag\n    local._db_ready = False\n    \n    with patch.object(local, \"check_postgres_installed\", return_value=True), \\\n         patch.object(local, \"is_initialized\", return_value=True), \\\n         patch.object(local, \"is_running\", return_value=True), \\\n         patch.object(local, \"ensure_migrated\", new_callable=MagicMock) as mock_migrate, \\\n         patch(\"sensei.database.local.settings\") as mock_settings:\n        \n        mock_settings.is_external_database = False\n        mock_migrate.return_value = None  # async mock\n        \n        # First call does work\n        await local.ensure_db_ready()\n        assert local._db_ready is True\n        \n        # Second call skips\n        mock_migrate.reset_mock()\n        await local.ensure_db_ready()\n        mock_migrate.assert_not_called()\n```\n\n**No integration tests** - we don't test actual PostgreSQL startup/shutdown. Unit tests with mocking are sufficient.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T06:11:35.024518-05:00","updated_at":"2025-12-02T08:05:52.904367-05:00","closed_at":"2025-12-02T08:05:52.904367-05:00","dependencies":[{"issue_id":"sensei-asv","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:35.033361-05:00","created_by":"daemon"},{"issue_id":"sensei-asv","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:11:40.061786-05:00","created_by":"daemon"},{"issue_id":"sensei-asv","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:11:40.093271-05:00","created_by":"daemon"}]}
{"id":"sensei-au8","title":"Create sensei/tome/ module structure","description":"Create sensei/tome/ with __init__.py, crawler.py, parser.py. Basic module structure for Tome.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:03.087901-05:00","updated_at":"2025-11-27T11:26:10.723597-05:00","closed_at":"2025-11-27T11:26:10.723597-05:00","dependencies":[{"issue_id":"sensei-au8","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:03.089268-05:00","created_by":"daemon"}]}
{"id":"sensei-ax9v","title":"Rewrite tests/eval/test_regression.py for pydantic-evals","description":"Replace DeepEval's `EvaluationDataset.evals_iterator()` with pydantic-evals:\n- Load datasets via `Dataset.from_file()`\n- Run `dataset.evaluate_sync(run_agent)`\n- Parametrize by dataset file for pytest -k filtering\n- Print/assert on report results","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:23.219182-05:00","updated_at":"2025-12-12T11:41:51.343556-05:00","closed_at":"2025-12-12T11:41:51.343556-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-ax9v","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.232661-05:00","created_by":"daemon"},{"issue_id":"sensei-ax9v","depends_on_id":"sensei-uuli","type":"blocks","created_at":"2025-12-12T09:42:43.014891-05:00","created_by":"daemon"},{"issue_id":"sensei-ax9v","depends_on_id":"sensei-jq6z","type":"blocks","created_at":"2025-12-12T09:42:43.05757-05:00","created_by":"daemon"},{"issue_id":"sensei-ax9v","depends_on_id":"sensei-b42t","type":"blocks","created_at":"2025-12-12T09:42:43.101022-05:00","created_by":"daemon"}]}
{"id":"sensei-axt2","title":"Migrate from DeepEval to pydantic-evals","description":"Replace DeepEval evaluation infrastructure with native pydantic-evals. This gives us: SpanTree API for trace-based evaluation, native PydanticAI integration, simpler codebase (delete ~300 line workaround), and access to tool outputs + reasoning tokens.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-12T09:42:00.484435-05:00","updated_at":"2025-12-12T11:41:51.805821-05:00","closed_at":"2025-12-12T11:41:51.805821-05:00","labels":["eval","infrastructure"]}
{"id":"sensei-azo","title":"Improve instructions for calling agent (Claude Code)","description":"Improve the instructions that tell Claude Code how to invoke Sensei. Currently in packages/marketplace/sensei/README.md. The calling agent needs to provide rich context (the problem/outcome, tech stack, constraints) so Sensei can do effective research. A weak question leads to weak research.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T22:15:17.863029-05:00","updated_at":"2025-12-18T12:11:34.702463-05:00","closed_at":"2025-12-18T12:11:34.702463-05:00"}
{"id":"sensei-b42t","title":"Rewrite tests/eval/conftest.py with logfire config","description":"Set up pytest fixtures for pydantic-evals:\n- Session-scoped fixture: `logfire.configure(send_to_logfire=False)` for local trace capture\n- Dataset loading fixtures as needed","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:23.155237-05:00","updated_at":"2025-12-12T11:41:51.259524-05:00","closed_at":"2025-12-12T11:41:51.259524-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-b42t","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.188545-05:00","created_by":"daemon"}]}
{"id":"sensei-b61g","title":"Make DATABASE_URL required in settings.py","description":"Remove the default database URL and the is_external_database property.\n\n**File:** `sensei/settings.py`\n\n**Changes:**\n1. Remove import of `get_local_database_url` from paths\n2. Change `database_url` field to have no default (required)\n3. Remove `is_external_database` property entirely","design":"```python\n# Before\nfrom sensei.paths import get_local_database_url\n\nclass SenseiSettings(BaseSettings):\n    database_url: str = Field(default_factory=get_local_database_url)\n    \n    @property\n    def is_external_database(self) -\u003e bool:\n        return self.database_url != get_local_database_url()\n\n# After\nclass SenseiSettings(BaseSettings):\n    database_url: str = Field(description=\"PostgreSQL connection URL\")\n    # No default, no is_external_database property\n```","acceptance_criteria":"- DATABASE_URL is required (no default_factory)\n- is_external_database property removed\n- No import of get_local_database_url\n- Pydantic raises error if DATABASE_URL not set","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.403077-05:00","updated_at":"2025-12-21T09:57:11.627442-05:00","closed_at":"2025-12-21T09:57:11.627442-05:00","dependencies":[{"issue_id":"sensei-b61g","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.406486-05:00","created_by":"daemon"},{"issue_id":"sensei-b61g","depends_on_id":"sensei-tueu","type":"blocks","created_at":"2025-12-21T09:46:22.967477-05:00","created_by":"daemon"}]}
{"id":"sensei-b8qb","title":"Add kura MCP server to plugin.json","description":"The sensei agent markdown references Kura (knowledge cache) extensively:\n\u003e \"The cache (Kura) stores past research as searchable building blocks...\"\n\nBut kura is not exposed in the plugin.json. Users will see the agent talking about a cache they can't access.\n\nAdd to `packages/sensei-claude-code/.claude-plugin/plugin.json`:\n```json\n\"kura\": {\n  \"args\": [\"--from\", \"sensei-ai\", \"kura\"],\n  \"command\": \"uvx\",\n  \"type\": \"stdio\"\n}\n```\n\nKura provides:\n- `search` - Full-text search across cached queries\n- `get` - Retrieve a full cached response by ID","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-17T12:19:16.839448-05:00","updated_at":"2025-12-17T12:19:16.839448-05:00"}
{"id":"sensei-b9k","title":"Fix test_cache_search_and_retrieve_flow integration test","description":"The test_cache_integration.py::test_cache_search_and_retrieve_flow test is failing because search_cache returns NoResults even though a query was just saved. This appears to be a pre-existing issue with FTS search not finding recently saved queries.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-30T09:49:27.618471-05:00","updated_at":"2025-12-01T07:30:18.433911-05:00","closed_at":"2025-12-01T07:30:18.433911-05:00","dependencies":[{"issue_id":"sensei-b9k","depends_on_id":"sensei-vrp","type":"discovered-from","created_at":"2025-11-30T09:49:27.62088-05:00","created_by":"daemon"}]}
{"id":"sensei-ben","title":"Add composite index on documents(domain, path) WHERE active","description":"**Current indexes on documents:**\n- `domain` - single column index\n- `url` - unique constraint (implicit index)\n- `idx_documents_domain_active` - partial index on (domain) WHERE generation_active=true\n\n**Problem:** Multiple functions filter by `(domain, path, generation_active=true)`:\n- `get_sections_by_document`\n- `get_section_subtree_by_heading`\n- `get_sections_for_toc`\n\nThe current partial index only covers `domain`. After filtering by domain, PostgreSQL still needs to scan for matching `path`.\n\n**Fix:** Create composite partial index:\n```sql\nCREATE INDEX idx_documents_domain_path_active\nON documents (domain, path)\nWHERE generation_active = true\n```\n\n**Migration:**\n```python\ndef upgrade():\n    op.execute(\"\"\"\n        CREATE INDEX idx_documents_domain_path_active\n        ON documents (domain, path)\n        WHERE generation_active = true\n    \"\"\")\n    # Optionally drop the old single-column partial index\n    # op.execute(\"DROP INDEX IF EXISTS idx_documents_domain_active\")\n\ndef downgrade():\n    op.execute(\"DROP INDEX IF EXISTS idx_documents_domain_path_active\")\n```\n\n**Benefits:**\n- Faster lookups for (domain, path) combination\n- Index-only scans possible for document existence checks\n- Partial index keeps it small (only active docs)\n\n**Note:** Evaluate whether to keep or drop `idx_documents_domain_active` - it may still be useful for domain-only queries like `search_sections_fts` and `cleanup_old_generations`.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:24:00.384681-05:00","updated_at":"2025-12-01T15:36:46.531948-05:00","closed_at":"2025-12-01T15:36:46.531948-05:00","labels":["database","index","performance"]}
{"id":"sensei-bff","title":"Fix is_markdown_content test - test expects None to return True but function returns False","description":"**Test:** `test_is_markdown_content_accepts_none` in `tests/test_tome_parser.py:213`\n\n**Issue:** Test asserts `is_markdown_content(None)` should return `True` with comment \"trust the URL\", but the function in `sensei/tome/crawler.py` returns `False` for `None`:\n\n```python\ndef is_markdown_content(content_type: str | None) -\u003e bool:\n    if not content_type:\n        return False  # \u003c-- returns False for None\n```\n\n**Decision needed:** Should we:\n1. Trust the URL when content-type is missing (change function to return True for None)\n2. Not trust, require explicit content-type (fix the test)\n\nCurrently the crawler skips documents with no content-type header, which may be overly restrictive.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-01T13:34:12.844313-05:00","updated_at":"2025-12-01T14:01:55.528723-05:00","closed_at":"2025-12-01T14:01:55.528723-05:00"}
{"id":"sensei-bgr","title":"Decide on created_at server_default strategy","description":"Migration has created_at columns without server_default. Need to decide: (1) Use server_default=sa.func.now() so DB handles timestamps even for raw SQL inserts, (2) Keep Python-only default and ensure all inserts go through ORM, (3) Use timezone-aware now() function. Also consider: should models.py Column definitions match migration server_defaults?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T16:10:31.669066-05:00","updated_at":"2025-11-30T08:39:17.995198-05:00","closed_at":"2025-11-30T08:39:17.995198-05:00","labels":["database","migrations"]}
{"id":"sensei-bjqo","title":"Export main() from sensei/kura/__init__.py","description":"Add main to __all__ exports so users can import it: `from sensei.kura import main`","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:36.358198-05:00","updated_at":"2025-12-02T12:37:14.657873-05:00","closed_at":"2025-12-02T12:37:14.657873-05:00","dependencies":[{"issue_id":"sensei-bjqo","depends_on_id":"sensei-5r2g","type":"blocks","created_at":"2025-12-02T12:34:36.360906-05:00","created_by":"daemon"}]}
{"id":"sensei-bm55","title":"Optimize feedback template to maximize response rate","description":"Research and implement best practices for feedback prompts that encourage higher user response rates. Consider timing, wording, friction reduction, and incentive structures.","design":"## Current State\n\n```\n---\n**Help improve sensei:** Rate this response using `feedback` tool after trying it.\n\nQuery ID: `{query_id}`\n```\n\n**Problems:**\n1. Generic CTA (\"Help improve sensei\") - no immediate user benefit\n2. Requires remembering tool name (\"feedback tool\")\n3. Asks user to \"try it first\" - adds delay/friction\n4. Query ID display is technical/unfriendly\n5. No sense of how quick/easy it is\n\n## Design Principles\n\nBased on response rate research:\n\n1. **Reduce friction** - Make it one action, not multi-step\n2. **Specificity over generality** - \"Was this helpful?\" beats \"Give us feedback\"\n3. **Reciprocity** - User just got value, strike while iron is hot\n4. **Binary first** - Yes/no easier than 1-5 scale initially\n5. **Show brevity** - Signal it takes \u003c5 seconds\n6. **Hide technical details** - Query ID should be invisible or minimal\n\n## Proposed Template Options\n\n### Option A: Binary with upgrade path\n```\n---\n👍 Helpful? Run: `sensei rate {short_id} --helpful`\n```\nIf they engage, follow-up tool can ask for more detail.\n\n### Option B: Quick scale\n```\n---\nRate (1-5): `sensei rate {short_id} 5`\n```\n\n### Option C: Inline sentiment\n```\n---\nWas this helpful? [👍 yes] [👎 no] → `sensei rate {short_id}`\n```\n\n## Implementation Considerations\n\n- Short IDs (e.g., `q7x`) instead of UUIDs reduce cognitive load\n- Consider if MCP/API clients can render interactive buttons\n- Track which template variant gets higher response rates (A/B test infrastructure?)\n- Response rate baseline needed before optimization","acceptance_criteria":"## Acceptance Criteria\n\n- [ ] Template text is ≤50 characters (excluding query ID)\n- [ ] Query ID is shortened or hidden (not full UUID)\n- [ ] CTA is action-oriented (\"Rate this\" not \"Help us\")\n- [ ] No multi-step instructions (one command/action)\n- [ ] Baseline response rate measured before change\n- [ ] Response rate tracked after change\n- [ ] A/B test infrastructure exists OR single best variant chosen with rationale","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-18T07:42:25.911211-05:00","updated_at":"2025-12-18T07:43:13.57228-05:00","labels":["feedback","ux"]}
{"id":"sensei-bqrs","title":"Remove query_id and parent_id from Deps","description":"Remove vestigial query_id and parent_id fields from Deps. Update build_deps() to no longer create/check these. Keep current_depth for recursion tracking.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:22:45.143962-05:00","updated_at":"2025-12-18T07:31:09.026101-05:00","closed_at":"2025-12-18T07:31:09.026101-05:00"}
{"id":"sensei-bsf9","title":"Export main() from sensei/tome/__init__.py","description":"Add main to __all__ exports so users can import it: `from sensei.tome import main`","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:36.47031-05:00","updated_at":"2025-12-02T12:37:14.792133-05:00","closed_at":"2025-12-02T12:37:14.792133-05:00","dependencies":[{"issue_id":"sensei-bsf9","depends_on_id":"sensei-32xv","type":"blocks","created_at":"2025-12-02T12:34:36.471787-05:00","created_by":"daemon"}]}
{"id":"sensei-bti","title":"Extract cache into kura module","description":"Create sensei/kura/ module mirroring Scout's structure. Extract existing cache functionality from sensei/tools/cache.py. Connect to same SQLite database as main Sensei app.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T08:55:29.016538-05:00","updated_at":"2025-11-27T08:57:25.865367-05:00","closed_at":"2025-11-27T08:57:25.865367-05:00"}
{"id":"sensei-c285","title":"Implement agentic metrics + G-Eval configuration","description":"Create metrics.py with agentic metrics and custom G-Eval for documentation quality.\n\n**Create:** `sensei/eval/metrics.py`\n\n```python\n\"\"\"DeepEval metrics configuration for Sensei evaluation.\n\nMetrics:\n- Task Completion: Did the agent accomplish the research task?\n- Argument Correctness: Were tool arguments appropriate?\n- Step Efficiency: Was execution efficient?\n- G-Eval (Doc Quality): Custom criteria for documentation responses\n\"\"\"\n\nfrom deepeval.metrics import (\n    TaskCompletionMetric,\n    ArgumentCorrectnessMetric,\n    StepEfficiencyMetric,\n    GEval,\n)\nfrom deepeval.metrics.g_eval import Rubric\nfrom deepeval.test_case import LLMTestCaseParams\nfrom deepeval.models import DeepEvalBaseLLM\n\n\ndef get_documentation_quality_metric(model: DeepEvalBaseLLM) -\u003e GEval:\n    \"\"\"\n    G-Eval metric for documentation-specific quality criteria.\n    \n    Evaluates responses without requiring expected_output (referenceless).\n    \"\"\"\n    return GEval(\n        name=\"Documentation Quality\",\n        evaluation_steps=[\n            \"Check if the response directly addresses the user's question with specific, actionable information\",\n            \"Verify any code examples are syntactically correct, complete, and follow best practices\",\n            \"Assess whether the response cites or references its sources (documentation, APIs, etc.)\",\n            \"Check if confidence levels are communicated appropriately (high/medium/low based on source quality)\",\n            \"Penalize vague statements, hedging without substance, or responses that fail to provide concrete guidance\",\n            \"Evaluate whether the response would help a developer implement or understand the topic\",\n        ],\n        evaluation_params=[\n            LLMTestCaseParams.INPUT,\n            LLMTestCaseParams.ACTUAL_OUTPUT,\n        ],\n        model=model,\n        threshold=0.7,\n        include_reason=True,\n        rubric=[\n            Rubric(score_range=(0, 2), expected_outcome=\"Response is vague, incorrect, or unhelpful\"),\n            Rubric(score_range=(3, 4), expected_outcome=\"Response addresses the question but lacks specificity or has issues\"),\n            Rubric(score_range=(5, 6), expected_outcome=\"Response is helpful but could be more complete or precise\"),\n            Rubric(score_range=(7, 8), expected_outcome=\"Response is accurate, actionable, and well-sourced\"),\n            Rubric(score_range=(9, 10), expected_outcome=\"Excellent response with clear guidance, working examples, and proper attribution\"),\n        ],\n    )\n\n\ndef get_metrics(model: DeepEvalBaseLLM, thresholds: dict[str, float] | None = None) -\u003e list:\n    \"\"\"\n    Return configured metrics for Sensei evaluation.\n    \n    All metrics except G-Eval require tracing to be set up.\n    \"\"\"\n    t = thresholds or {}\n    \n    return [\n        # Agentic metrics (require tracing)\n        TaskCompletionMetric(\n            threshold=t.get(\"task_completion\", 0.7),\n            model=model,\n            include_reason=True,\n        ),\n        ArgumentCorrectnessMetric(\n            threshold=t.get(\"argument_correctness\", 0.7),\n            model=model,\n            include_reason=True,\n        ),\n        StepEfficiencyMetric(\n            threshold=t.get(\"step_efficiency\", 0.7),\n            model=model,\n            include_reason=True,\n        ),\n        # Custom G-Eval for documentation quality\n        get_documentation_quality_metric(model),\n    ]\n\n\ndef get_eval_model(fast_mode: bool = False) -\u003e DeepEvalBaseLLM:\n    \"\"\"\n    Get evaluation model.\n    \n    Default: Claude Sonnet (accurate)\n    Fast mode: Gemini Flash (cheap, for local iteration)\n    \"\"\"\n    if fast_mode:\n        from deepeval.models import GeminiModel\n        return GeminiModel(model_name=\"gemini-2.0-flash\", temperature=0)\n    \n    from deepeval.models import AnthropicModel\n    return AnthropicModel(model=\"claude-sonnet-4-20250514\", temperature=0)\n```\n\n**Metrics rationale:**\n\n| Metric | Why |\n|--------|-----|\n| Task Completion | Core measure - did Sensei answer the question? |\n| Argument Correctness | Catches bad tool usage (wrong queries, missing params) |\n| Step Efficiency | Catches unnecessary tool calls or redundant searches |\n| G-Eval Doc Quality | Domain-specific criteria for documentation responses |\n\n**NOT included:**\n- Tool Correctness (requires expected_tools in goldens)\n- Plan Adherence/Quality (requires explicit planning)\n- Faithfulness/AnswerRelevancy/ContextualRelevancy (RAG metrics - replaced by above)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:13.407315-05:00","updated_at":"2025-12-06T12:59:06.21696-05:00","closed_at":"2025-12-06T12:59:06.21696-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-c285","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.016133-05:00","created_by":"daemon"},{"issue_id":"sensei-c285","depends_on_id":"sensei-ip47","type":"blocks","created_at":"2025-12-06T10:36:30.893049-05:00","created_by":"daemon"}]}
{"id":"sensei-c4qq","title":"ClosedResourceError when MCP stream closes during query tool","description":"ClosedResourceError in anyio/streams/memory.py:218\n\nMemory stream is closed when trying to send data. Happens in the query tool after previous cancel scope crash - likely a cascading failure from the first error.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-18T19:51:24.956759-05:00","updated_at":"2025-12-20T07:28:59.987223-05:00","closed_at":"2025-12-20T07:28:59.987223-05:00","dependencies":[{"issue_id":"sensei-c4qq","depends_on_id":"sensei-s2s3","type":"related","created_at":"2025-12-18T19:51:31.958248-05:00","created_by":"daemon"}]}
{"id":"sensei-c5uy","title":"Remove deepeval dependency from pyproject.toml","description":"Remove `deepeval\u003e=3.7.4` from dependencies. Keep `pydantic-evals[logfire]` which is already present.","status":"closed","priority":1,"issue_type":"chore","created_at":"2025-12-12T09:42:23.527013-05:00","updated_at":"2025-12-12T11:41:51.705017-05:00","closed_at":"2025-12-12T11:41:51.705017-05:00","labels":["dependencies","eval"],"dependencies":[{"issue_id":"sensei-c5uy","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.434354-05:00","created_by":"daemon"},{"issue_id":"sensei-c5uy","depends_on_id":"sensei-evl0","type":"blocks","created_at":"2025-12-12T09:42:43.296306-05:00","created_by":"daemon"}]}
{"id":"sensei-cfg","title":"Tome: Use proper public suffix resolution for domain comparison","description":"Current Domain value object only strips www. prefix, but subdomains like api.example.com, docs.example.com should resolve to the same root domain (example.com). Need proper public suffix handling (e.g., tldextract library) to correctly identify registrable domain. Example: foo.bar.co.uk should resolve to bar.co.uk, not co.uk.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T13:44:08.328063-05:00","updated_at":"2025-11-27T13:58:32.321651-05:00","closed_at":"2025-11-27T13:58:32.321651-05:00","labels":["architecture","important","tome"]}
{"id":"sensei-cfl4","title":"Improve agent triggering with whenToUse examples","description":"The sensei-researcher agent in `packages/sensei-claude-code/agents/sensei.md` has a generic description. Add stronger whenToUse examples in the frontmatter to help Claude Code better identify when to invoke the agent.\n\nCurrent:\n```yaml\ndescription: \u003e-\n  Use when researching documentation, exploring external GitHub repositories,\n  or understanding how code works in codebases outside the current workspace.\n```\n\nShould add concrete examples like:\n- \"How do I use React Server Components?\"\n- \"What's the best way to handle auth in FastAPI?\"\n- \"Show me how pydantic-ai's tool system works\"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-17T12:19:05.438457-05:00","updated_at":"2025-12-18T12:11:34.604265-05:00","closed_at":"2025-12-18T12:11:34.604265-05:00"}
{"id":"sensei-cij","title":"Remove depth column from Document model","description":"The `depth` column on Document tracks crawl depth from llms.txt (0 = llms.txt itself, 1 = directly linked, etc.). \n\nThis is crawl metadata that doesn't belong on the Document model - it's an artifact of how we discovered the document, not a property of the document itself.\n\n**Changes:**\n- Remove `depth` column from Document model\n- Remove `depth` from DocumentContent type\n- Update migration to not include depth\n- Update crawler to not track/save depth\n\n**Note:** If we need crawl metadata in the future, it belongs in a separate crawl_history or document_source table, not on the document itself.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T09:59:01.100177-05:00","updated_at":"2025-12-01T10:00:41.774917-05:00","closed_at":"2025-12-01T10:00:41.774917-05:00","labels":["cleanup","database","tome"],"dependencies":[{"issue_id":"sensei-cij","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T09:59:01.102535-05:00","created_by":"daemon"}]}
{"id":"sensei-cn3","title":"Deploy to Cloud Run","description":"Run: gcloud run deploy sensei --source . --region us-east1. Configure secrets for API keys and DATABASE_URL.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.914569-05:00","updated_at":"2025-11-27T15:59:46.012164-05:00","closed_at":"2025-11-27T15:59:46.012164-05:00","dependencies":[{"issue_id":"sensei-cn3","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.915243-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-5wd","type":"blocks","created_at":"2025-11-27T09:00:51.527228-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-0nt","type":"blocks","created_at":"2025-11-27T09:00:51.568133-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-3f9","type":"blocks","created_at":"2025-11-27T09:00:51.6073-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-anj","type":"blocks","created_at":"2025-11-27T09:00:51.645743-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-z8x","type":"blocks","created_at":"2025-11-27T09:00:51.68622-05:00","created_by":"daemon"}]}
{"id":"sensei-cox","title":"Set up TailwindCSS v4","description":"Configure TailwindCSS v4 with Vite plugin and base styles","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.152555-05:00","updated_at":"2025-11-27T13:27:48.472576-05:00","closed_at":"2025-11-27T13:27:48.472576-05:00","dependencies":[{"issue_id":"sensei-cox","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.153763-05:00","created_by":"daemon"},{"issue_id":"sensei-cox","depends_on_id":"sensei-6br","type":"blocks","created_at":"2025-11-27T13:22:35.425621-05:00","created_by":"daemon"}]}
{"id":"sensei-czf","title":"Create sensei/cli.py - CLI with subcommands","description":"Create a CLI module with subcommands for server and database management.\n\n**File:** `sensei/cli.py`\n\n**Commands:**\n```\nsensei                    # With no args: start HTTP server\nsensei serve              # Explicit: start HTTP server  \nsensei db init            # Initialize PostgreSQL data directory\nsensei db start           # Start PostgreSQL\nsensei db stop            # Stop PostgreSQL\nsensei db status          # Show PostgreSQL status\nsensei db migrate         # Run pending migrations\nsensei db destroy         # Remove data directory (with confirmation)\n```\n\n**Implementation using argparse (or click):**\n```python\nimport argparse\nimport asyncio\nimport sys\n\ndef main():\n    parser = argparse.ArgumentParser(prog=\"sensei\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n    \n    # sensei serve\n    subparsers.add_parser(\"serve\", help=\"Start HTTP server\")\n    \n    # sensei db \u003csubcommand\u003e\n    db_parser = subparsers.add_parser(\"db\", help=\"Database management\")\n    db_sub = db_parser.add_subparsers(dest=\"db_command\")\n    db_sub.add_parser(\"init\", help=\"Initialize database\")\n    db_sub.add_parser(\"start\", help=\"Start PostgreSQL\")\n    db_sub.add_parser(\"stop\", help=\"Stop PostgreSQL\")\n    db_sub.add_parser(\"status\", help=\"Show status\")\n    db_sub.add_parser(\"migrate\", help=\"Run migrations\")\n    db_sub.add_parser(\"destroy\", help=\"Remove data directory\")\n    \n    args = parser.parse_args()\n    \n    if args.command is None or args.command == \"serve\":\n        run_server()\n    elif args.command == \"db\":\n        handle_db_command(args.db_command)\n\ndef run_server():\n    import uvicorn\n    from sensei.__main__ import app\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\ndef handle_db_command(cmd: str):\n    from sensei.database import local\n    # ... dispatch to local.* functions\n```\n\n**Note:** Keep it simple with argparse. No need for click unless complexity grows.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:46.501223-05:00","updated_at":"2025-12-02T07:02:43.34575-05:00","closed_at":"2025-12-02T07:02:43.34575-05:00","dependencies":[{"issue_id":"sensei-czf","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:46.502747-05:00","created_by":"daemon"},{"issue_id":"sensei-czf","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:10:55.891069-05:00","created_by":"daemon"}]}
{"id":"sensei-czoh","title":"Add explicit tool usage correctness/success metrics to eval harness","description":"Current metrics (TaskCompletion, ArgumentCorrectness, StepEfficiency, custom GEval) do not explicitly assert tool success or expected/missing tool calls. This only partially covers the goal of validating appropriate and successful tool usage.\n\nWork needed: add/choose metrics that separately evaluate expected_tools vs tools_called and surface tool failure/success as first-class signals.","acceptance_criteria":"- Eval results include a clear signal for missing/extra tools and tool failures.\n- Tool-usage scoring is separate from answer-quality scoring.\n- Goldens can specify expected tool behavior where relevant.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T20:42:35.094948-05:00","updated_at":"2025-12-12T11:41:39.393837-05:00","closed_at":"2025-12-12T11:41:39.393837-05:00"}
{"id":"sensei-dbo","title":"Create root.tsx entry point and routes","description":"Create src/root.tsx (HTML shell), src/routes.ts, and src/routes/home.tsx placeholder","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.277674-05:00","updated_at":"2025-11-27T13:27:48.582151-05:00","closed_at":"2025-11-27T13:27:48.582151-05:00","dependencies":[{"issue_id":"sensei-dbo","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.278804-05:00","created_by":"daemon"},{"issue_id":"sensei-dbo","depends_on_id":"sensei-6br","type":"blocks","created_at":"2025-11-27T13:22:35.508318-05:00","created_by":"daemon"},{"issue_id":"sensei-dbo","depends_on_id":"sensei-cox","type":"blocks","created_at":"2025-11-27T13:22:35.554226-05:00","created_by":"daemon"}]}
{"id":"sensei-ddo","title":"Tome: Handle port numbers in domain comparison","description":"parser.py:48-50 netloc comparison includes port numbers - example.com:443 != example.com. Should strip default ports (80/443) during comparison.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, port stripping happens automatically in Domain._normalize().","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:44:07.128291-05:00","updated_at":"2025-11-27T13:42:03.676023-05:00","closed_at":"2025-11-27T13:42:03.676023-05:00","labels":["critical","tome"],"dependencies":[{"issue_id":"sensei-ddo","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.887605-05:00","created_by":"daemon"}]}
{"id":"sensei-die","title":"Build and publish sensei to PyPI","description":"Use uv build and uv publish to publish package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.447842-05:00","updated_at":"2025-11-27T06:15:23.12644-05:00","closed_at":"2025-11-27T06:15:23.12644-05:00","dependencies":[{"issue_id":"sensei-die","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.448451-05:00","created_by":"daemon"}]}
{"id":"sensei-dkne","title":"Move eval datasets into sensei/eval","description":"Relocate pydantic-evals YAML datasets from tests tree into the eval package (e.g. sensei/eval/datasets/) and update loaders, tests, and scripts accordingly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T11:54:58.312113-05:00","updated_at":"2025-12-12T11:58:01.477243-05:00","closed_at":"2025-12-12T11:58:01.477243-05:00","dependencies":[{"issue_id":"sensei-dkne","depends_on_id":"sensei-axt2","type":"discovered-from","created_at":"2025-12-12T11:54:58.316113-05:00","created_by":"daemon"}]}
{"id":"sensei-dnj","title":"Update sensei/database/storage.py - use effective_database_url","description":"Update storage.py to use `settings.database_url` (which now always has a value).\n\n**File:** `sensei/database/storage.py`\n\n**Current (line 27-31):**\n```python\ndef _get_engine():\n    global _engine\n    if _engine is None:\n        _engine = create_async_engine(\n            settings.database_url,\n            ...\n        )\n```\n\n**NO CHANGES NEEDED to the engine creation!**\n\nThe `settings.database_url` field now:\n- Uses `default_factory=get_local_database_url` so it's never None\n- Gets overridden by `DATABASE_URL` env var if set\n\n**What to verify:**\n- `settings.database_url` is used (not `settings.effective_database_url`)\n- No None checks needed (it's always a string)\n\n**Data flow:**\n```\nsettings.database_url (always a valid string)\n    │\n    ├── DATABASE_URL env var set? → uses that value\n    │\n    └── DATABASE_URL not set? → uses get_local_database_url()\n                                 → \"postgresql+asyncpg:///sensei?host=~/.sensei/pgdata\"\n```\n\n**Actual change needed:** Just verify the existing code is correct. If it already uses `settings.database_url`, no changes required.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:24:35.355757-05:00","updated_at":"2025-12-02T08:04:15.011338-05:00","closed_at":"2025-12-02T08:04:15.011338-05:00","dependencies":[{"issue_id":"sensei-dnj","depends_on_id":"sensei-gvh","type":"blocks","created_at":"2025-12-02T06:24:35.357211-05:00","created_by":"daemon"},{"issue_id":"sensei-dnj","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:24:35.358118-05:00","created_by":"daemon"}]}
{"id":"sensei-drsl","title":"Better query_id extraction than regex matching","description":"Current design extracts query_id from sensei_query response using regex:\n```\nconst match = event.result.match(/Query ID: ([a-f0-9-]+)/)\n```\n\nThis is fragile. Better approaches:\n- Return structured response from tool (not just string)\n- Store query_id in plugin state when tool executes\n- Use OpenCode's tool result metadata if available\n\nResearch OpenCode's tool.execute.after event shape to find cleaner solution.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T16:48:15.843867-05:00","updated_at":"2025-12-17T18:13:54.305591-05:00","dependencies":[{"issue_id":"sensei-drsl","depends_on_id":"sensei-gy0d","type":"parent-child","created_at":"2025-12-17T16:48:22.937207-05:00","created_by":"daemon"}],"comments":[{"id":2,"issue_id":"sensei-drsl","author":"alizain","text":"User wants these kept open for future improvement","created_at":"2025-12-17T23:13:54Z"}]}
{"id":"sensei-ec9","title":"Update crawler to use chunker and save sections","description":"Modify crawler.py to chunk documents and save sections instead of raw content.\n\n**Changes to ingest flow:**\n1. Fetch document content (unchanged)\n2. Compute content_hash (unchanged)\n3. Check if document exists and hash matches (unchanged)\n4. **NEW:** Chunk content using `chunk_markdown()`\n5. Save Document metadata (no content)\n6. Save Sections with parent relationships and positions\n\n**Key changes:**\n- Import chunker module\n- After fetching content, call `chunk_markdown(content)`\n- Build Section objects with parent_section_id tracking\n- Use position counter for global ordering\n- Call `save_sections()` instead of `save_document()`\n\n**Handle parent relationships:**\n- Track current parent as we recurse\n- Assign position incrementally in document order","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:55.507402-05:00","updated_at":"2025-12-01T10:16:27.986935-05:00","closed_at":"2025-12-01T10:16:27.986935-05:00","labels":["crawler","tome"],"dependencies":[{"issue_id":"sensei-ec9","depends_on_id":"sensei-7gz","type":"blocks","created_at":"2025-12-01T09:56:55.509738-05:00","created_by":"daemon"},{"issue_id":"sensei-ec9","depends_on_id":"sensei-v52","type":"blocks","created_at":"2025-12-01T09:56:55.510465-05:00","created_by":"daemon"},{"issue_id":"sensei-ec9","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.803889-05:00","created_by":"daemon"}]}
{"id":"sensei-ecej","title":"Add research methodology skill","description":"Extract the research methodology from the sensei agent into a reusable skill that any agent can use. This includes:\n\n- Iterative wide-deep exploration\n- Source trust hierarchy\n- Engineering judgment for evaluating solutions\n- When to say \"not found\" vs giving poor answers\n\nLocation: `packages/sensei-claude-code/skills/research-methodology/SKILL.md`\n\nThis allows the main coding agent to apply the same rigorous research approach when sensei isn't invoked.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-17T12:19:05.644531-05:00","updated_at":"2025-12-18T12:11:34.506726-05:00","closed_at":"2025-12-18T12:11:34.506726-05:00"}
{"id":"sensei-edz","title":"Tome: Section-based storage for large documents","description":"Refactor tome storage model from monolithic documents to section-based storage.\n\n**Core insight:** Documents are containers, sections are content. Markdown headings provide natural boundaries for chunking.\n\n**Benefits:**\n- FTS always works (sections fit within tsvector limit)\n- Better search results (return relevant section, not 1MB blob)\n- Granular retrieval (full doc OR specific section subtree)\n- Natural TOC derived from section tree\n\n**Design doc:** docs/plans/2025-12-01-tome-section-based-storage.md","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-01T09:56:20.361081-05:00","updated_at":"2025-12-01T10:25:39.054492-05:00","closed_at":"2025-12-01T10:25:39.054492-05:00","labels":["architecture","epic","tome"]}
{"id":"sensei-eg1","title":"RatingRequest duplicates Rating model - violates single source of truth","description":"server/models.py duplicates domain models from types.py, violating single source of truth:\n\n**Duplications found:**\n1. `RatingRequest` duplicates `Rating` (7 fields)\n2. `QueryResponse` duplicates `QueryResult` (2 fields)\n\nThe ONLY difference is edge models add `json_schema_extra` for OpenAPI examples.\n\n**Fix: Use inheritance**\n- `RatingRequest(Rating)` - inherits fields, adds only `model_config` with examples\n- `QueryResponse(QueryResult)` - inherits fields, adds only `model_config` with examples\n\n**Files affected:**\n- sensei/server/models.py\n- sensei/types.py (may need Field descriptions moved here)\n\n**Conversion points to verify:**\n- api.py:180 - `Rating(**request.model_dump())` → can use `request` directly if RatingRequest inherits Rating\n- api.py:53 - `QueryResponse(query_id=..., markdown=...)` → can return result directly","design":"Use Pydantic inheritance: edge models inherit from domain models, adding only json_schema_extra for OpenAPI examples. This keeps field definitions in types.py (single source of truth) while allowing API-specific presentation at the edge.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:03:17.207517-05:00","updated_at":"2025-11-30T08:57:47.739968-05:00","closed_at":"2025-11-30T08:57:47.739968-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-ellw","title":"Research OpenCode, Claude Code plugins, and Sourcegraph Amp for best practices","description":"Study how leading coding agents work to inform sensei improvements across the board. Targets:\n1. OpenCode - open source coding agent\n2. Claude Code plugins - official Anthropic plugin system\n3. Sourcegraph Amp - if code is available\n\nGoal: Learn patterns for tool descriptions, agent prompts, plugin structure, commands, skills, hooks, feedback mechanisms, and anything else we can apply to make sensei better.","design":"## Research Summary\n\n### Sources Examined\n1. **OpenCode** (sst/opencode) - Open source, comprehensive\n2. **Claude Code plugins** (anthropics/claude-code) - Official, excellent patterns\n3. **Amp** (ampcode.com) - Not open source, limited public docs\n\n---\n\n## Key Patterns from Claude Code\n\n### 1. Plugin Structure\n```\nplugin/\n├── .claude-plugin/plugin.json  # metadata\n├── commands/                   # slash commands\n├── agents/                     # specialized agents\n├── skills/                     # knowledge modules\n├── hooks/                      # event handlers\n└── README.md\n```\n\n### 2. Confidence-Based Filtering\n- Agents output confidence scores (0-100)\n- Findings below 80% confidence are filtered\n- Reduces false positives\n\n### 3. Skills = Reusable Knowledge Modules\n- Loaded on demand when relevant\n- Can be composed for complex tasks\n- Documented methodology becomes executable\n\n### 4. Hooks for Event-Driven Behavior\n- `SessionStart` - offer help proactively\n- `PreToolUse` - validate/suggest before action\n- `Stop` - cleanup, feedback prompt\n\n---\n\n## Key Patterns from OpenCode\n\n### 1. Multi-Agent Architecture\n- **Build** agent: full dev access\n- **Plan** agent: read-only analysis\n- Subagents via `@agent-name` mentions\n\n### 2. Layered Prompt Assembly\n1. Provider-specific base\n2. Environment context (cwd, git, platform)\n3. Project structure (file tree)\n4. Custom instructions (AGENTS.md)\n5. Agent-specific prompts\n\n### 3. Permission System (ask/allow/deny)\n- Per-tool granularity\n- Per-agent overrides\n- User grants once, system remembers\n\n### 4. No Explicit Feedback - Permissions ARE Feedback\n- User approvals/denials train autonomy level\n- Implicit feedback loop through permission flow\n\n---\n\n## Amp (Limited Info - Not Open Source)\n- Has \"Agent Skills\" feature (reuse Claude Code skills)\n- Python SDK available\n- \"Handoff\" instead of compaction for context management\n- No public codebase to examine\n\n---\n\n## Actionable Takeaways for Sensei\n\n### High Priority\n1. **Add feedback prompt to agent instructions** (sensei-z563)\n   - After research, prompt: \"Was this helpful? Use sensei_feedback...\"\n   - This is the simplest change with immediate impact\n\n2. **Add slash commands**\n   - `/sensei:research [topic]` - direct invocation\n   - `/sensei:explore [repo-url]` - external repo deep dive\n   - Improves discoverability\n\n3. **Confidence thresholds for cache**\n   - Score cache hits by freshness + relevance\n   - Only surface high-confidence results\n\n### Medium Priority\n4. **Extract skills from agent prompt**\n   - `research-methodology.md`\n   - `source-evaluation.md`\n   - `confidence-communication.md`\n   - Makes methodology reusable\n\n5. **Add SessionStart hook**\n   - \"Searching cache... need help with decomposition?\"\n   - Proactive guidance\n\n### Lower Priority\n6. **Specialized sub-agents**\n   - `sensei-explorer` - deep dive single topic\n   - `sensei-synthesizer` - combine multiple sources\n   - `sensei-validator` - verify cached answers still correct\n\n---\n\n## Related\n- sensei-z563: Improve feedback collection frequency","notes":"Research completed 2025-12-17. Claude Code has the most applicable patterns. OpenCode has interesting permission-as-feedback approach. Amp is closed source but has similar features.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T12:29:45.290907-05:00","updated_at":"2025-12-17T12:37:49.646337-05:00"}
{"id":"sensei-esxx","title":"Update tests for simplified database lifecycle","description":"Update or delete tests that relied on sensei-managed PostgreSQL.\n\n**Files:**\n- `tests/test_database_local.py` - Most tests should be deleted\n- `tests/test_paths.py` - Remove tests for deleted path functions\n\n**test_database_local.py changes:**\nDelete tests for removed functions:\n- `test_check_postgres_installed_true`\n- `test_check_postgres_installed_false`\n- `test_is_initialized_false`\n- `test_is_initialized_true`\n- `test_ensure_db_ready_skips_for_external_db`\n- `test_ensure_db_ready_idempotent`\n\nKeep test for `ensure_migrated()` if it exists.\n\n**test_paths.py changes:**\nRemove tests for:\n- `test_get_pgdata`\n- `test_get_pg_log`\n- `test_get_local_database_url`\n\nKeep tests for:\n- `test_get_sensei_home_*`\n- `test_get_scout_repos`","acceptance_criteria":"- All tests pass: `uv run pytest`\n- No tests reference deleted functions\n- No import errors in test files","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:40.128695-05:00","updated_at":"2025-12-21T09:57:40.797151-05:00","closed_at":"2025-12-21T09:57:40.797151-05:00","dependencies":[{"issue_id":"sensei-esxx","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:40.133711-05:00","created_by":"daemon"},{"issue_id":"sensei-esxx","depends_on_id":"sensei-1lmy","type":"blocks","created_at":"2025-12-21T09:46:23.217468-05:00","created_by":"daemon"},{"issue_id":"sensei-esxx","depends_on_id":"sensei-rzw8","type":"blocks","created_at":"2025-12-21T09:46:23.262336-05:00","created_by":"daemon"},{"issue_id":"sensei-esxx","depends_on_id":"sensei-5g0x","type":"blocks","created_at":"2025-12-21T09:46:23.309365-05:00","created_by":"daemon"},{"issue_id":"sensei-esxx","depends_on_id":"sensei-tkwi","type":"blocks","created_at":"2025-12-21T09:46:23.358604-05:00","created_by":"daemon"}]}
{"id":"sensei-evl0","title":"Delete DeepEval-specific files","description":"Remove files that are no longer needed:\n- `sensei/eval/instrumentation_workaround.py` (~295 lines)\n- `sensei/eval/metrics.py` (DeepEval metrics)\n- `sensei/eval/loader.py` (DeepEval Golden loader)\n- `scripts/test_deepeval_workaround.py`\n- `scripts/test_instrumentation.py`","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:23.439218-05:00","updated_at":"2025-12-12T11:41:51.607113-05:00","closed_at":"2025-12-12T11:41:51.607113-05:00","labels":["cleanup","eval"],"dependencies":[{"issue_id":"sensei-evl0","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.380638-05:00","created_by":"daemon"},{"issue_id":"sensei-evl0","depends_on_id":"sensei-ax9v","type":"blocks","created_at":"2025-12-12T09:42:43.239038-05:00","created_by":"daemon"}]}
{"id":"sensei-ewf7","title":"Clarify noReply parameter in session.idle hook","description":"In the OpenCode plugin design, the session.idle hook uses ctx.client.session.prompt() with noReply: false.\n\nQuestion: Should this be noReply: true or false?\n- false = agent responds (current design)\n- true = inject context without response\n\nNeed to research OpenCode SDK behavior and decide the right approach for prompting agent to call sensei_feedback.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T16:48:15.73148-05:00","updated_at":"2025-12-17T18:13:54.301131-05:00","dependencies":[{"issue_id":"sensei-ewf7","depends_on_id":"sensei-gy0d","type":"parent-child","created_at":"2025-12-17T16:48:22.81256-05:00","created_by":"daemon"}],"comments":[{"id":1,"issue_id":"sensei-ewf7","author":"alizain","text":"User wants these kept open for future improvement","created_at":"2025-12-17T23:13:54Z"}]}
{"id":"sensei-f2f4","title":"Use @pytest.mark.usefixtures for tests that don't need fixture value","description":"Tests in `test_database.py` and `test_tome_service.py` pass `test_db` as a parameter even when they don't use the fixture's return value. Use `@pytest.mark.usefixtures(\"test_db\")` for tests that only need the side effect (database setup/teardown), keeping parameter injection only for tests that actually use the engine.\n\n**Wait for**: sensei-ucwq (transaction rollback) to stabilize the fixture architecture first.\n\n**Example**:\n```python\n# Before - passes fixture but doesn't use it\nasync def test_save_query(test_db):\n    await save_query(...)\n\n# After - uses decorator for side effect only\n@pytest.mark.usefixtures(\"test_db\")\nasync def test_save_query():\n    await save_query(...)\n```","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-20T07:48:19.359567-05:00","updated_at":"2025-12-20T13:13:36.60859-05:00","closed_at":"2025-12-20T13:13:36.60859-05:00","dependencies":[{"issue_id":"sensei-f2f4","depends_on_id":"sensei-ucwq","type":"blocks","created_at":"2025-12-20T12:55:39.44298-05:00","created_by":"daemon"}]}
{"id":"sensei-fdc","title":"Update tome crawler to fetch llms-full.txt","description":"Currently the crawler only fetches `/llms.txt` and follows its links. We need to also fetch `/llms-full.txt` when available.\n\n**Current behavior:**\n1. GET /llms.txt (depth=0)\n2. Parse links, crawl linked docs (depth=1+)\n\n**New behavior:**\n1. GET /llms.txt (depth=0) - the INDEX\n2. GET /llms-full.txt (depth=0) - the FULL content (404 is fine, not all sites have it)\n3. Parse links from llms.txt\n4. Crawl linked docs (depth=1+)\n\n**Implementation:**\n- Modify `ingest_domain()` in `sensei/tome/crawler.py`\n- Start crawl with TWO initial URLs instead of one\n- Handle 404 gracefully for llms-full.txt (many sites won't have it)\n\n**Testing:**\n- Test domain with both files\n- Test domain with only llms.txt (llms-full.txt 404)\n- Verify both stored correctly with proper paths","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:36.15785-05:00","updated_at":"2025-12-01T08:48:54.937877-05:00","closed_at":"2025-12-01T08:48:54.937877-05:00","labels":["crawler","tome"],"dependencies":[{"issue_id":"sensei-fdc","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.640039-05:00","created_by":"daemon"}]}
{"id":"sensei-fhbe","title":"Simplify sensei/scout/__main__.py to just bootstrap python -m","description":"Reduce __main__.py to its single purpose: enabling `python -m sensei.scout`. Just import main from server and call it in if __name__ block.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:15.625571-05:00","updated_at":"2025-12-02T12:37:14.531567-05:00","closed_at":"2025-12-02T12:37:14.531567-05:00","dependencies":[{"issue_id":"sensei-fhbe","depends_on_id":"sensei-yonb","type":"blocks","created_at":"2025-12-02T12:34:15.626384-05:00","created_by":"daemon"}]}
{"id":"sensei-fkle","title":"Use DeepEval's native PydanticAI integration","description":"Use DeepEval's built-in PydanticAI integration via `ConfidentInstrumentationSettings`.\n\n## Official Integration\n\nDeepEval provides native PydanticAI support: https://deepeval.com/integrations/frameworks/pydanticai\n\n```python\nfrom pydantic_ai import Agent\nfrom deepeval.integrations.pydantic_ai.instrumentator import (\n    ConfidentInstrumentationSettings,\n)\nfrom deepeval.metrics import AnswerRelevancyMetric\n\nagent = Agent(\n    \"openai:gpt-5\",\n    instrument=ConfidentInstrumentationSettings(\n        is_test_mode=True,\n        agent_metrics=[AnswerRelevancyMetric()]\n    ),\n)\n```\n\n## Current State\n\nSensei already has instrumentation:\n- `Agent.instrument_all()` at line 32\n- `instrument=True` at line 206\n\n## Implementation\n\n**Option A: Eval-time configuration (recommended)**\n\nKeep production code unchanged. Configure instrumentation only during evaluation:\n\n```python\n# sensei/eval/runner.py\nfrom deepeval.integrations.pydantic_ai.instrumentator import ConfidentInstrumentationSettings\nfrom deepeval.dataset import EvaluationDataset, Golden\nfrom sensei.eval.metrics import get_metrics\n\nasync def run_evaluation(goldens: list[Golden], fast_mode: bool = False):\n    model = get_eval_model(fast_mode)\n    metrics = get_metrics(model)\n    \n    # Create agent with eval instrumentation\n    from sensei.agent import create_agent\n    eval_agent = create_agent_for_eval(metrics)\n    \n    dataset = EvaluationDataset(goldens=goldens)\n    \n    for golden in dataset.evals_iterator():\n        task = asyncio.create_task(eval_agent.run(golden.input))\n        dataset.evaluate(task)\n```\n\n**Option B: Factory function for eval agent**\n\nAdd to `sensei/agent.py`:\n\n```python\nfrom deepeval.integrations.pydantic_ai.instrumentator import ConfidentInstrumentationSettings\n\ndef create_agent_for_eval(\n    metrics: list,\n    include_spawn: bool = True,\n    include_exec_plan: bool = True,\n) -\u003e Agent[deps_module.Deps, str]:\n    \"\"\"Create an agent configured for DeepEval evaluation.\"\"\"\n    tools = []\n    if include_exec_plan:\n        tools.append(Tool(wrap_tool(add_exec_plan), takes_ctx=True))\n        tools.append(Tool(wrap_tool(update_exec_plan), takes_ctx=True))\n    if include_spawn:\n        tools.append(Tool(spawn_sub_agent, takes_ctx=True))\n\n    return Agent(\n        model=DEFAULT_MODEL,\n        system_prompt=SYSTEM_PROMPT,\n        deps_type=deps_module.Deps,\n        output_type=str,\n        toolsets=[\n            create_context7_server(settings.context7_api_key),\n            create_tavily_server(settings.tavily_api_key),\n            create_scout_server(),\n            create_kura_server(),\n            create_tome_server(),\n        ],\n        tools=tools,\n        instructions=[current_exec_plan, prefetch_cache_hits],\n        instrument=ConfidentInstrumentationSettings(\n            is_test_mode=True,\n            agent_metrics=metrics,\n        ),\n    )\n```\n\n## What Gets Auto-Captured\n\nPer DeepEval docs, metrics receive:\n- `input` - user query\n- `output` - agent response  \n- `tools_called` - tool calls made\n\n**Note:** Only metrics that accept these parameters qualify for automatic evaluation.\n\n## Evaluation Pattern\n\n```python\ndataset = EvaluationDataset(goldens=[...])\n\nfor golden in dataset.evals_iterator():\n    task = asyncio.create_task(eval_agent.run(golden.input))\n    dataset.evaluate(task)\n```\n\n## No Manual Tracing Needed\n\nWith native integration, we **don't need**:\n- `@observe` decorator\n- `update_current_trace()` calls\n- Manual message parsing for tools_called\n- Client patching\n\nDeepEval handles everything through `ConfidentInstrumentationSettings`.\n\n## Dependencies\n\n- sensei-c285 (metrics configuration)\n- sensei-uy71 (YAML loader for goldens)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T12:07:48.700355-05:00","updated_at":"2025-12-06T13:04:44.934501-05:00","closed_at":"2025-12-06T13:04:44.934501-05:00","labels":["eval","tracing"],"dependencies":[{"issue_id":"sensei-fkle","depends_on_id":"sensei-qxlo","type":"blocks","created_at":"2025-12-06T12:08:18.79822-05:00","created_by":"daemon"},{"issue_id":"sensei-fkle","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T12:08:19.056219-05:00","created_by":"daemon"}]}
{"id":"sensei-fnh","title":"Add kura MCP server with stdio mode","description":"Create server.py with FastMCP wrapping cache functions (search_cache, get_cached_response). Add __main__.py for stdio entry point. Mirror Scout's server pattern.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.090543-05:00","updated_at":"2025-11-27T08:57:25.907756-05:00","closed_at":"2025-11-27T08:57:25.907756-05:00","dependencies":[{"issue_id":"sensei-fnh","depends_on_id":"sensei-bti","type":"blocks","created_at":"2025-11-27T08:55:36.496058-05:00","created_by":"daemon"}]}
{"id":"sensei-fvhz","title":"Add build_response_with_feedback to build.py","description":"Extract FEEDBACK_TEMPLATE and response building logic from core.handle_query() into build.py. Creates build_response_with_feedback(output, query_id) -\u003e str.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:22:45.096376-05:00","updated_at":"2025-12-18T07:25:15.565455-05:00","closed_at":"2025-12-18T07:25:15.565455-05:00"}
{"id":"sensei-fxli","title":"PydanticAI tool exceeds max retries during concurrent execution","description":"UnexpectedModelBehavior: Tool 'grep_searchGitHub' exceeded max retries count of 1\n\nHappens in pydantic_ai/_tool_manager.py:181 when running scout_read and grep_searchGitHub concurrently. May be related to the cancel scope issues corrupting state.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-18T19:51:24.999353-05:00","updated_at":"2025-12-20T07:29:00.027194-05:00","closed_at":"2025-12-20T07:29:00.027194-05:00","dependencies":[{"issue_id":"sensei-fxli","depends_on_id":"sensei-s2s3","type":"related","created_at":"2025-12-18T19:51:31.993463-05:00","created_by":"daemon"}]}
{"id":"sensei-gc1","title":"Remove dead code from tools/common.py","description":"tools/common.py has unused functions that should be removed after exec_plan refactor:\n\nDead code:\n- format_entries (line 15) - never imported or used\n- get_client (line 44) - never imported or used  \n- DEFAULT_TIMEOUT (line 12) - only used by get_client\n\nKeep:\n- wrap_tool (line 52) - will be used after exec_plan refactor\n\nAfter exec_plan refactor is complete, audit again and remove any remaining dead code.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:14:52.27395-05:00","updated_at":"2025-11-30T08:32:57.613046-05:00","closed_at":"2025-11-30T08:32:57.613046-05:00","labels":["cleanup"],"dependencies":[{"issue_id":"sensei-gc1","depends_on_id":"sensei-aky","type":"blocks","created_at":"2025-11-30T06:14:52.275468-05:00","created_by":"daemon"}]}
{"id":"sensei-gcu","title":"Update tests for section-based storage","description":"Update test_tome_service.py and add new tests for section-based functionality.\n\n**Update existing tests:**\n- Fixtures need to create Document + Sections instead of Documents with content\n- Search tests should verify heading_path in results\n\n**Add new tests:**\n\n**Chunker tests (test_tome_chunker.py):**\n- Small content returns single section\n- Large content splits by headings\n- Recursive splitting works at multiple levels\n- Raises error when content too large with no headings\n- Position ordering is correct\n- Parent relationships are correct\n\n**Service tests:**\n- `tome_toc` returns correct structure\n- `tome_get` without heading returns full doc\n- `tome_get` with heading returns subtree only\n- `tome_search` returns heading_path breadcrumb\n\n**Integration tests:**\n- Ingest real domain, verify sections created\n- Search returns sections with context\n- TOC structure matches document","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T09:57:24.666195-05:00","updated_at":"2025-12-01T10:25:32.187954-05:00","closed_at":"2025-12-01T10:25:32.187954-05:00","labels":["testing","tome"],"dependencies":[{"issue_id":"sensei-gcu","depends_on_id":"sensei-m7g","type":"blocks","created_at":"2025-12-01T09:57:24.669272-05:00","created_by":"daemon"},{"issue_id":"sensei-gcu","depends_on_id":"sensei-7gz","type":"blocks","created_at":"2025-12-01T09:57:24.670269-05:00","created_by":"daemon"},{"issue_id":"sensei-gcu","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.933641-05:00","created_by":"daemon"}]}
{"id":"sensei-gdv","title":"Add ingest_domain() public API","description":"Create async def ingest_domain(domain: str, max_depth: int = 3) -\u003e IngestResult in sensei/tome/__init__.py. This is the main entry point for crawling a domain's llms.txt.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:05.602081-05:00","updated_at":"2025-11-27T11:27:50.74934-05:00","closed_at":"2025-11-27T11:27:50.74934-05:00","dependencies":[{"issue_id":"sensei-gdv","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:05.603335-05:00","created_by":"daemon"},{"issue_id":"sensei-gdv","depends_on_id":"sensei-zii","type":"blocks","created_at":"2025-11-27T11:16:18.536978-05:00","created_by":"daemon"}]}
{"id":"sensei-gg1","title":"Remove unused documents_active view from migration","description":"**Current state:**\nMigration 004 creates a `documents_active` view that is never used:\n```python\n# alembic/versions/004_generation_based_crawls.py lines 77-80\nop.execute(\"\"\"\n    CREATE VIEW documents_active AS\n    SELECT * FROM documents WHERE generation_active = true\n\"\"\")\n```\n\n**Problem:** Dead code. All queries use `Document.generation_active == True` directly via SQLAlchemy ORM. The view adds no value and clutters the schema.\n\n**Fix:** Remove view creation from migration 004:\n\n1. Delete lines 77-80 (CREATE VIEW in upgrade)\n2. Delete line 96 (DROP VIEW in downgrade)\n3. Update docstring to remove mention of view\n\n**Note:** Do NOT create a new migration. We'll wipe the DB and start clean.\n\n**Why not use the view:**\n- SQLAlchemy ORM doesn't naturally map to views\n- `generation_active == True` filter is explicit and clear\n- Partial index `idx_documents_domain_active` already optimizes these queries\n- Keeping dead abstractions adds confusion","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-01T15:25:39.526155-05:00","updated_at":"2025-12-01T15:31:09.586521-05:00","closed_at":"2025-12-01T15:31:09.586521-05:00","labels":["cleanup","database","dead-code"]}
{"id":"sensei-ggd","title":"Deploy to Fly.io and verify","description":"Run fly deploy, verify app is running, test /health endpoint, test MCP endpoints, verify Scout can clone repos to /data volume.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.981935-05:00","updated_at":"2025-11-30T05:57:26.787413-05:00","closed_at":"2025-11-30T05:57:26.787413-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-ggd","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:18.052622-05:00","created_by":"daemon"},{"issue_id":"sensei-ggd","depends_on_id":"sensei-51u","type":"blocks","created_at":"2025-11-27T16:00:24.630788-05:00","created_by":"daemon"}]}
{"id":"sensei-gun","title":"Tome: Move UUID generation to storage layer","description":"crawler.py:72 caller generates UUID instead of storage layer defaulting it. Make id parameter optional with default in save_tome_document().","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:47.570845-05:00","updated_at":"2025-11-27T16:08:50.950692-05:00","closed_at":"2025-11-27T16:08:50.950692-05:00","labels":["minor","tome"]}
{"id":"sensei-gvh","title":"Update sensei/config.py - dynamic database_url","description":"Update config.py with dynamic database_url.\n\n**File:** `sensei/config.py`\n\n**Current:**\n```python\ndatabase_url: str = Field(\n    default=\"postgresql+asyncpg://sensei:sensei@localhost:5432/sensei\",\n    description=\"Database connection URL\",\n)\n```\n\n**Change to:**\n```python\nimport os\nfrom sensei.paths import get_local_database_url\n\nclass Settings(BaseSettings):\n    # ... other fields ...\n    \n    database_url: str = Field(\n        default_factory=get_local_database_url,\n        description=\"Database connection URL (defaults to local PostgreSQL via Unix socket)\",\n    )\n    \n    @property\n    def is_external_database(self) -\u003e bool:\n        \"\"\"Check if using an external (user-provided) database.\n        \n        Returns True if DATABASE_URL env var was explicitly set.\n        If external, sensei won't start PostgreSQL and won't run migrations\n        (user is responsible for their own DB).\n        \"\"\"\n        return os.environ.get(\"DATABASE_URL\") is not None\n```\n\n**Simplifications from review:**\n- No `sensei_home` field (paths.py handles it directly)\n- No `database_auto_migrate` field - simpler rule:\n  - Local DB → always migrate\n  - External DB → never migrate (user's responsibility)\n\n**Behavior:**\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No |","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:20.601992-05:00","updated_at":"2025-12-02T08:01:38.613196-05:00","closed_at":"2025-12-02T08:01:38.613196-05:00","dependencies":[{"issue_id":"sensei-gvh","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:10:20.602946-05:00","created_by":"daemon"},{"issue_id":"sensei-gvh","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:20.605029-05:00","created_by":"daemon"}]}
{"id":"sensei-gwq","title":"Create Section model and migration","description":"Create the new Section table and modify Document table.\n\n**Section table:**\n- id (UUID, PK)\n- document_id (FK → Document, indexed)\n- parent_section_id (FK → Section, nullable)\n- heading (String, nullable)\n- level (Integer)\n- content (Text)\n- position (Integer)\n- search_vector (TSVECTOR, computed on content)\n- inserted_at, updated_at\n\n**Document table changes:**\n- Remove: content, search_vector columns\n- Keep: id, domain, url, path, content_hash, depth, timestamps\n\n**Migration:**\n- Create sections table with all columns and indexes\n- Add GIN index on search_vector\n- Alter documents table to drop content/search_vector\n- Add FK constraints","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:27.900632-05:00","updated_at":"2025-12-01T10:07:58.108514-05:00","closed_at":"2025-12-01T10:07:58.108514-05:00","labels":["database","migration","tome"],"dependencies":[{"issue_id":"sensei-gwq","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T09:56:27.903439-05:00","created_by":"daemon"}]}
{"id":"sensei-gy0d","title":"Implement OpenCode plugin for Sensei","description":"Create an OpenCode plugin that exposes Sensei research capabilities with an OpenCode-native experience.\n\nSee design doc: docs/plans/2025-12-17-opencode-plugin-design.md\n\nComponents:\n- npm package `@sensei-ai/opencode`\n- `sensei_query` tool (calls /query API)\n- `sensei_feedback` tool (calls /rate API)  \n- `sensei.ts` plugin (session.idle hook for agent feedback)\n- Install script (downloads from unpkg)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-17T16:48:15.607679-05:00","updated_at":"2025-12-17T18:13:20.474758-05:00","closed_at":"2025-12-17T18:13:20.474758-05:00","dependencies":[{"issue_id":"sensei-gy0d","depends_on_id":"sensei-ellw","type":"related","created_at":"2025-12-17T16:48:27.938035-05:00","created_by":"daemon"}]}
{"id":"sensei-hkk","title":"save_query takes individual fields instead of domain model","description":"storage.py:53-64 save_query() takes 10 individual parameters while save_rating() correctly accepts the Rating domain model.\n\nThis violates: \"Pass models, not individual fields\"\n\nFix: Create QueryInput model in types.py and refactor save_query to accept it:\n```python\nasync def save_query(query: QueryInput) -\u003e None:\n    ...\n```\n\nFiles affected:\n- sensei/database/storage.py\n- sensei/types.py (add QueryInput model)\n- sensei/core.py (update call sites)","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.402667-05:00","updated_at":"2025-11-30T09:19:54.039708-05:00","closed_at":"2025-11-30T09:19:54.039708-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-hon","title":"Tome: Return Success[IngestResult] | NoResults instead of raw IngestResult","description":"ingest_domain() returns IngestResult directly. Per CLAUDE.md Result Types, tools should return Success[T] | NoResults pattern.","notes":"System thinking review: Part of error boundary pattern. Should be done together with typed exceptions (sensei-rhx) as they're both about proper edge handling.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:43:46.263262-05:00","updated_at":"2025-11-27T13:42:04.653563-05:00","closed_at":"2025-11-27T13:42:04.653563-05:00","labels":["critical","tome"]}
{"id":"sensei-hp08","title":"Use typed Sensei exceptions for eval model config errors","description":"`get_eval_model()` currently raises a bare `ValueError` on unknown model names. Per repo error-handling rules, config/setup errors should raise a typed exception (likely `BrokenInvariant`) with preserved chains.","acceptance_criteria":"- Unknown/invalid eval model names raise an appropriate typed Sensei exception.\n- No bare `ValueError` is thrown from eval config paths.\n- Exception chains are preserved where relevant.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-11T20:42:54.075102-05:00","updated_at":"2025-12-12T11:41:39.04723-05:00","closed_at":"2025-12-12T11:41:39.04723-05:00"}
{"id":"sensei-hvqs","title":"Remove database_auto_migrate setting","description":"The `database_auto_migrate` setting is no longer needed after sensei-4d2l moves migrations to CI/CD.\n\n**New behavior**:\n- External database (`DATABASE_URL` set): Never auto-migrate, CI/CD handles it\n- Sensei-managed (local PG): Always auto-migrate (single instance by definition)\n\nRemove the setting and simplify `ensure_db_ready()` logic.","acceptance_criteria":"- [ ] Remove `database_auto_migrate` from settings\n- [ ] Simplify `ensure_db_ready()` - no conditional migration logic\n- [ ] Update any documentation referencing this setting","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-20T06:28:10.888503-05:00","updated_at":"2025-12-20T13:15:02.175134-05:00","closed_at":"2025-12-20T13:15:02.175134-05:00","dependencies":[{"issue_id":"sensei-hvqs","depends_on_id":"sensei-4d2l","type":"blocks","created_at":"2025-12-20T06:28:10.89041-05:00","created_by":"daemon"}]}
{"id":"sensei-i26s","title":"Implement evaluation runner with native PydanticAI integration","description":"Create runner.py using DeepEval's native PydanticAI integration.\n\n**Create:** `sensei/eval/runner.py`\n\n```python\n\"\"\"Evaluation runner using DeepEval's native PydanticAI integration.\n\nUses ConfidentInstrumentationSettings for automatic tracing and evaluation.\n\"\"\"\n\nimport asyncio\n\nfrom deepeval.dataset import EvaluationDataset, Golden\nfrom deepeval.integrations.pydantic_ai.instrumentator import ConfidentInstrumentationSettings\n\nfrom sensei.eval.loader import load_goldens\nfrom sensei.eval.metrics import get_metrics, get_eval_model\n\n\ndef create_eval_agent(metrics: list):\n    \"\"\"Create agent configured for DeepEval evaluation.\"\"\"\n    from pydantic_ai import Agent, Tool\n    \n    from sensei import deps as deps_module\n    from sensei.agent import (\n        DEFAULT_MODEL,\n        SYSTEM_PROMPT,\n        current_exec_plan,\n        prefetch_cache_hits,\n        spawn_sub_agent,\n    )\n    from sensei.config import settings\n    from sensei.tools.common import wrap_tool\n    from sensei.tools.context7 import create_context7_server\n    from sensei.tools.exec_plan import add_exec_plan, update_exec_plan\n    from sensei.tools.kura import create_kura_server\n    from sensei.tools.scout import create_scout_server\n    from sensei.tools.tavily import create_tavily_server\n    from sensei.tools.tome import create_tome_server\n    \n    tools = [\n        Tool(wrap_tool(add_exec_plan), takes_ctx=True),\n        Tool(wrap_tool(update_exec_plan), takes_ctx=True),\n        Tool(spawn_sub_agent, takes_ctx=True),\n    ]\n    \n    return Agent(\n        model=DEFAULT_MODEL,\n        system_prompt=SYSTEM_PROMPT,\n        deps_type=deps_module.Deps,\n        output_type=str,\n        toolsets=[\n            create_context7_server(settings.context7_api_key),\n            create_tavily_server(settings.tavily_api_key),\n            create_scout_server(),\n            create_kura_server(),\n            create_tome_server(),\n        ],\n        tools=tools,\n        instructions=[current_exec_plan, prefetch_cache_hits],\n        instrument=ConfidentInstrumentationSettings(\n            is_test_mode=True,\n            agent_metrics=metrics,\n        ),\n    )\n\n\nasync def run_evaluation(\n    goldens: list[Golden] | None = None,\n    filter_library: str | None = None,\n    filter_category: str | None = None,\n    fast_mode: bool = False,\n) -\u003e None:\n    \"\"\"Run evaluation on Goldens using native PydanticAI integration.\"\"\"\n    if goldens is None:\n        goldens = load_goldens(\n            filter_library=filter_library,\n            filter_category=filter_category,\n        )\n    \n    if not goldens:\n        print(\"No goldens to evaluate\")\n        return\n    \n    model = get_eval_model(fast_mode)\n    metrics = get_metrics(model)\n    eval_agent = create_eval_agent(metrics)\n    \n    dataset = EvaluationDataset(goldens=goldens)\n    \n    print(f\"Evaluating {len(goldens)} cases with {len(metrics)} metrics...\")\n    print(f\"Model: {'Gemini Flash (fast)' if fast_mode else 'Claude Sonnet (accurate)'}\")\n    print()\n    \n    for golden in dataset.evals_iterator():\n        metadata = golden.additional_metadata or {}\n        case_id = metadata.get(\"id\", \"unknown\")\n        \n        print(f\"Running: {case_id}\")\n        \n        # Run agent and let DeepEval handle tracing\n        task = asyncio.create_task(eval_agent.run(golden.input))\n        dataset.evaluate(task)\n    \n    print()\n    print(\"Evaluation complete.\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Run Sensei evaluation\")\n    parser.add_argument(\"--library\", help=\"Filter by library\")\n    parser.add_argument(\"--category\", help=\"Filter by category\")\n    parser.add_argument(\"--fast\", action=\"store_true\", help=\"Use fast eval model\")\n    \n    args = parser.parse_args()\n    \n    asyncio.run(run_evaluation(\n        filter_library=args.library,\n        filter_category=args.category,\n        fast_mode=args.fast,\n    ))\n```\n\n## Key Changes from Previous Design\n\n1. **Uses `ConfidentInstrumentationSettings`** instead of `@observe` decorator\n2. **Creates separate eval agent** with metrics baked in\n3. **Uses `dataset.evaluate(task)`** pattern per DeepEval docs\n4. **No manual tracing** - DeepEval auto-captures input, output, tools_called\n\n## Dependencies\n\n- sensei-c285 (metrics)\n- sensei-uy71 (loader)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:57.238568-05:00","updated_at":"2025-12-06T13:04:44.994158-05:00","closed_at":"2025-12-06T13:04:44.994158-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-i26s","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.113467-05:00","created_by":"daemon"},{"issue_id":"sensei-i26s","depends_on_id":"sensei-9bui","type":"blocks","created_at":"2025-12-06T10:36:30.993822-05:00","created_by":"daemon"},{"issue_id":"sensei-i26s","depends_on_id":"sensei-c285","type":"blocks","created_at":"2025-12-06T10:36:31.046858-05:00","created_by":"daemon"},{"issue_id":"sensei-i26s","depends_on_id":"sensei-uy71","type":"blocks","created_at":"2025-12-06T10:36:31.100954-05:00","created_by":"daemon"},{"issue_id":"sensei-i26s","depends_on_id":"sensei-fkle","type":"blocks","created_at":"2025-12-06T12:08:21.935303-05:00","created_by":"daemon"}]}
{"id":"sensei-i8d4","title":"Create tests/eval directory structure","description":"Create the test directory structure for evaluation tests.\n\n**Create:**\n```\ntests/eval/\n├── __init__.py\n├── conftest.py\n└── datasets/\n    └── _schema.yaml\n```\n\n**conftest.py:**\n```python\n\"\"\"Pytest fixtures for evaluation tests.\"\"\"\n\nimport os\nimport pytest\n\n\n@pytest.fixture(scope=\"module\")\ndef eval_model():\n    \"\"\"\n    Get evaluation model based on environment.\n    \n    Fast mode (default): Gemini Flash - cheap, quick iteration\n    Accurate mode (EVAL_ACCURATE=1): Claude Sonnet - slower, better judgment\n    \"\"\"\n    from sensei.eval.metrics import get_eval_model\n    \n    fast_mode = os.environ.get(\"EVAL_ACCURATE\") != \"1\"\n    return get_eval_model(fast_mode)\n\n\n@pytest.fixture(scope=\"module\")\ndef eval_metrics(eval_model):\n    \"\"\"Get configured metrics for evaluation.\"\"\"\n    from sensei.eval.metrics import get_metrics\n    return get_metrics(eval_model)\n```\n\n**datasets/_schema.yaml:**\n```yaml\n# Schema documentation for test case YAML files\n#\n# Each YAML file contains test cases (Goldens) for evaluation.\n# Goldens are MINIMAL - only input is required.\n# All other test case attributes are populated by tracing during evaluation.\n#\n# Structure:\n# metadata:\n#   library: string?       # Library name for filtering (e.g., \"fastapi\", \"react\")\n#\n# cases:\n#   - id: string           # Unique identifier (e.g., \"fastapi-di-01\")\n#     query: string        # The question to ask Sensei (REQUIRED)\n#     category: string?    # Category for filtering (e.g., \"api-usage\", \"debugging\")\n#\n# Example:\n# metadata:\n#   library: fastapi\n#\n# cases:\n#   - id: fastapi-di-01\n#     query: \"How do I use dependency injection in FastAPI?\"\n#     category: api-usage\n#\n#   - id: fastapi-ws-01\n#     query: \"How do I implement WebSocket endpoints in FastAPI?\"\n#     category: api-usage\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:57.387844-05:00","updated_at":"2025-12-06T12:27:31.407958-05:00","closed_at":"2025-12-06T12:27:31.407958-05:00","labels":["eval","testing"],"dependencies":[{"issue_id":"sensei-i8d4","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.161966-05:00","created_by":"daemon"}]}
{"id":"sensei-icow","title":"Standardize age formatting to one helper","description":"Two different age formats exist:\n\n```python\n# agent.py:50 - compact\nf\"{hit.age_days}d ago\" if hit.age_days \u003e 0 else \"today\"\n\n# kura/tools.py:46 - verbose  \nf\"**Age:** {age_days} days\"\n```\n\nCreate a single `format_age(days: int) -\u003e str` helper and use it everywhere for consistency.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-12-20T15:02:15.222906-05:00","updated_at":"2025-12-20T15:02:15.222906-05:00","labels":["lifetime-audit","tech-debt"],"dependencies":[{"issue_id":"sensei-icow","depends_on_id":"sensei-st72","type":"related","created_at":"2025-12-20T15:02:20.271052-05:00","created_by":"daemon"}]}
{"id":"sensei-iid","title":"Update Dockerfile for Fly.io deployment","description":"Update existing Dockerfile: add gcc/g++ for numpy build, use PORT env var, ensure uv sync works correctly. Reference /data mount point for Scout cache.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.696172-05:00","updated_at":"2025-11-30T05:57:26.599434-05:00","closed_at":"2025-11-30T05:57:26.599434-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-iid","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.892747-05:00","created_by":"daemon"},{"issue_id":"sensei-iid","depends_on_id":"sensei-8mm","type":"blocks","created_at":"2025-11-27T16:00:24.428165-05:00","created_by":"daemon"}]}
{"id":"sensei-ip4","title":"Add PostgreSQL full-text search to documents table","description":"Add FTS capability to the documents table for tome_search.\n\n**Changes:**\n1. Create Alembic migration that adds:\n   - `search_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED`\n   - GIN index: `CREATE INDEX idx_documents_search ON documents USING GIN(search_vector)`\n\n2. Update SQLAlchemy model in `sensei/database/models.py`:\n   - Add `search_vector` column (computed, so read-only in SQLAlchemy)\n\n**Why GENERATED ALWAYS AS STORED:**\n- Postgres auto-updates vector when content changes\n- No triggers needed\n- Zero application code to maintain it\n\n**Testing:**\n- Verify migration runs cleanly\n- Verify search_vector is populated for existing docs\n- Verify new docs get vector automatically","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:35.707324-05:00","updated_at":"2025-12-01T08:48:08.142603-05:00","closed_at":"2025-12-01T08:48:08.142603-05:00","labels":["database","fts","tome"],"dependencies":[{"issue_id":"sensei-ip4","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.605586-05:00","created_by":"daemon"}]}
{"id":"sensei-ip47","title":"Create sensei/eval module structure","description":"Create the eval module directory structure.\n\n**Create:**\n```\nsensei/eval/\n├── __init__.py      # exports\n```\n\n**__init__.py:**\n```python\n\"\"\"Evaluation module for Sensei quality assessment.\n\nUses DeepEval with tracing + agentic metrics.\n\"\"\"\n\n# Re-export deepeval types for convenience\nfrom deepeval.dataset import Golden, EvaluationDataset\nfrom deepeval.test_case import LLMTestCase\n\n# Export our utilities (added by other beads)\nfrom sensei.eval.loader import load_goldens\nfrom sensei.eval.message_parser import extract_trace_data\nfrom sensei.eval.metrics import get_metrics, get_eval_model\nfrom sensei.eval.runner import run_evaluation\n\n__all__ = [\n    # DeepEval types\n    \"Golden\",\n    \"EvaluationDataset\", \n    \"LLMTestCase\",\n    # Our utilities\n    \"load_goldens\",\n    \"extract_trace_data\",\n    \"get_metrics\",\n    \"get_eval_model\",\n    \"run_evaluation\",\n]\n```\n\n**Note:** Initial creation just makes the `__init__.py`. Imports will fail until other beads add the modules. That's expected - commit the structure first.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:34:45.547556-05:00","updated_at":"2025-12-06T12:27:31.459863-05:00","closed_at":"2025-12-06T12:27:31.459863-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-ip47","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:15.926223-05:00","created_by":"daemon"},{"issue_id":"sensei-ip47","depends_on_id":"sensei-qxlo","type":"blocks","created_at":"2025-12-06T10:36:30.795471-05:00","created_by":"daemon"}]}
{"id":"sensei-ipx","title":"Separate save_sections into explicit delete + insert operations","description":"**Current state (storage.py:243-324):**\n`save_sections` implicitly deletes all sections then re-inserts. The caller has no visibility into what's happening.\n\n```python\nasync def save_sections(document_id, sections):\n    async with AsyncSessionLocal() as session:\n        await session.execute(delete(Section)...)  # Implicit delete!\n        # ... complex recursive save logic\n```\n\n**Problem:**\n1. Implicit delete is surprising - function name suggests \"save\" not \"replace all\"\n2. Caller doesn't control the transaction boundary\n3. If insert fails after delete, data is lost\n\n**Fix:**\n- Keep `delete_sections_by_document(document_id)` - already exists at line 327\n- Create `insert_sections(document_id, sections: list[Section])` - simple bulk insert, no tree logic\n- Crawler explicitly calls both:\n```python\nawait delete_sections_by_document(doc_id)\nawait insert_sections(doc_id, flat_sections)\n```\n\nThis makes the replace-all behavior explicit and gives caller control.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.133062-05:00","updated_at":"2025-12-01T15:34:29.68655-05:00","closed_at":"2025-12-01T15:34:29.68655-05:00","dependencies":[{"issue_id":"sensei-ipx","depends_on_id":"sensei-10i","type":"blocks","created_at":"2025-12-01T13:01:31.187763-05:00","created_by":"daemon"}]}
{"id":"sensei-irnw","title":"Deploy to Fly.io","description":"Run fly deploy, verify volume mounts, test health endpoint","status":"closed","priority":1,"issue_type":"task","assignee":"user","created_at":"2025-12-17T16:06:44.520123-05:00","updated_at":"2025-12-17T18:04:08.318308-05:00","closed_at":"2025-12-17T18:04:08.318308-05:00","dependencies":[{"issue_id":"sensei-irnw","depends_on_id":"sensei-xbo9","type":"parent-child","created_at":"2025-12-17T16:06:44.52101-05:00","created_by":"daemon"},{"issue_id":"sensei-irnw","depends_on_id":"sensei-pyyh","type":"blocks","created_at":"2025-12-17T16:06:51.88867-05:00","created_by":"daemon"},{"issue_id":"sensei-irnw","depends_on_id":"sensei-2oup","type":"blocks","created_at":"2025-12-17T16:06:51.956098-05:00","created_by":"daemon"},{"issue_id":"sensei-irnw","depends_on_id":"sensei-j383","type":"blocks","created_at":"2025-12-17T16:06:52.016962-05:00","created_by":"daemon"}]}
{"id":"sensei-izi","title":"Set up Alembic for PostgreSQL migrations","description":"Replace init_db() with Alembic. PostgreSQL-only (drop SQLite). CLI-driven migrations, not at app startup.","design":"## Decisions\n- Migrations run via CLI only (avoids race conditions with Cloud Run replicas)\n- DATABASE_URL from environment variable (standard Alembic pattern)\n- Fresh start (data is disposable)\n- Full downgrade support (useful for branch switching)\n- Tests run migrations (no create_all shortcut)\n- PostgreSQL only (drop SQLite support)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T13:38:39.292935-05:00","updated_at":"2025-11-27T16:01:50.570063-05:00","closed_at":"2025-11-27T16:01:50.570063-05:00","labels":["database","migrations"]}
{"id":"sensei-j383","title":"Set up Doppler → Fly.io sync","description":"Configure Doppler integration to auto-sync secrets to Fly.io app. Create Fly.io access token, connect in Doppler dashboard.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T16:06:44.436831-05:00","updated_at":"2025-12-17T16:07:43.079435-05:00","closed_at":"2025-12-17T16:07:43.079435-05:00","dependencies":[{"issue_id":"sensei-j383","depends_on_id":"sensei-xbo9","type":"parent-child","created_at":"2025-12-17T16:06:44.438545-05:00","created_by":"daemon"}]}
{"id":"sensei-ja0","title":"Tome: Clean up Crawlee storage directory after crawl","description":"Crawlee creates storage/ directory with request queues and state. No cleanup after crawl. Accumulates disk space and could interfere with subsequent crawls.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:40.144904-05:00","updated_at":"2025-11-27T16:08:51.011648-05:00","closed_at":"2025-11-27T16:08:51.011648-05:00","labels":["minor","tome"]}
{"id":"sensei-jk3w","title":"Add /sensei slash command for explicit invocation","description":"Add a `/sensei` command in `packages/sensei-claude-code/commands/` for users who want to explicitly invoke the sensei research agent rather than relying on automatic triggering.\n\nThe command should:\n- Accept a query as an argument\n- Invoke the sensei-researcher agent\n- Optionally specify focus areas (docs, code, web)","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-17T12:19:05.541613-05:00","updated_at":"2025-12-17T12:19:05.541613-05:00"}
{"id":"sensei-jmk","title":"Tome: Add content-type checking for responses","description":"Design doc mentions is_markdown_content() but not implemented. Could prevent crawling non-markdown responses (HTML, JSON). Consider adding content-type validation.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-11-27T11:44:46.408882-05:00","updated_at":"2025-11-27T16:10:14.644596-05:00","closed_at":"2025-11-27T16:10:14.644596-05:00","labels":["minor","tome"]}
{"id":"sensei-jq6z","title":"Create run_agent task function for pydantic-evals","description":"Create async task function that wraps create_agent() for pydantic-evals:\n```python\nasync def run_agent(query: str) -\u003e str:\n    agent = create_agent(instrument=True)\n    deps = Deps()\n    async with agent:\n        result = await agent.run(query, deps=deps)\n    return result.output\n```\nThis is the function passed to `dataset.evaluate_sync()`.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:23.092304-05:00","updated_at":"2025-12-12T11:41:51.176696-05:00","closed_at":"2025-12-12T11:41:51.176696-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-jq6z","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.144937-05:00","created_by":"daemon"}]}
{"id":"sensei-jwz","title":"Remove search_queries_fts alias and library/version params from search_queries","description":"storage.py:312 has `search_queries_fts = search_queries` as backwards-compat alias that should be removed.\n\nAdditionally, search_queries() should NOT accept library/version parameters - the cache search should be broad, letting the agent decide relevance from results.\n\nChanges needed:\n1. Remove the alias `search_queries_fts = search_queries` from storage.py:312\n2. Remove library/version params from search_queries() signature\n3. Update all call sites to use `search_queries` directly:\n   - core.py:12, 94, 155\n   - kura/tools.py:81\n   - sub_agent.py:12, 77\n4. Remove library/version from kura/tools.py search_cache() and kura/server.py search()\n\nThis supersedes sensei-rpo which only addressed the naming issue.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-30T06:12:30.344411-05:00","updated_at":"2025-11-30T07:53:07.226884-05:00","closed_at":"2025-11-30T07:53:07.226884-05:00","labels":["breaking-change","cleanup","database"]}
{"id":"sensei-k1i","title":"Mount kura MCP server in main Sensei app","description":"Add HTTP endpoint for kura MCP at /kura/mcp in sensei/__main__.py, similar to how Scout is mounted at /scout/mcp.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.197141-05:00","updated_at":"2025-11-27T08:58:06.405082-05:00","closed_at":"2025-11-27T08:58:06.405082-05:00","dependencies":[{"issue_id":"sensei-k1i","depends_on_id":"sensei-fnh","type":"blocks","created_at":"2025-11-27T08:55:36.561192-05:00","created_by":"daemon"}]}
{"id":"sensei-k644","title":"Auto-ingest unknown domains in tome MCP tools","description":"When LLM calls get() or search() for a domain we haven't ingested, automatically crawl it first then return results.","design":"MCP layer orchestrates auto-ingest, service layer stays pure. Flow: check has_active_documents() → if false, ingest → call service function.","acceptance_criteria":"- get() auto-ingests unknown domains\n- search() auto-ingests unknown domains\n- ingest() MCP tool removed\n- Detailed error messages for failed ingests\n- Log warning if ingest succeeds but 0 docs","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-02T12:08:53.321315-05:00","updated_at":"2025-12-02T12:13:04.42767-05:00","closed_at":"2025-12-02T12:13:04.42767-05:00"}
{"id":"sensei-k7u","title":"Move search_sections_fts presentation logic to service layer","description":"**Current state (storage.py:460-572):**\n`search_sections_fts` mixes query logic with presentation/transformation:\n\n1. **Path normalization** (lines 514-515):\n```python\nprefix = path if path.startswith(\"/\") else f\"/{path}\"\n```\n\n2. **Snippet formatting** embedded in SQL (line 547-548):\n```python\nts_headline('english', fp.content, ..., 'MaxWords=50, MinWords=20, StartSel=**, StopSel=**')\n```\n\n3. **SearchResult construction** (lines 563-572):\n```python\nreturn [\n    SearchResult(\n        url=row.url,\n        path=row.path,\n        snippet=row.snippet,\n        rank=row.rank,\n        heading_path=row.heading_path or \"\",\n    )\n    for row in rows\n]\n```\n\n**Problem:** Storage layer should be a thin persistence layer. Path normalization and result transformation are business/presentation logic that belongs in the service layer.\n\n**Fix:**\n1. Storage returns raw row data (namedtuple or dataclass with url, path, content, rank, heading_path)\n2. Service layer handles:\n   - Path prefix normalization before calling storage\n   - Snippet generation (or pass snippet config to storage)\n   - SearchResult construction\n\n**Pattern to follow:** `get_sections_for_toc` returns raw tuples, `_build_toc_tree` in service.py does the transformation. Same pattern should apply here.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:19:45.516871-05:00","updated_at":"2025-12-01T15:39:06.149685-05:00","closed_at":"2025-12-01T15:39:06.149685-05:00","labels":["refactoring","service-layer","storage"]}
{"id":"sensei-kcp","title":"Extract crawler business logic from save_document_metadata","description":"**Current state (storage.py:186-240):**\nThe function does too much:\n1. Checks if document exists by URL\n2. Compares content hashes to decide if update needed\n3. Decides to skip/update/insert based on business logic\n4. Actually saves\n\n**Problem:** Storage layer is making decisions about crawl behavior. This couples storage to crawler logic.\n\n**New context:** With generation-based crawls (sensei-67v), we no longer need skip logic. Every crawl is a full re-process - we insert new documents with the new generation_id, then atomically swap.\n\n**Fix:** Storage becomes simple CRUD:\n- `insert_document(domain, url, path, content_hash, generation_id) -\u003e Document` - just insert, no upsert logic\n- Remove `save_document_metadata` entirely\n- Remove `SaveResult` enum (no longer needed)\n\nCrawler handles orchestration:\n```python\ngen_id = uuid4()\n# For each document:\ndoc = await insert_document(\n    domain=domain, url=url, path=path,\n    content_hash=hash_value, generation_id=gen_id\n)\nsections = flatten_section_tree(chunk_markdown(content), doc.id)\nawait insert_sections(sections)\n\n# After crawl completes:\nawait activate_generation(domain, gen_id)\nawait cleanup_old_generations(domain)\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:20.971057-05:00","updated_at":"2025-12-01T13:34:48.13617-05:00","closed_at":"2025-12-01T13:34:48.13617-05:00","dependencies":[{"issue_id":"sensei-kcp","depends_on_id":"sensei-67v","type":"related","created_at":"2025-12-01T13:23:12.191412-05:00","created_by":"daemon"}]}
{"id":"sensei-ke1","title":"Create initial migration with all 3 tables","description":"Create 001_initial_schema.py with queries, ratings, tome_documents tables. Include all constraints, indexes, foreign keys. Full downgrade support.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.319483-05:00","updated_at":"2025-11-27T16:01:50.401331-05:00","closed_at":"2025-11-27T16:01:50.401331-05:00","labels":["database","migrations"],"dependencies":[{"issue_id":"sensei-ke1","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.894251-05:00","created_by":"daemon"},{"issue_id":"sensei-ke1","depends_on_id":"sensei-1ha","type":"blocks","created_at":"2025-11-27T13:39:08.212697-05:00","created_by":"daemon"},{"issue_id":"sensei-ke1","depends_on_id":"sensei-198","type":"blocks","created_at":"2025-11-27T13:39:08.264434-05:00","created_by":"daemon"}]}
{"id":"sensei-kj3p","title":"Add multi-model DeepEval benchmarking matrix","description":"The current DeepEval harness evaluates only the default Sensei model. There is no way to run the same Goldens across multiple frontier models and compare per-metric performance.\n\nWork needed: parameterize evaluation runs over a model list and produce a comparable report (per metric + overall) for each model.","acceptance_criteria":"- Harness can execute the same Golden set against multiple models in one run.\n- Outputs include per-model metric scores and an aggregate comparison.\n- Easy to add/remove models without rewriting tests.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T20:42:29.825736-05:00","updated_at":"2025-12-12T11:41:39.494614-05:00","closed_at":"2025-12-12T11:41:39.494614-05:00"}
{"id":"sensei-kt2","title":"Tome: Create Domain value object with normalization","description":"Create a Domain value object that normalizes on construction: strips protocol, removes www. prefix, removes default ports (80/443), lowercases. This upstream fix simplifies issues sensei-543 (subdomain), sensei-ddo (ports), sensei-225 (validation).","design":"```python\n@dataclass(frozen=True)\nclass Domain:\n    value: str\n    \n    def __post_init__(self):\n        normalized = self._normalize(self.value)\n        object.__setattr__(self, 'value', normalized)\n    \n    @staticmethod\n    def _normalize(raw: str) -\u003e str:\n        if \"://\" in raw:\n            raw = raw.split(\"://\", 1)[1]\n        raw = raw.split(\"/\")[0]\n        raw = raw.removeprefix(\"www.\")\n        raw = re.sub(r':(?:80|443)$', '', raw)\n        return raw.lower()\n```\n\nOnce this exists:\n- is_same_domain() becomes: `Domain(url1) == Domain(url2)`\n- ingest_domain() validates by constructing Domain\n- No scattered normalization logic","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-11-27T13:28:46.143648-05:00","updated_at":"2025-11-27T13:42:03.480346-05:00","closed_at":"2025-11-27T13:42:03.480346-05:00","labels":["architecture","critical","tome"]}
{"id":"sensei-kte","title":"Integrate tome server into Sensei agent","description":"Wire up the tome MCP server to the main Sensei agent.\n\n**Changes to `sensei/agent.py`:**\n\n1. Import tome server factory:\n```python\nfrom sensei.tools.tome import create_tome_server\n```\n\n2. Add to toolsets in `create_agent()`:\n```python\ntoolsets=[\n    create_context7_server(settings.context7_api_key),\n    create_tavily_server(settings.tavily_api_key),\n    create_scout_server(),\n    create_kura_server(),\n    create_tome_server(),  # NEW\n],\n```\n\n**Update prompts (optional):**\nConsider adding guidance to `sensei/prompts.py` about when to use tome vs context7:\n- **tome**: For domains you've explicitly ingested, want fresh/authoritative llms.txt content\n- **context7**: For general library lookups, unknown domains, semantic search\n\n**Testing:**\n- Verify agent can call tome_get and tome_search\n- Test with real ingested domain\n- Verify graceful handling when domain not ingested","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.998726-05:00","updated_at":"2025-12-01T08:56:20.092922-05:00","closed_at":"2025-12-01T08:56:20.092922-05:00","labels":["agent","integration","tome"],"dependencies":[{"issue_id":"sensei-kte","depends_on_id":"sensei-z1e","type":"blocks","created_at":"2025-12-01T08:38:58.878001-05:00","created_by":"daemon"},{"issue_id":"sensei-kte","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.811449-05:00","created_by":"daemon"}]}
{"id":"sensei-kvi","title":"Fix test_search_cache_tool test for search_queries signature change","description":"test_cache.py::test_search_cache_tool fails because it expects `search_queries(string)` but the function now takes `search_queries(list[str])`.\n\nThe mock assertion expects:\n```\nsearch_queries('React hooks', limit=5)\n```\n\nBut actual call is:\n```\nsearch_queries(['React', 'hooks'], limit=5)\n```\n\nUpdate the test to match the new signature.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-01T07:37:26.099678-05:00","updated_at":"2025-12-01T14:01:55.481507-05:00","closed_at":"2025-12-01T14:01:55.481507-05:00","labels":["bug","testing"]}
{"id":"sensei-l5w","title":"Global mutable state in exec_plan.py is not request-scoped","description":"tools/exec_plan.py:18 uses module-level mutable dict `_exec_plans: Dict[str, str] = {}` for storing ExecPlans.\n\nProblems:\n- Not thread-safe for concurrent requests\n- No cleanup mechanism if clear_plan() isn't called\n- State persists across requests in same process\n\nFix options:\n1. Use contextvars for request-scoped state\n2. Move to database or Redis for persistence\n3. Attach to Deps object which is per-request","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:03:17.301718-05:00","updated_at":"2025-11-30T09:13:01.94712-05:00","closed_at":"2025-11-30T09:13:01.94712-05:00","labels":["architecture","concurrency"]}
{"id":"sensei-lglw","title":"Remove ExecPlan and subagent references from Claude Code agent","description":"The sensei agent markdown references features that only exist in the PydanticAI agent:\n\n1. **ExecPlan** (line 106): \"Use ExecPlan to track your branching paths of discovery\"\n2. **Subagents** (lines 69-79): \"strongly consider spawning subagents\"\n\nThese features don't exist in the Claude Code plugin - only in the standalone PydanticAI agent.\n\nOptions:\nA. Create separate agent markdown files for Claude Code vs PydanticAI\nB. Remove these sections from the shared agent markdown\nC. Make the prompt generation context-aware (already has `build_prompt(context)` in prompts.py)\n\nThe agent markdown in `packages/sensei-claude-code/agents/sensei.md` is currently duplicating content from `sensei/prompts.py`. Consider whether to:\n- Keep them separate (CC plugin vs PydanticAI have different capabilities)\n- Generate the CC agent from the same prompt components","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-17T12:19:05.853903-05:00","updated_at":"2025-12-18T09:00:21.29271-05:00","closed_at":"2025-12-18T09:00:21.29271-05:00"}
{"id":"sensei-ljd","title":"Write CLAUDE.md snippet for plugin README","description":"Create instructions for users to copy into their CLAUDE.md. Prioritize spawning sensei agent for ANY research task. Document MCP tools (scout, kura) as secondary/direct access options.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.317689-05:00","updated_at":"2025-11-27T08:58:45.077401-05:00","closed_at":"2025-11-27T08:58:45.077401-05:00"}
{"id":"sensei-ljg","title":"Add search_documents_fts() to storage layer","description":"Add PostgreSQL full-text search function to storage layer for tome_search.\n\n**Function signature:**\n```python\nasync def search_documents_fts(\n    domain: str,\n    query: str,\n    paths: list[str] | None = None,\n    limit: int = 10,\n) -\u003e list[SearchResult]\n```\n\n**Behavior:**\n- Use `plainto_tsquery()` or `websearch_to_tsquery()` for query parsing\n- Filter by domain (required)\n- Optionally filter by path prefixes (e.g., paths=[\"/hooks\"] matches \"/hooks/useState\")\n- Return ranked results with:\n  - url, path, title (first H1 or path)\n  - snippet (ts_headline for context)\n  - relevance score\n\n**SQL approach:**\n```sql\nSELECT url, path, \n       ts_headline('english', content, query) as snippet,\n       ts_rank(search_vector, query) as rank\nFROM documents\nWHERE domain = :domain\n  AND search_vector @@ websearch_to_tsquery('english', :query)\n  AND (path LIKE ANY(:path_patterns) OR :path_patterns IS NULL)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Domain model:**\nAdd `SearchResult` to `sensei/types.py`:\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T08:38:36.641793-05:00","updated_at":"2025-12-01T08:43:58.268791-05:00","closed_at":"2025-12-01T08:43:58.268791-05:00","labels":["fts","storage","tome"]}
{"id":"sensei-lnu","title":"Build and publish sensei to PyPI","description":"Use uv build and uv publish to publish package","notes":"Package built successfully. Waiting for PyPI account recovery to publish. Run `uv publish` when ready.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:15:01.305215-05:00","updated_at":"2025-11-27T06:33:16.918807-05:00","closed_at":"2025-11-27T06:33:16.918807-05:00","dependencies":[{"issue_id":"sensei-lnu","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:15:01.305847-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-u5g","type":"blocks","created_at":"2025-11-27T06:15:07.495691-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-3na","type":"blocks","created_at":"2025-11-27T06:15:07.520515-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-afa","type":"blocks","created_at":"2025-11-27T06:15:07.548945-05:00","created_by":"daemon"}]}
{"id":"sensei-lw13","title":"Refactor agent from singleton to fresh-per-request pattern","description":"Refactor agent usage from singleton pattern to fresh agent per request to fix cancel scope corruption issues.\n\n## Root Cause\nThe singleton agent shares toolset state (`_exit_stack`, `_entered_count`) across concurrent requests, causing anyio cancel scope corruption. See pydantic-ai#2818.\n\n## Solution\nCreate fresh agent per request. No `async with agent:` wrapper needed because:\n- `agent.run()` internally uses `async with self.iter(...)` which manages toolset lifecycle\n- `agent.run_stream_events()` calls `self.run()` internally\n- VercelAIAdapter calls `agent.run_stream_events()` via `run_stream()`\n\nFresh agent = fresh toolsets = isolated state per request.\n\n## Locations requiring changes\n\n### Production code (MUST change):\n\n1. **sensei/core.py:9** - Change import from `from sensei.agent import agent` to `from sensei.agent import create_agent`\n\n2. **sensei/core.py:45** - `stream_query()` function\n   ```python\n   # Before\n   async for event in agent.run_stream_events(enhanced_query, deps=deps):\n   \n   # After  \n   agent = create_agent()\n   async for event in agent.run_stream_events(enhanced_query, deps=deps):\n   ```\n\n3. **sensei/core.py:86** - `handle_query()` function\n   ```python\n   # Before\n   result = await agent.run(enhanced_query, deps=deps)\n   \n   # After\n   agent = create_agent()\n   result = await agent.run(enhanced_query, deps=deps)\n   ```\n\n4. **sensei/api/__init__.py:38** - Change import from `from sensei.agent import agent` to `from sensei.agent import create_agent`\n\n5. **sensei/api/__init__.py:180-184** - `chat()` endpoint\n   ```python\n   # Before\n   adapter = VercelAIAdapter(\n       agent=agent,\n       ...\n   )\n   \n   # After\n   adapter = VercelAIAdapter(\n       agent=create_agent(),\n       ...\n   )\n   ```\n\n### Scripts (SHOULD change for consistency):\n\n6. **scripts/run_query.py:14** - Change import\n7. **scripts/run_query.py:44** - Create fresh agent in `run_query()`\n\n### Tests (MAY need updates):\n\n8. **tests/test_cache.py:230** - Accesses `agent._function_toolset.tools` for introspection. Need to either:\n   - Create a fresh agent for the test\n   - Or keep a test-only import if introspection is needed\n\n9. **tests/test_cancel_scope_bug_real.py:266** - Uses singleton for bug reproduction. May want to keep as-is to verify the bug is fixed.\n\n### Final cleanup:\n\n10. **sensei/agent.py:163** - Remove singleton: `agent = create_agent()`\n\n## Already correct (no change needed):\n- `sensei/eval/task.py` - creates fresh agent with `async with agent:`\n- `sensei/dojo/optimizer.py` - creates fresh agent per call\n- `scripts/dump_full_request.py` - creates fresh agent\n- `spawn_sub_agent()` in agent.py - creates sub-agent per call","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T07:05:05.300794-05:00","updated_at":"2025-12-20T07:28:41.87117-05:00","closed_at":"2025-12-20T07:28:41.87117-05:00","dependencies":[{"issue_id":"sensei-lw13","depends_on_id":"sensei-s2s3","type":"blocks","created_at":"2025-12-20T07:05:05.308471-05:00","created_by":"daemon"}]}
{"id":"sensei-lzn","title":"Stub out sensei/dojo/ module","description":"Create sensei/dojo/ with __init__.py and README.md describing the objective: DSPy prompt optimization using MIPROv2 with user feedback ratings.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T11:16:07.203856-05:00","updated_at":"2025-11-27T11:28:17.946563-05:00","closed_at":"2025-11-27T11:28:17.946563-05:00","dependencies":[{"issue_id":"sensei-lzn","depends_on_id":"sensei-9pf","type":"parent-child","created_at":"2025-11-27T11:16:07.205529-05:00","created_by":"daemon"}]}
{"id":"sensei-m7g","title":"Update tome service layer for section-based retrieval","description":"Update service.py with new section-based functions.\n\n**Update existing:**\n- `tome_get(domain, path)` - concatenate all sections by position\n- `tome_get(domain, path, heading)` - NEW param: get section subtree\n- `tome_search()` - use section search, return heading_path in results\n\n**Add new:**\n- `tome_toc(domain, path) -\u003e TOC` - return heading structure derived from sections\n\n**TOC structure:**\n```python\n@dataclass\nclass TOCEntry:\n    heading: str\n    level: int\n    children: list[TOCEntry]\n\ndef tome_toc(domain: str, path: str) -\u003e Success[list[TOCEntry]] | NoResults:\n    # Query sections, build tree from parent_section_id relationships\n```\n\n**tome_get with heading:**\n```python\nasync def tome_get(\n    domain: str, \n    path: str, \n    heading: str | None = None\n) -\u003e Success[str] | NoResults:\n    if heading:\n        # Find section, get subtree, concatenate\n        sections = await storage.get_section_subtree_by_heading(domain, path, heading)\n    else:\n        # Get all sections for document\n        sections = await storage.get_sections_by_document(domain, path)\n    \n    if not sections:\n        return NoResults()\n    \n    content = \"\\n\\n\".join(s.content for s in sections)\n    return Success(content)\n```","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:57:05.474973-05:00","updated_at":"2025-12-01T10:13:43.543094-05:00","closed_at":"2025-12-01T10:13:43.543094-05:00","labels":["service","tome"],"dependencies":[{"issue_id":"sensei-m7g","depends_on_id":"sensei-v52","type":"blocks","created_at":"2025-12-01T09:57:05.477867-05:00","created_by":"daemon"},{"issue_id":"sensei-m7g","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.844978-05:00","created_by":"daemon"}]}
{"id":"sensei-mci","title":"Make scout installable via uvx --from sensei scout","description":"Add script entry point to pyproject.toml and publish sensei to PyPI so users can run `uvx --from sensei scout` for stdio MCP","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T06:14:48.004237-05:00","updated_at":"2025-11-27T06:33:16.956555-05:00","closed_at":"2025-11-27T06:33:16.956555-05:00"}
{"id":"sensei-mwh","title":"Add --force flag to crawler to overwrite existing data","description":"**Context:**\nCurrently the crawler skips documents if content_hash matches (SaveResult.SKIPPED). This is good for incremental updates but sometimes we want to force a full re-crawl.\n\n**Need:**\nA `force: bool = False` parameter on `ingest_domain()` that:\n- When `force=True`: Skip the content_hash check, always overwrite document and sections\n- When `force=False` (default): Current behavior - skip unchanged documents\n\n**Use cases:**\n1. Debugging chunker changes - need to re-process all docs with new chunking logic\n2. Schema changes - need to re-save sections with new structure\n3. Testing - ensure fresh data\n\n**Implementation:**\nAfter sensei-kcp (extract business logic from storage), the crawler will have the skip logic. Add:\n```python\nif not force:\n    existing = await get_document_by_url(url)\n    if existing and existing.content_hash == content_hash:\n        result.documents_skipped += 1\n        return\n# Always proceed if force=True or content changed\n```\n\n**CLI:**\n```bash\nuv run python scripts/ingest_domain.py crawlee.dev --force\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:03:02.680725-05:00","updated_at":"2025-12-01T13:23:18.237716-05:00","closed_at":"2025-12-01T13:23:18.237716-05:00","dependencies":[{"issue_id":"sensei-mwh","depends_on_id":"sensei-kcp","type":"blocks","created_at":"2025-12-01T13:03:02.681854-05:00","created_by":"daemon"}]}
{"id":"sensei-n82","title":"Configure Scout to use /data volume for git cache","description":"Update Scout configuration to use /data directory for cloning and caching git repositories. This path will be mounted as a persistent Fly volume.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.854244-05:00","updated_at":"2025-11-30T05:57:26.690525-05:00","closed_at":"2025-11-30T05:57:26.690525-05:00","labels":["deployment","scout"],"dependencies":[{"issue_id":"sensei-n82","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.967425-05:00","created_by":"daemon"}]}
{"id":"sensei-nbu","title":"Extend link parsing to handle all markdown link formats","description":"`parse_llms_txt_links` only handles `[text](url)` style links.\n\nMissing formats in valid markdown:\n- Reference links: `[text][ref]` with `[ref]: url`\n- Auto-links: `\u003chttps://example.com\u003e`\n- Bare URLs (some llms.txt files use these)\n\nConsider using a proper markdown parser for link extraction or extending the regex patterns.","status":"closed","priority":3,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:36.027082-05:00","updated_at":"2025-12-01T07:48:02.813748-05:00","closed_at":"2025-12-01T07:48:02.813748-05:00","labels":["p3","parser","tome"]}
{"id":"sensei-nc7","title":"Pass DocumentContent model instead of individual fields to save_document","description":"Per CLAUDE.md guidelines: \"Pass models, not individual fields\"\n\nCurrent code in crawler.py:116-123 passes 6 individual fields to save_document(). This violates the domain model pattern.\n\nFix:\n1. Create `DocumentContent` Pydantic model in `types.py`\n2. Update `save_document()` signature to accept `DocumentContent`\n3. Update crawler to construct and pass the model","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:35.744653-05:00","updated_at":"2025-12-01T07:37:26.144902-05:00","closed_at":"2025-12-01T07:37:26.144902-05:00","labels":["architecture","p1","tome"]}
{"id":"sensei-nd9","title":"Fix Crawlee storage race condition in integration tests","description":"Integration tests for tome crawler fail due to Crawlee storage race condition.\n\n**Problem:**\nWhen running multiple integration tests sequentially, Crawlee's file-based storage gets into a bad state:\n1. First test runs and creates `storage/` directory\n2. Cleanup runs after first test\n3. Second test starts but Crawlee's internal state still references deleted files\n4. `FileNotFoundError` when trying to write to deleted directory\n\n**Error:**\n```\nFileNotFoundError: [Errno 2] No such file or directory: \n'/Users/alizain/Experiments/sensei/storage/request_queues/default/...json.tmp'\n```\n\n**Solution options:**\n1. Use `MemoryStorageClient` instead of file-based storage for testing\n2. Configure unique storage directories per test\n3. Use Crawlee's `purge_request_queue=True` properly\n\n**References:**\n- https://crawlee.dev/python/docs/guides/storage-clients.md\n- Crawlee Python v1 API uses `storage_client` parameter in crawler init\n\n**Files:**\n- `sensei/tome/crawler.py` - crawler implementation\n- `tests/test_tome_service.py` - integration tests","notes":"Created as blocker for second integration test. First integration test (test_ingest_real_domain_llmstxt_org) passes.","status":"closed","priority":1,"issue_type":"bug","assignee":"claude","created_at":"2025-12-01T09:03:06.503042-05:00","updated_at":"2025-12-01T09:31:13.353195-05:00","closed_at":"2025-12-01T09:31:13.353195-05:00","labels":["crawlee","testing","tome"]}
{"id":"sensei-neo","title":"Update agent.py to use Kura via MCP toolset","description":"Replace direct cache function imports (search_cache, get_cached_response) with create_kura_server() in toolsets. Remove cache tools from tools=[] list. This makes Kura consistent with Scout - both accessed via MCP.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.683223-05:00","updated_at":"2025-11-27T09:06:08.470345-05:00","closed_at":"2025-11-27T09:06:08.470345-05:00","dependencies":[{"issue_id":"sensei-neo","depends_on_id":"sensei-x4g","type":"blocks","created_at":"2025-11-27T09:03:51.319716-05:00","created_by":"daemon"}]}
{"id":"sensei-ngv6","title":"Verify MCP server exception handling (no changes expected)","description":"Verify that `sensei/server.py` MCP handlers don't need manual capture_exception().\n\n## Analysis\nThe MCP integration (`sentry_sdk.integrations.mcp`) automatically:\n1. Wraps all tool/prompt/resource handlers\n2. Calls `sentry_sdk.capture_exception(e)` on any exception (see mcp.py lines 429, 495)\n3. Re-raises the exception\n\n## Verification\n- The catch blocks in server.py (lines 76-84, 142) convert exceptions to MCPToolError\n- These exceptions will be captured by the MCP integration wrapper BEFORE our catch blocks\n- No manual instrumentation needed\n\n## Action\n- Verify this understanding is correct by reading the code flow\n- If MCP integration doesn't capture (e.g., because we catch first), add capture_exception()\n- Document findings in this issue","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T10:16:55.087943-05:00","updated_at":"2025-12-18T10:21:02.857382-05:00","closed_at":"2025-12-18T10:21:02.857382-05:00","dependencies":[{"issue_id":"sensei-ngv6","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:55.089293-05:00","created_by":"daemon"}]}
{"id":"sensei-nym","title":"Build Tome: Documentation repository from llms.txt","description":"Canonical documentation repository built by crawling llms.txt files. Uses Crawlee HttpCrawler for orchestration (queue, dedup, retries, depth control) with plain httpx fetch. No Playwright needed since llms.txt and linked files are markdown.","design":"## Stack\n- Crawlee HttpCrawler for orchestration\n- httpx under the hood (no browser)\n- Sensei's existing database\n\n## Core function\n```python\nasync def ingest_domain(domain: str, max_depth: int = 3) -\u003e IngestResult\n```\n\n## Flow\n1. Fetch https://{domain}/llms.txt\n2. Parse markdown, extract links\n3. Enqueue same-domain links (respect depth)\n4. Store in tome_documents table\n\n## Why Crawlee HttpCrawler\n- Automatic RequestQueue with deduplication\n- Visited URL tracking\n- Retry logic with exponential backoff\n- Concurrency control\n- State persistence (resume interrupted crawls)\n- No parsing overhead (we're fetching markdown)\n\n## What we implement\n- parse_llms_txt_links() - extract URLs from llms.txt\n- is_same_domain() - domain filtering\n- is_markdown_content() - content type check\n- Storage layer integration","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T11:12:59.092712-05:00","updated_at":"2025-11-27T11:27:57.277048-05:00","closed_at":"2025-11-27T11:27:57.277048-05:00"}
{"id":"sensei-o0ms","title":"Add capture_exception() to tools/common.py wrap_tool decorator","description":"Add `sentry_sdk.capture_exception()` to the `wrap_tool` decorator in `sensei/tools/common.py`.\n\n## Location\nLines 32-39 - catch blocks for TransientError and ToolError\n\n## Implementation\n\n```python\nexcept TransientError as e:\n    sentry_sdk.capture_exception(e)  # ADD THIS\n    logfire.exception(str(e), tool=fn.__name__)\n    return f\"Tool temporarily unavailable: {e}\"\nexcept ToolError as e:\n    sentry_sdk.capture_exception(e)  # ADD THIS\n    logfire.exception(str(e), tool=fn.__name__)\n    return f\"Tool failed: {e}\"\n```\n\n## Note\n- BrokenInvariant is re-raised (line 38-39), so Sentry will capture it automatically when it bubbles up\n- No need to capture BrokenInvariant here","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-18T10:16:54.81551-05:00","updated_at":"2025-12-18T10:21:02.614075-05:00","closed_at":"2025-12-18T10:21:02.614075-05:00","dependencies":[{"issue_id":"sensei-o0ms","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:54.816542-05:00","created_by":"daemon"}]}
{"id":"sensei-o3n9","title":"Update pyproject.toml console scripts to point to server:main","description":"Change console script entry points from `.__main__:main` to `.server:main` for kura, scout, and tome. This makes console scripts point to the actual module rather than the __main__.py bootstrap file.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:44.904641-05:00","updated_at":"2025-12-02T12:38:13.253675-05:00","closed_at":"2025-12-02T12:38:13.253675-05:00","dependencies":[{"issue_id":"sensei-o3n9","depends_on_id":"sensei-s1dv","type":"blocks","created_at":"2025-12-02T12:34:44.905907-05:00","created_by":"daemon"},{"issue_id":"sensei-o3n9","depends_on_id":"sensei-fhbe","type":"blocks","created_at":"2025-12-02T12:34:44.906765-05:00","created_by":"daemon"},{"issue_id":"sensei-o3n9","depends_on_id":"sensei-vr0t","type":"blocks","created_at":"2025-12-02T12:34:44.907613-05:00","created_by":"daemon"}]}
{"id":"sensei-o50l","title":"Remove parent_id from Query model and storage","description":"Remove parent_id column from Query model and parent_id parameter from storage.save_query(). Column was never populated. Requires DB migration.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-18T07:22:45.20069-05:00","updated_at":"2025-12-18T07:36:35.507981-05:00","closed_at":"2025-12-18T07:36:35.507981-05:00"}
{"id":"sensei-o8ip","title":"Prevent spawn_sub_agent failures in DeepEval eval runs","description":"Eval tests create `Deps()` with no `query_id`, but the eval agent still exposes `spawn_sub_agent`, which raises `ToolError` when `query_id` is missing. If any Golden triggers spawning, the eval crashes and scores become meaningless.\n\nWork needed: either disable spawn tool in eval agents or supply a safe query_id/fixture so evaluation cannot fail on missing context.","acceptance_criteria":"- Goldens that would trigger spawning no longer crash eval runs.\n- Eval harness either omits spawn tool or guarantees required deps are present.\n- Regression suite remains focused on response/tool quality, not setup errors.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-11T20:42:25.035162-05:00","updated_at":"2025-12-12T11:41:38.899369-05:00","closed_at":"2025-12-12T11:41:38.899369-05:00"}
{"id":"sensei-o8yy","title":"Update README with new database setup flow","description":"Update documentation to reflect the new explicit database setup.\n\n**File:** `README.md` (or relevant docs)\n\n**Add section for local development:**\n```markdown\n## Database Setup\n\n1. Start PostgreSQL:\n   ```bash\n   brew services start postgresql@17\n   # or: docker-compose up -d postgres\n   ```\n\n2. Create database:\n   ```bash\n   createdb sensei\n   ```\n\n3. Set environment variable:\n   ```bash\n   export DATABASE_URL=\"postgresql+asyncpg://localhost/sensei\"\n   ```\n\n4. Run migrations:\n   ```bash\n   alembic upgrade head\n   ```\n```\n\n**Remove any mentions of:**\n- Automatic PostgreSQL management\n- sensei-managed mode\n- Auto-migration on startup","acceptance_criteria":"- README has clear database setup instructions\n- No mention of auto-managed PostgreSQL\n- Local dev flow documented","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T09:45:40.243352-05:00","updated_at":"2025-12-21T09:57:48.202995-05:00","closed_at":"2025-12-21T09:57:48.202995-05:00","dependencies":[{"issue_id":"sensei-o8yy","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:40.245299-05:00","created_by":"daemon"},{"issue_id":"sensei-o8yy","depends_on_id":"sensei-esxx","type":"blocks","created_at":"2025-12-21T09:46:23.410345-05:00","created_by":"daemon"}]}
{"id":"sensei-ob7g","title":"Change Query.messages from Text to JSONB","description":"Migrate the messages column to JSONB and update storage layer to preserve rich types.\n\n**Why:** JSONB enables querying message content in SQL, is more efficient for JSON data, and aligns with \"serialize at edges\" principle.\n\n**Database migration:**\n```python\n# alembic revision --autogenerate -m \"query_messages_to_jsonb\"\n\ndef upgrade():\n    # Convert existing Text JSON to JSONB\n    op.execute(\"\"\"\n        ALTER TABLE queries \n        ALTER COLUMN messages TYPE JSONB \n        USING messages::jsonb\n    \"\"\")\n\ndef downgrade():\n    op.execute(\"\"\"\n        ALTER TABLE queries \n        ALTER COLUMN messages TYPE TEXT \n        USING messages::text\n    \"\"\")\n```\n\n**Model change (sensei/database/models.py):**\n```python\nfrom sqlalchemy.dialects.postgresql import JSONB, UUID\n\nclass Query(TimestampMixin, Base):\n    # ...\n    messages = Column(JSONB, nullable=True)  # Was: Column(Text, nullable=True)\n```\n\n**Storage change (sensei/database/storage.py):**\n\nCurrent (line 58-89):\n```python\nasync def save_query(\n    ...\n    messages: bytes | None = None,  # JSON bytes from PydanticAI\n    ...\n):\n    messages=messages.decode(\"utf-8\") if messages else None,  # String\n```\n\nNew:\n```python\nasync def save_query(\n    ...\n    messages: list[dict] | None = None,  # Rich type, SQLAlchemy handles JSONB\n    ...\n):\n    messages=messages,  # Direct assignment, SQLAlchemy serializes to JSONB\n```\n\n**Verification:**\n- Existing queries with text messages should auto-convert via `USING messages::jsonb`\n- New queries store native JSONB\n- Retrieval returns Python dicts, not strings","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:48:17.78421-05:00","updated_at":"2025-12-06T12:37:33.488811-05:00","closed_at":"2025-12-06T12:37:33.488811-05:00","labels":["database","eval","migration"],"dependencies":[{"issue_id":"sensei-ob7g","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:48:46.421901-05:00","created_by":"daemon"}]}
{"id":"sensei-on8","title":"Create packages/www marketing site with React Router v7 Framework Mode","description":"Single-page product landing page for Sensei using React Router v7 Framework Mode, TailwindCSS, and React. Minimal/clean design, CTA links to GitHub README for setup instructions.","design":"## Technical Stack\n- React Router v7 Framework Mode (Vite-based)\n- TailwindCSS for styling\n- TypeScript\n- SSR enabled for SEO\n\n## Package Structure\npackages/www/\n├── package.json\n├── react-router.config.ts\n├── vite.config.ts\n├── tsconfig.json\n├── tailwind.config.ts\n├── src/\n│   ├── root.tsx (HTML shell)\n│   ├── routes.ts (route definitions)\n│   ├── routes/\n│   │   └── home.tsx (landing page)\n│   └── app.css (Tailwind imports)\n\n## Integration\n- Integrates with existing turbo monorepo\n- Uses npm workspaces (packages/www)\n- Follows existing biome/typescript conventions","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T13:22:14.763677-05:00","updated_at":"2025-11-27T16:14:31.647865-05:00","closed_at":"2025-11-27T16:14:31.647865-05:00"}
{"id":"sensei-ori","title":"Re-enable aider-chat and repo_map functionality","description":"aider-chat was disabled due to dependency conflict with pydantic-ai:\n- aider-chat pins openai==1.99.1\n- pydantic-ai requires openai\u003e=1.107.2\n\nWhen aider-chat updates to support openai\u003e=1.107.2, uncomment:\n1. pyproject.toml - aider-chat dependency (line 17)\n2. sensei/scout/repomap.py - entire file\n3. sensei/scout/server.py - repo_map tool and imports (lines 28-92)\n4. sensei/scout/__init__.py - repomap exports (lines 41-42, 63-65)\n5. tests/test_scout.py - TestRepoMap class (lines 448-488)","status":"open","priority":3,"issue_type":"chore","created_at":"2025-11-30T10:35:49.916993-05:00","updated_at":"2025-11-30T10:35:49.916993-05:00","labels":["blocked-upstream","dependencies"]}
{"id":"sensei-p57r","title":"Export main() from sensei/scout/__init__.py","description":"Add main to __all__ exports so users can import it: `from sensei.scout import main`","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:36.412005-05:00","updated_at":"2025-12-02T12:37:14.726751-05:00","closed_at":"2025-12-02T12:37:14.726751-05:00","dependencies":[{"issue_id":"sensei-p57r","depends_on_id":"sensei-yonb","type":"blocks","created_at":"2025-12-02T12:34:36.413454-05:00","created_by":"daemon"}]}
{"id":"sensei-pok5","title":"Remove ensure_migrated helper and unused database.local artifacts","description":"Remove sensei/database/local.py and any tests/docs referencing ensure_migrated since migrations are now explicit via alembic CLI.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:12:15.622628-05:00","updated_at":"2025-12-21T17:15:11.384662-05:00","closed_at":"2025-12-21T17:15:11.384662-05:00"}
{"id":"sensei-psb","title":"Add integration tests for tome crawler","description":"Only `test_tome_parser.py` exists - tests parsing logic but not:\n- The crawler behavior with mocked HTTP responses\n- Database integration (save_document flow)\n- Error scenarios (network failures, invalid content)\n\nAdd integration tests using httpx mock or similar to test the full crawl flow.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T07:30:36.182239-05:00","updated_at":"2025-12-02T12:45:20.358087-05:00","closed_at":"2025-12-02T12:45:20.358087-05:00","labels":["p3","testing","tome"],"dependencies":[{"issue_id":"sensei-psb","depends_on_id":"sensei-3ao","type":"related","created_at":"2025-12-01T07:30:44.050631-05:00","created_by":"daemon"}]}
{"id":"sensei-pyyh","title":"Create Dockerfile for Fly.io","description":"Create Dockerfile with Python 3.13 + uv, git (apt), rg + lstr (cargo). Entry point: uv run start","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T16:06:44.224365-05:00","updated_at":"2025-12-17T16:09:37.725721-05:00","closed_at":"2025-12-17T16:09:37.725721-05:00","dependencies":[{"issue_id":"sensei-pyyh","depends_on_id":"sensei-xbo9","type":"parent-child","created_at":"2025-12-17T16:06:44.229502-05:00","created_by":"daemon"}]}
{"id":"sensei-qm8","title":"Convert IngestResult from dataclass to Pydantic model","description":"IngestResult is currently a dataclass while the rest of the codebase uses Pydantic models. This breaks consistency and loses validation benefits.\n\nConvert to Pydantic BaseModel for consistency with other domain models.","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:35.813873-05:00","updated_at":"2025-12-01T07:39:27.481527-05:00","closed_at":"2025-12-01T07:39:27.481527-05:00","labels":["consistency","p1","tome"]}
{"id":"sensei-qow","title":"Add Tome storage functions","description":"Add storage functions to sensei/database/storage.py: save_tome_document(), get_tome_document_by_url(), search_tome_documents(), delete_tome_documents_by_domain(). Upsert logic based on url, update if content_hash changed.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:02.359116-05:00","updated_at":"2025-11-27T11:26:50.761285-05:00","closed_at":"2025-11-27T11:26:50.761285-05:00","dependencies":[{"issue_id":"sensei-qow","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:02.361933-05:00","created_by":"daemon"},{"issue_id":"sensei-qow","depends_on_id":"sensei-3c9","type":"blocks","created_at":"2025-11-27T11:16:18.290789-05:00","created_by":"daemon"}]}
{"id":"sensei-qxlo","title":"Update QueryResult to include messages for tracing","description":"Update QueryResult to include messages with `Field(exclude=True)`.\n\n**Depends on:** sensei-ob7g (JSONB migration) must complete first.\n\n**Purpose:** Make messages available for tracing/evaluation without exposing in API responses.\n\n**Changes:**\n\n1. **types.py - QueryResult:**\n```python\nfrom pydantic import BaseModel, Field\n\nclass QueryResult(BaseModel):\n    \"\"\"Result of a query operation.\"\"\"\n    \n    query_id: UUID = Field(..., description=\"Unique identifier for this query\")\n    output: str = Field(..., description=\"Documentation response with code examples\")\n    messages: list[dict] | None = Field(\n        default=None, \n        exclude=True,  # Not serialized to JSON for API responses\n        description=\"Message history including tool calls (for evaluation)\",\n    )\n```\n\n2. **core.py - handle_query():**\n```python\nasync def handle_query(...) -\u003e QueryResult:\n    result = await agent.run(enhanced_query, deps=deps)\n    \n    # Get messages as list[dict] for tracing\n    # PydanticAI's new_messages_json() returns bytes, decode and parse\n    messages_json = result.new_messages_json()\n    messages = json.loads(messages_json) if messages_json else None\n    \n    query_id = await _save_query_result(\n        query, result.output, messages,\n        language, library, version,\n    )\n    \n    return QueryResult(\n        query_id=query_id,\n        output=result.output,  # Note: renamed from markdown\n        messages=messages,\n    )\n```\n\n3. **Verify unchanged:**\n   - MCP server returns `result.output` only - messages never sent to production LLMs\n   - API JSON responses don't include messages due to `exclude=True`\n\n**Evaluation usage:**\n```python\n# In eval/runner.py\nresult = await core.handle_query(query, library=library)\ntrace_data = extract_trace_data(result.messages)  # Direct access\nupdate_current_trace(\n    input=query,\n    output=result.output,\n    tools_called=trace_data[\"tools_called\"],\n    ...\n)\n```\n\n**Note:** Also renames `markdown` to `output` for consistency (see sensei-2p9l).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:34:45.48476-05:00","updated_at":"2025-12-06T13:05:59.170688-05:00","closed_at":"2025-12-06T13:05:59.170688-05:00","labels":["api-change","eval"],"dependencies":[{"issue_id":"sensei-qxlo","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:15.872992-05:00","created_by":"daemon"},{"issue_id":"sensei-qxlo","depends_on_id":"sensei-ob7g","type":"blocks","created_at":"2025-12-06T10:48:46.455993-05:00","created_by":"daemon"}]}
{"id":"sensei-r0g","title":"Add retry logic to tome crawler","description":"A single network hiccup currently fails the entire crawl. Crawlee supports request retries but they're not configured.\n\nFix: Configure `max_request_retries=3` (or similar) in HttpCrawler initialization.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T07:30:35.883197-05:00","updated_at":"2025-12-02T12:42:18.769515-05:00","closed_at":"2025-12-02T12:42:18.769515-05:00","labels":["p2","robustness","tome"]}
{"id":"sensei-r4h","title":"Handle PostgreSQL tsvector size limit for large documents","description":"PostgreSQL's tsvector has a hard limit of ~1MB (1048575 bytes). Large llms-full.txt files (e.g., crawlee.dev/python at 1.1MB) exceed this limit, causing errors:\n\n```\nERROR: string is too long for tsvector (1111118 bytes, max 1048575 bytes)\n```\n\n**Current behavior:** The error is silently ignored somewhere, but documents aren't being saved properly.\n\n**Solution:** Truncate content for FTS indexing while keeping full content for retrieval. Change the computed column to truncate at 1MB:\n\n```sql\nto_tsvector('english', left(content, 1000000))\n```\n\nThis ensures:\n- Full content is stored and retrievable via tome_get\n- Search works on first ~1MB of content (sufficient for most use cases)\n- No errors on large documents","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-01T09:32:06.018975-05:00","updated_at":"2025-12-01T09:56:06.984668-05:00","closed_at":"2025-12-01T09:56:06.984668-05:00","labels":["database","fts","p0","tome"]}
{"id":"sensei-r556","title":"Implement pytest integration with native PydanticAI integration","description":"Create test_regression.py with pytest + DeepEval using native PydanticAI integration.\n\n**Create:** `tests/eval/test_regression.py`\n\n```python\n\"\"\"Regression tests for Sensei response quality.\n\nUses DeepEval's native PydanticAI integration via ConfidentInstrumentationSettings.\n\"\"\"\n\nimport asyncio\nimport os\n\nimport pytest\nfrom deepeval.dataset import EvaluationDataset\n\nfrom sensei.eval import load_goldens\nfrom sensei.eval.runner import create_eval_agent\n\npytestmark = [pytest.mark.eval, pytest.mark.asyncio]\n\n\n@pytest.fixture(scope=\"module\")\ndef eval_agent(eval_metrics):\n    \"\"\"Create eval agent with metrics.\"\"\"\n    return create_eval_agent(eval_metrics)\n\n\n@pytest.mark.parametrize(\n    \"golden\",\n    load_goldens(),\n    ids=lambda g: g.additional_metadata.get(\"id\", \"unknown\") if g.additional_metadata else \"unknown\"\n)\nasync def test_response_quality(golden, eval_agent):\n    \"\"\"Evaluate response quality for each Golden.\"\"\"\n    dataset = EvaluationDataset(goldens=[golden])\n    \n    for g in dataset.evals_iterator():\n        task = asyncio.create_task(eval_agent.run(g.input))\n        dataset.evaluate(task)\n    \n    # TODO: Enable hard assertions once dataset is validated\n    # Check dataset.test_results for failures\n```\n\n**Add pytest marker to pyproject.toml:**\n```toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"integration: marks tests as integration tests\",\n    \"eval: marks tests as evaluation tests (require LLM API calls)\",\n]\n```\n\n**CLI usage:**\n```bash\n# Run all eval tests (fast mode by default)\nuv run pytest tests/eval/ -m eval -v\n\n# Run with accurate eval model\nEVAL_ACCURATE=1 uv run pytest tests/eval/ -m eval -v\n\n# Run specific library\nuv run pytest tests/eval/ -m eval -k \"fastapi\"\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:57.676039-05:00","updated_at":"2025-12-06T13:05:59.108019-05:00","closed_at":"2025-12-06T13:05:59.108019-05:00","labels":["eval","testing"],"dependencies":[{"issue_id":"sensei-r556","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.271057-05:00","created_by":"daemon"},{"issue_id":"sensei-r556","depends_on_id":"sensei-i26s","type":"blocks","created_at":"2025-12-06T10:36:31.226605-05:00","created_by":"daemon"},{"issue_id":"sensei-r556","depends_on_id":"sensei-8gqb","type":"blocks","created_at":"2025-12-06T10:36:31.350831-05:00","created_by":"daemon"}]}
{"id":"sensei-rcrf","title":"Refactor database engine to idiomatic module-level singleton","description":"Current database setup uses lazy initialization with mutable globals (`_engine`, `_async_session_local`). Refactor to idiomatic Python pattern with module-level constants and a testing hook.\n\n**Current** (storage.py):\n```python\n_engine = None\n_async_session_local = None\n\ndef _get_engine():\n    global _engine\n    if _engine is None:\n        _engine = create_async_engine(...)\n    return _engine\n```\n\n**Target** (new database/engine.py):\n```python\nengine = create_async_engine(\n    sensei_settings.database_url,\n    pool_pre_ping=True,\n    pool_size=5,\n    max_overflow=10,\n)\n\nasync_session_factory = async_sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False,\n)\n\n# Testing hook - allows test fixtures to override session behavior\n_test_session_factory: async_sessionmaker | None = None\n\ndef get_session_factory() -\u003e async_sessionmaker:\n    \"\"\"Get session factory, preferring test override if set.\"\"\"\n    return _test_session_factory or async_session_factory\n\ndef set_test_session_factory(factory: async_sessionmaker | None) -\u003e None:\n    \"\"\"Set test session factory override (for transaction rollback pattern).\"\"\"\n    global _test_session_factory\n    _test_session_factory = factory\n\nasync def dispose_engine() -\u003e None:\n    await engine.dispose()\n```\n\nThis enables:\n1. Proper engine disposal in lifespan\n2. Test fixtures to override session factory for transaction rollback isolation","acceptance_criteria":"- [ ] Create `sensei/database/engine.py` with module-level engine and session factory\n- [ ] Add `pool_pre_ping=True` and connection pool config to engine\n- [ ] Add `dispose_engine()` function for lifespan cleanup\n- [ ] Add `get_session_factory()` accessor function\n- [ ] Add `set_test_session_factory()` hook for test overrides\n- [ ] Update `storage.py` to use `get_session_factory()` instead of direct access\n- [ ] Update lifespan in `unified.py` to call `dispose_engine()` on shutdown\n- [ ] Remove lazy init functions from storage.py","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T06:26:02.182877-05:00","updated_at":"2025-12-20T13:00:19.535924-05:00","closed_at":"2025-12-20T13:00:19.535924-05:00"}
{"id":"sensei-rhx","title":"Tome: Use typed exceptions instead of generic Exception","description":"crawler.py:100-101 catches generic Exception and appends string to errors list. Should use TransientError for network issues, BrokenInvariant for config issues per CLAUDE.md.","notes":"System thinking review: Part of error boundary confusion. Errors should be typed internally, only stringified at API edge. Consider grouping with Result type fix (sensei-hon).","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:43:44.722869-05:00","updated_at":"2025-11-27T13:42:03.880413-05:00","closed_at":"2025-11-27T13:42:03.880413-05:00","labels":["critical","tome"]}
{"id":"sensei-rkc","title":"ingest.py -vv doesn't show all found links","description":"The `-vv` flag should enable DEBUG logging which shows all links found during crawl. Currently the debug logs in crawler.py:212-218 aren't appearing. Need to investigate why DEBUG level isn't propagating to the crawler logger.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-02T09:42:07.192667-05:00","updated_at":"2025-12-02T09:54:50.022384-05:00","closed_at":"2025-12-02T09:54:50.022384-05:00"}
{"id":"sensei-rpo","title":"Resolve search_queries_fts usage - remove or rename consistently","description":"Multiple files import search_queries_fts from storage.py. Instead of adding a backwards-compat alias, decide: rename all usages to search_queries, or keep search_queries_fts as the canonical name. Files affected: sensei/core.py, sensei/kura/tools.py, sensei/tools/sub_agent.py, tests/test_cache.py","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:10:31.621192-05:00","updated_at":"2025-11-30T06:17:58.10903-05:00","closed_at":"2025-11-30T06:17:58.10903-05:00","labels":["cleanup","database"]}
{"id":"sensei-rzw8","title":"Remove lifespan from kura/server.py","description":"Remove the lifespan function from kura server. It only existed to call ensure_db_ready().\n\n**File:** `sensei/kura/server.py`\n\n**Remove:**\n- The entire `lifespan` async context manager function\n- The `asynccontextmanager` import from contextlib\n- The `lifespan=lifespan` parameter from FastMCP constructor","design":"```python\n# Before\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\nmcp = FastMCP(name=\"kura\", lifespan=lifespan)\n\n# After\nmcp = FastMCP(name=\"kura\")\n```\n\nThis enables FastMCP direct mounting (faster than proxy mounting).","acceptance_criteria":"- No lifespan function\n- No asynccontextmanager import\n- FastMCP created without lifespan parameter\n- Server still works when run standalone","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.614672-05:00","updated_at":"2025-12-21T09:57:26.361605-05:00","closed_at":"2025-12-21T09:57:26.361605-05:00","dependencies":[{"issue_id":"sensei-rzw8","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.619035-05:00","created_by":"daemon"},{"issue_id":"sensei-rzw8","depends_on_id":"sensei-zujl","type":"blocks","created_at":"2025-12-21T09:46:23.088714-05:00","created_by":"daemon"}]}
{"id":"sensei-s1dv","title":"Simplify sensei/kura/__main__.py to just bootstrap python -m","description":"Reduce __main__.py to its single purpose: enabling `python -m sensei.kura`. Just import main from server and call it in if __name__ block.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:15.578886-05:00","updated_at":"2025-12-02T12:37:14.467662-05:00","closed_at":"2025-12-02T12:37:14.467662-05:00","dependencies":[{"issue_id":"sensei-s1dv","depends_on_id":"sensei-5r2g","type":"blocks","created_at":"2025-12-02T12:34:15.580255-05:00","created_by":"daemon"}]}
{"id":"sensei-s2s3","title":"Cancel scope misalignment in concurrent MCP tool execution","description":"RuntimeError: Attempted to exit a cancel scope that isn't the current tasks's current cancel scope\n\nOccurs in fastmcp tool execution when multiple tools run concurrently. Stack trace shows anyio cancel scope issue in mcp/shared/session.py and fastmcp/tools/tool_manager.py.\n\nTriggered when running parallel tavily_search calls followed by context7 calls.","notes":"## Resolution\n\nFixed by refactoring from singleton agent to fresh-agent-per-request pattern (sensei-lw13).\n\n### Changes made:\n1. `sensei/core.py` - Create fresh agent in `stream_query()` and `handle_query()`\n2. `sensei/api/__init__.py` - Create fresh agent for VercelAIAdapter\n3. `sensei/agent.py` - Removed singleton export\n4. `sensei/eval/task.py` - Removed redundant `async with agent:` wrapper (caused double-entry of context)\n5. `scripts/run_query.py` - Updated to use `create_agent()`\n6. `tests/test_cache.py` - Updated to create fresh agent for introspection test\n\nRoot cause: Singleton agent shared toolset state across concurrent requests, corrupting anyio cancel scopes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-18T19:51:24.903341-05:00","updated_at":"2025-12-20T07:28:52.584069-05:00","closed_at":"2025-12-20T07:28:52.584069-05:00"}
{"id":"sensei-sop","title":"Update README with PostgreSQL installation instructions","description":"Update documentation to explain the PostgreSQL requirement.\n\n**File:** `README.md`\n\n**Content to add:**\n\n### Prerequisites section\n```markdown\n## Prerequisites\n\n### PostgreSQL 17+\n\nSensei requires PostgreSQL to be installed on your system. Install it using your package manager:\n\n**macOS:**\n```bash\nbrew install postgresql@17\n```\n\n**Ubuntu/Debian:**\n```bash\nsudo apt install postgresql-17\n```\n\n**Windows:**\nDownload from https://www.postgresql.org/download/windows/\n\n**Note:** You don't need to configure PostgreSQL or create databases manually. Sensei automatically manages a local PostgreSQL instance in `~/.sensei/pgdata/` using a Unix socket (no port conflicts with system PostgreSQL).\n```\n\n### For development\n```markdown\n## Development Setup\n\nSet `SENSEI_HOME=.sensei` in your `.env` file to keep development data local to the repo instead of `~/.sensei/`.\n```\n\n**No CLI commands section** - Sensei auto-manages PostgreSQL, no manual commands needed.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T06:11:21.329657-05:00","updated_at":"2025-12-02T08:05:52.883375-05:00","closed_at":"2025-12-02T08:05:52.883375-05:00","dependencies":[{"issue_id":"sensei-sop","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:21.331393-05:00","created_by":"daemon"},{"issue_id":"sensei-sop","depends_on_id":"sensei-6ul","type":"blocks","created_at":"2025-12-02T06:11:34.975338-05:00","created_by":"daemon"}]}
{"id":"sensei-sp6","title":"Simplify get_sections_by_document with a JOIN","description":"**Current state (storage.py:342-371):**\nTwo separate queries:\n```python\nasync def get_sections_by_document(domain, path):\n    # Query 1: Find document\n    doc_result = await session.execute(\n        select(Document).where(Document.domain == domain, Document.path == path)\n    )\n    doc = doc_result.scalar_one_or_none()\n    if not doc:\n        return []\n    # Query 2: Get sections\n    result = await session.execute(\n        select(Section).where(Section.document_id == doc.id).order_by(Section.position)\n    )\n```\n\n**Problem:** Two queries when one JOIN would do.\n\n**Fix:**\n```python\nasync def get_sections_by_document(domain: str, path: str) -\u003e list[Section]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(Section)\n            .join(Document, Document.id == Section.document_id)\n            .where(Document.domain == domain, Document.path == path)\n            .order_by(Section.position)\n        )\n        return list(result.scalars().all())\n```\n\nSingle query, same result. Also simpler to read.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T13:01:21.405961-05:00","updated_at":"2025-12-01T15:35:00.784237-05:00","closed_at":"2025-12-01T15:35:00.784237-05:00"}
{"id":"sensei-st72","title":"Consolidate age computation to single source","description":"Age is computed in 3 places:\n1. **SQL** - `storage.py:125` - `EXTRACT(DAY FROM NOW() - inserted_at)` → goes into `CacheHit.age_days`\n2. **Python** - `kura/tools.py:18-32` - `_compute_age_days(inserted_at)` recomputes from timestamp\n3. **Display** - `agent.py:50` - formats `hit.age_days`\n\nProblem: SQL computes age into `CacheHit.age_days`, but `kura/tools.py:40` ignores it and recomputes from `query.inserted_at`.\n\nFix: Either trust `CacheHit.age_days` everywhere, or compute in one canonical Python function and use it everywhere.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-12-20T15:02:15.182548-05:00","updated_at":"2025-12-20T15:02:15.182548-05:00","labels":["lifetime-audit","tech-debt"]}
{"id":"sensei-t0t","title":"Deploy Sensei to Fly.io with persistent volume","description":"Deploy Sensei HTTP server to Fly.io with Neon Postgres and persistent volume for Scout's git repo caching. Replaces Cloud Run approach due to filesystem persistence requirements.","design":"## Why Fly.io over Cloud Run?\n- Scout needs persistent filesystem for git repo caching\n- Cloud Run filesystem is ephemeral (resets on restart/scale)\n- Fly.io volumes provide real persistent disk at $0.15/GB/month\n\n## Architecture\n- Single Fly Machine with 1GB RAM\n- 10GB persistent volume mounted at /data for Scout cache\n- Neon Postgres for database (already configured)\n- Stateless MCP endpoints for horizontal scaling potential\n\n## Components\n- fly.toml configuration\n- Dockerfile (already exists, needs gcc for numpy)\n- Volume mount for /data\n- Environment secrets for API keys and DATABASE_URL","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T15:59:54.321949-05:00","updated_at":"2025-11-30T05:57:26.462284-05:00","closed_at":"2025-11-30T05:57:26.462284-05:00","labels":["deployment","fly.io"]}
{"id":"sensei-t4oq","title":"Make DeepEval regression harness deterministic/repeatable","description":"Current DeepEval regression tests run the full Sensei agent with live toolsets and no cached contexts, so outputs/tool behavior can drift over time. This undermines regression value and makes frontier-model comparisons noisy.\n\nWork needed: design a deterministic eval mode (e.g., fixture/replay/stub for tools or traces, stable dataset inputs, controlled caching) so repeated runs produce stable scores and tool-call patterns.","acceptance_criteria":"- Running evals multiple times on same commit yields stable metric scores within tight tolerance.\n- Harness can run without live network/tool dependencies (or clearly separates offline vs online modes).\n- Tool-call traces used for scoring are reproducible across runs.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T20:42:19.997983-05:00","updated_at":"2025-12-12T11:41:38.978431-05:00","closed_at":"2025-12-12T11:41:38.978431-05:00"}
{"id":"sensei-tkwi","title":"Remove ensure_db_ready from scripts/ingest.py","description":"Remove database initialization from the ingest script. It should assume DB is ready.\n\n**File:** `scripts/ingest.py`\n\n**Remove:**\n- Import of `ensure_db_ready` from database.local\n- Call to `await ensure_db_ready()` before ingestion","design":"The script should assume DATABASE_URL is set and database is ready. If not, it will fail with a clear error on first DB operation.","acceptance_criteria":"- No import of ensure_db_ready\n- No call to ensure_db_ready\n- Script still works when DATABASE_URL is set and DB is ready","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:40.024377-05:00","updated_at":"2025-12-21T09:57:35.946451-05:00","closed_at":"2025-12-21T09:57:35.946451-05:00","dependencies":[{"issue_id":"sensei-tkwi","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:40.026041-05:00","created_by":"daemon"},{"issue_id":"sensei-tkwi","depends_on_id":"sensei-zujl","type":"blocks","created_at":"2025-12-21T09:46:23.17362-05:00","created_by":"daemon"}]}
{"id":"sensei-tsbj","title":"Move build_enhanced_query to build.py","description":"Move _build_enhanced_query from core.py to build.py. Consolidates all pre-agent preparation into the build module.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-18T07:22:45.011557-05:00","updated_at":"2025-12-18T07:24:08.142251-05:00","closed_at":"2025-12-18T07:24:08.142251-05:00"}
{"id":"sensei-tueu","title":"Remove PostgreSQL paths from paths.py","description":"Remove PostgreSQL-specific path functions that are no longer needed.\n\n**File:** `sensei/paths.py`\n\n**Remove:**\n- `get_pgdata()` - PostgreSQL data directory\n- `get_pg_log()` - PostgreSQL log file  \n- `get_local_database_url()` - Unix socket URL for local PostgreSQL\n\n**Keep:**\n- `get_sensei_home()` - Still used for scout cache\n- `get_scout_repos()` - Scout repository cache","design":"1. Read sensei/paths.py\n2. Delete the three functions: get_pgdata, get_pg_log, get_local_database_url\n3. Keep get_sensei_home and get_scout_repos\n4. Run `uv run ruff check sensei/paths.py` to verify no syntax errors","acceptance_criteria":"- get_pgdata, get_pg_log, get_local_database_url removed\n- get_sensei_home, get_scout_repos remain\n- No import errors when importing sensei.paths","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.287278-05:00","updated_at":"2025-12-21T09:57:06.969872-05:00","closed_at":"2025-12-21T09:57:06.969872-05:00","dependencies":[{"issue_id":"sensei-tueu","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.292727-05:00","created_by":"daemon"}]}
{"id":"sensei-u30l","title":"Update get() and search() MCP tools to use auto-ingest","description":"Call _ensure_domain_ingested before calling service functions","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T12:09:02.926211-05:00","updated_at":"2025-12-02T12:11:50.747211-05:00","closed_at":"2025-12-02T12:11:50.747211-05:00","dependencies":[{"issue_id":"sensei-u30l","depends_on_id":"sensei-k644","type":"parent-child","created_at":"2025-12-02T12:09:02.927195-05:00","created_by":"daemon"}]}
{"id":"sensei-u5g","title":"Remove hardcoded API keys from config.py","description":"Replace hardcoded API keys with empty defaults before publishing to PyPI - SECURITY CRITICAL","status":"closed","priority":0,"issue_type":"task","created_at":"2025-11-27T06:15:01.270024-05:00","updated_at":"2025-11-27T06:17:25.346124-05:00","closed_at":"2025-11-27T06:17:25.346124-05:00","dependencies":[{"issue_id":"sensei-u5g","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:15:01.270764-05:00","created_by":"daemon"}]}
{"id":"sensei-u6r","title":"Add rate limiting / 429 handling to tome crawler","description":"The crawler doesn't handle HTTP 429 (Too Many Requests) or respect `Retry-After` headers. Some documentation sites will throttle aggressive crawlers.\n\nAdd request rate limiting or handle 429 responses with exponential backoff.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-01T07:30:36.103623-05:00","updated_at":"2025-12-01T07:30:36.103623-05:00","labels":["p3","politeness","tome"]}
{"id":"sensei-ubc2","title":"Clarify and mark DeepEval regression tests as integration/CI-safe","description":"DeepEval regression tests currently rely on live network/tool calls but are not marked as integration tests and lack clear CI guidance. This risks flaky CI and confusing contributor expectations.\n\nWork needed: add proper pytest markers/docs for online vs offline evals and ensure CI does not run networked evals unintentionally.","acceptance_criteria":"- Eval tests are clearly labeled as integration/online where appropriate.\n- Documentation explains how to run evals locally vs in CI.\n- CI configuration avoids running networked DeepEval evals by default.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T20:42:39.860825-05:00","updated_at":"2025-12-12T11:41:39.306804-05:00","closed_at":"2025-12-12T11:41:39.306804-05:00"}
{"id":"sensei-ucwq","title":"Refactor test_db fixture to use transaction rollback instead of TRUNCATE","description":"Replace TRUNCATE-based test isolation with transaction rollback for O(1) cleanup instead of O(n).\n\n**Challenge**: Storage functions commit explicitly (`await session.commit()` in every write operation). Need savepoints/nested transactions so rollback undoes all commits.\n\n**Depends on**: sensei-2fya (test fixtures using new engine pattern with `set_test_session_factory()` hook)\n\n**Files involved**:\n- `tests/conftest.py` - test_db fixture\n- `sensei/database/engine.py` - testing hook (from sensei-rcrf)","design":"## Approach: Connection-bound sessions with outer transaction\n\nUses the `set_test_session_factory()` hook from sensei-rcrf to override session creation.\n\n```python\nfrom sensei.database.engine import engine, set_test_session_factory\n\n@pytest_asyncio.fixture\nasync def test_db():\n    # Migrations run once per session (separate fixture)\n    \n    # Start outer transaction on a connection\n    async with engine.connect() as conn:\n        trans = await conn.begin()\n        \n        # Create session factory bound to this connection\n        # When sessions \"commit\", they commit to a savepoint\n        bound_session_factory = async_sessionmaker(\n            bind=conn,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )\n        \n        # Override session factory via testing hook\n        set_test_session_factory(bound_session_factory)\n        \n        yield engine\n        \n        # Rollback outer transaction - undoes ALL test changes including commits\n        await trans.rollback()\n        \n        # Clear testing hook\n        set_test_session_factory(None)\n\n# Separate session-scoped fixture for migrations (run once)\n@pytest_asyncio.fixture(scope=\"session\")\nasync def _run_migrations():\n    await run_migrations(TEST_DATABASE_URL)\n```\n\n**Key SQLAlchemy behavior**:\n- Session bound to connection with active transaction → `session.commit()` becomes savepoint\n- `trans.rollback()` at end rolls back everything including all savepoint commits\n- Standard pattern for test isolation with explicit commits\n\n**If savepoints don't auto-work**, add event listener:\n```python\n@event.listens_for(session, \"after_transaction_end\")\ndef restart_savepoint(session, trans):\n    if trans.nested and not trans._parent.nested:\n        session.begin_nested()\n```","acceptance_criteria":"- [ ] `test_db` fixture uses transaction rollback via `set_test_session_factory()` hook\n- [ ] Remove TRUNCATE cleanup from fixture\n- [ ] Split migrations to session-scoped fixture (run once per test session)\n- [ ] All existing tests pass without modification\n- [ ] Test isolation verified: data from one test not visible in next\n- [ ] Performance improvement measured (optional but nice to have)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T07:53:17.364939-05:00","updated_at":"2025-12-20T13:11:08.026892-05:00","closed_at":"2025-12-20T13:11:08.026892-05:00","dependencies":[{"issue_id":"sensei-ucwq","depends_on_id":"sensei-2fya","type":"blocks","created_at":"2025-12-20T12:55:39.408749-05:00","created_by":"daemon"}]}
{"id":"sensei-und8","title":"Create unified API server (separate from MCP server)","description":"Consolidate into single api/__init__.py. Keep ALL existing endpoints: /query, /query/stream, /rate, /health. Add CORS middleware. Keep unified.py separate for stdio MCP clients.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T18:30:56.615659-05:00","updated_at":"2025-12-17T18:31:11.739566-05:00","closed_at":"2025-12-17T18:31:11.739566-05:00","dependencies":[{"issue_id":"sensei-und8","depends_on_id":"sensei-5t3e","type":"parent-child","created_at":"2025-12-17T18:30:56.619196-05:00","created_by":"daemon"}]}
{"id":"sensei-uuli","title":"Rewrite sensei/eval/__init__.py for pydantic-evals","description":"Replace current loader with native pydantic-evals API:\n- `load_dataset(name: str) -\u003e Dataset[str, str, Any]` using `Dataset.from_file()`\n- Export public API for evaluators and runner","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:23.026792-05:00","updated_at":"2025-12-12T11:41:51.094747-05:00","closed_at":"2025-12-12T11:41:51.094747-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-uuli","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.10254-05:00","created_by":"daemon"},{"issue_id":"sensei-uuli","depends_on_id":"sensei-z2ox","type":"blocks","created_at":"2025-12-12T09:42:42.970264-05:00","created_by":"daemon"}]}
{"id":"sensei-uy71","title":"Implement YAML loader for minimal Goldens","description":"Create loader.py to load test cases from YAML as minimal Golden objects (input only).\n\n**Create:** `sensei/eval/loader.py`\n\n```python\n\"\"\"Load evaluation Goldens from YAML files.\"\"\"\n\nfrom pathlib import Path\n\nimport yaml\nfrom deepeval.dataset import Golden\n\nDATASETS_DIR = Path(__file__).parent.parent.parent / \"tests\" / \"eval\" / \"datasets\"\n\n\ndef load_goldens(\n    datasets_dir: Path | None = None,\n    filter_library: str | None = None,\n    filter_category: str | None = None,\n) -\u003e list[Golden]:\n    \"\"\"\n    Load test cases from YAML files as Golden objects.\n    \n    Goldens contain ONLY input - all other fields (actual_output, tools_called,\n    retrieval_context) are populated by tracing during evaluation.\n    \n    Args:\n        datasets_dir: Directory containing YAML files (default: tests/eval/datasets/)\n        filter_library: Only load cases for this library\n        filter_category: Only load cases in this category\n    \n    Returns:\n        List of Golden objects ready for evaluation\n    \"\"\"\n    dir_path = datasets_dir or DATASETS_DIR\n    goldens = []\n    \n    for yaml_file in dir_path.glob(\"*.yaml\"):\n        if yaml_file.name.startswith(\"_\"):  # Skip schema files\n            continue\n        \n        with open(yaml_file) as f:\n            data = yaml.safe_load(f)\n        \n        metadata = data.get(\"metadata\", {})\n        library = metadata.get(\"library\")\n        \n        if filter_library and library != filter_library:\n            continue\n        \n        for case in data.get(\"cases\", []):\n            category = case.get(\"category\")\n            if filter_category and category != filter_category:\n                continue\n            \n            # Minimal golden - only input required\n            # Tracing populates actual_output, tools_called, retrieval_context\n            golden = Golden(\n                input=case[\"query\"],\n                additional_metadata={\n                    \"id\": case[\"id\"],\n                    \"library\": library,\n                    \"category\": category,\n                    \"source_file\": yaml_file.name,\n                },\n            )\n            goldens.append(golden)\n    \n    return goldens\n```\n\n**YAML schema (for reference):**\n```yaml\nmetadata:\n  library: fastapi  # Optional, for filtering\n\ncases:\n  - id: fastapi-di-01\n    query: \"How do I use dependency injection in FastAPI?\"\n    category: api-usage  # Optional, for filtering\n```\n\n**What changed from original design:**\n- Removed `expected_answer` field - not needed for our metrics\n- Removed `difficulty` field - not used\n- Golden only has `input` + metadata for organization\n- All runtime data comes from tracing","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T10:35:13.522112-05:00","updated_at":"2025-12-06T13:00:15.126525-05:00","closed_at":"2025-12-06T13:00:15.126525-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-uy71","depends_on_id":"sensei-1ntj","type":"parent-child","created_at":"2025-12-06T10:36:16.063856-05:00","created_by":"daemon"},{"issue_id":"sensei-uy71","depends_on_id":"sensei-ip47","type":"blocks","created_at":"2025-12-06T10:36:30.942599-05:00","created_by":"daemon"}]}
{"id":"sensei-v3l4","title":"Add _ensure_domain_ingested() helper to server layer","description":"Add helper that checks has_active_documents, ingests if needed, returns error string or None","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T12:09:02.882045-05:00","updated_at":"2025-12-02T12:11:13.174334-05:00","closed_at":"2025-12-02T12:11:13.174334-05:00","dependencies":[{"issue_id":"sensei-v3l4","depends_on_id":"sensei-k644","type":"parent-child","created_at":"2025-12-02T12:09:02.882892-05:00","created_by":"daemon"}]}
{"id":"sensei-v52","title":"Update storage layer for Section-based model","description":"Update storage.py to work with the new Section-based model.\n\n**Remove/modify:**\n- `save_document()` - no longer saves content, just metadata\n- `get_document_by_url()` - keep for metadata lookup\n- `search_documents_fts()` - replace with section-based search\n\n**Add:**\n- `save_sections(document_id: UUID, sections: list[SectionData]) -\u003e None` - bulk insert sections\n- `get_sections_by_document(document_id: UUID) -\u003e list[Section]` - ordered by position\n- `get_section_subtree(section_id: UUID) -\u003e list[Section]` - recursive CTE for heading param\n- `search_sections_fts(domain, query, paths, limit) -\u003e list[SearchResult]` - search with heading_path\n- `get_heading_path(section_id: UUID) -\u003e str` - derive breadcrumb via recursive CTE\n\n**SearchResult update:**\n- Add `heading_path: str` field for breadcrumb context","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:46.544518-05:00","updated_at":"2025-12-01T10:12:45.227035-05:00","closed_at":"2025-12-01T10:12:45.227035-05:00","labels":["database","storage","tome"],"dependencies":[{"issue_id":"sensei-v52","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T09:56:46.546785-05:00","created_by":"daemon"},{"issue_id":"sensei-v52","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.759238-05:00","created_by":"daemon"}]}
{"id":"sensei-v7g","title":"Add tests for tome service and FTS","description":"Comprehensive tests for the new tome functionality.\n\n**New test file:** `tests/test_tome_service.py`\n\n**Test cases:**\n\n### tome_get tests\n- `test_tome_get_index_returns_llms_txt` - INDEX sentinel\n- `test_tome_get_full_returns_llms_full_txt` - FULL sentinel  \n- `test_tome_get_specific_path` - regular path lookup\n- `test_tome_get_not_found_returns_no_results` - missing doc\n- `test_tome_get_domain_not_ingested` - domain never crawled\n\n### tome_search tests\n- `test_tome_search_finds_matching_content` - basic FTS\n- `test_tome_search_respects_path_filter` - paths=[\"/hooks\"] works\n- `test_tome_search_empty_paths_searches_all` - paths=[] searches everything\n- `test_tome_search_ranks_by_relevance` - better matches first\n- `test_tome_search_returns_snippets` - ts_headline works\n- `test_tome_search_no_results` - query matches nothing\n\n### Integration tests\n- `test_ingest_then_search` - full flow: ingest domain, search it\n- `test_ingest_fetches_both_index_and_full` - crawler change works\n\n**Fixtures needed:**\n- Test documents with known content for FTS testing\n- Mock HTTP responses for crawler tests","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:38.496488-05:00","updated_at":"2025-12-01T09:03:06.564384-05:00","closed_at":"2025-12-01T09:03:06.564384-05:00","labels":["testing","tome"],"dependencies":[{"issue_id":"sensei-v7g","depends_on_id":"sensei-8qn","type":"blocks","created_at":"2025-12-01T08:38:58.919769-05:00","created_by":"daemon"},{"issue_id":"sensei-v7g","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.856582-05:00","created_by":"daemon"}]}
{"id":"sensei-vae","title":"Add error handling tests for tome service","description":"The tome service layer needs tests for error conditions.\n\n**Error cases to test:**\n- `tome_search` with empty query raises `ToolError`\n- `tome_search` with whitespace-only query raises `ToolError`\n- `tome_get` with invalid domain format\n- Database connection errors (mock storage layer failures)\n\n**Note:** Some of these are already in `test_tome_service.py` but should verify the full error chain.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T09:04:15.939836-05:00","updated_at":"2025-12-02T12:44:05.959165-05:00","closed_at":"2025-12-02T12:44:05.959165-05:00","labels":["testing","tome"]}
{"id":"sensei-vbq","title":"Generation cleanup logic not working - 0 old documents cleaned","description":"After re-ingesting llmstxt.org, the log shows \"Cleaned up 0 old documents\" but there should have been old documents from the previous generation to clean up. The generation-based crawl logic may have issues with:\n1. Old documents not being marked correctly\n2. cleanup_old_generations() not finding the right documents\n3. The activate_generation() swap not working as expected\n\nNeed to trace the generation lifecycle: insert with generation_active=false → activate_generation() → cleanup_old_generations()","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-02T09:42:07.252679-05:00","updated_at":"2025-12-02T09:44:13.589473-05:00","closed_at":"2025-12-02T09:44:13.589473-05:00"}
{"id":"sensei-vr0t","title":"Simplify sensei/tome/__main__.py to just bootstrap python -m","description":"Reduce __main__.py to its single purpose: enabling `python -m sensei.tome`. Just import main from server and call it in if __name__ block.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:34:15.672686-05:00","updated_at":"2025-12-02T12:37:14.591133-05:00","closed_at":"2025-12-02T12:37:14.591133-05:00","dependencies":[{"issue_id":"sensei-vr0t","depends_on_id":"sensei-32xv","type":"blocks","created_at":"2025-12-02T12:34:15.673825-05:00","created_by":"daemon"}]}
{"id":"sensei-vrp","title":"Refactor core.py and sub_agent.py to reduce duplication","description":"Comprehensive cleanup of core/sub-agent code:\n1. Extract shared query preparation in core.py (_prepare_query, _save_query_result)\n2. Unify agent creation in agent.py with create_agent() factory\n3. Simplify sub_agent.py to use shared agent factory\n4. Standardize import style (module imports)","design":"## Design\n\n### 1. Extract shared query preparation (core.py)\n- `_prepare_query()` - builds enhanced query and deps with cache hits\n- `_save_query_result()` - saves query result to storage\n\n### 2. Unify agent creation (agent.py)\n- `create_agent(include_spawn, include_exec_plan)` - configurable factory\n- `create_sub_agent()` - wrapper for sub-agents (no spawn, no exec_plan)\n- Same model (gemini) for all agents\n\n### 3. Simplify sub_agent.py\n- Remove agent creation (use agent.py factory)\n- Remove grok model setup\n- Standardize imports to module style\n\n### 4. Import standardization\n- Use `from sensei.database import storage` pattern\n- Call as `storage.search_queries()` etc.\n\n## Acceptance\n- No duplicate query preparation logic in core.py\n- Single agent factory in agent.py\n- sub_agent.py uses shared factory\n- Consistent import style across files\n- All tests pass","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-30T09:44:29.41995-05:00","updated_at":"2025-11-30T09:49:35.975193-05:00","closed_at":"2025-11-30T09:49:35.975193-05:00"}
{"id":"sensei-vxc","title":"Remove hardcoded API keys from config.py","description":"Replace hardcoded API keys with empty defaults before publishing to PyPI","status":"closed","priority":0,"issue_type":"task","created_at":"2025-11-27T06:14:56.372621-05:00","updated_at":"2025-11-27T06:15:23.094068-05:00","closed_at":"2025-11-27T06:15:23.094068-05:00"}
{"id":"sensei-w1qn","title":"Add has_active_documents() to storage layer","description":"Add storage function to check if domain has any active documents: SELECT 1 FROM documents WHERE domain = :domain AND generation_active = true LIMIT 1","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T12:09:02.814985-05:00","updated_at":"2025-12-02T12:10:32.992811-05:00","closed_at":"2025-12-02T12:10:32.992811-05:00","dependencies":[{"issue_id":"sensei-w1qn","depends_on_id":"sensei-k644","type":"parent-child","created_at":"2025-12-02T12:09:02.818218-05:00","created_by":"daemon"}]}
{"id":"sensei-w2z7","title":"Verify langfuse integration is working after get_client removal","description":"The `langfuse` import and `get_client()` call were removed from `sensei/agent.py`, but `Agent.instrument_all()` remains on line 30. Langfuse needs to be re-added for production tracing.\n\nCurrent state:\n- `langfuse\u003e=3.10.5` is still in dependencies (pyproject.toml:27)\n- `Agent.instrument_all()` is called (agent.py:30)\n- The `langfuse = get_client()` line was removed\n\nRequirements:\n1. Re-add explicit `get_client()` call to initialize langfuse for production tracing\n2. Langfuse must NOT interfere with logfire instrumentation used for evals\n   - Evals use logfire/pydantic-evals for span collection\n   - Both should coexist: langfuse for prod observability, logfire for eval spans\n3. Verify both work independently when their respective env vars are set","acceptance_criteria":"- [ ] `langfuse.get_client()` is called in agent.py\n- [ ] Langfuse traces appear in dashboard when LANGFUSE_* env vars are set\n- [ ] Logfire/pydantic-evals still captures spans correctly for eval runs\n- [ ] No conflicts between the two instrumentation systems","notes":"Fixed by adding Langfuse settings to config.py (langfuse_public_key, langfuse_secret_key, langfuse_host) and initializing Langfuse with explicit credentials in agent.py instead of using get_client() which reads env vars directly.","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-12-12T12:19:03.298466-05:00","updated_at":"2025-12-12T16:42:24.03961-05:00","closed_at":"2025-12-12T16:34:54.876664-05:00"}
{"id":"sensei-wawt","title":"Add Firecrawl as a tool","description":"Add Firecrawl integration as a tool for web scraping and crawling capabilities. Firecrawl provides structured web content extraction that can be useful for gathering information from websites.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-17T13:49:50.087769-05:00","updated_at":"2025-12-17T13:49:50.087769-05:00"}
{"id":"sensei-wkhi","title":"Add sentry_sdk.init() to sensei/api/__init__.py","description":"Add Sentry initialization at the TOP of `sensei/api/__init__.py`, BEFORE any other imports.\n\n## Implementation\n\n```python\n# sensei/api/__init__.py - VERY TOP OF FILE\nimport os\nimport sentry_sdk\n\nsentry_sdk.init(dsn=os.environ.get(\"SENTRY_DSN\"))\n\n# ... rest of existing imports below\n```\n\n## Why top of file?\n- Sentry patches `fastapi.routing.get_request_handler` at init time\n- These patches must happen BEFORE `from fastapi import FastAPI`\n- If DSN is None (not set), init is a no-op (uses NonRecordingClient)\n\n## Do NOT\n- Add integrations parameter (auto-detected)\n- Add traces_sample_rate (keep defaults for now)\n- Put in lifespan (too late, app already created)\n- Import sentry_sdk elsewhere (single init point)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-18T10:16:54.576467-05:00","updated_at":"2025-12-18T10:21:02.47028-05:00","closed_at":"2025-12-18T10:21:02.47028-05:00","dependencies":[{"issue_id":"sensei-wkhi","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:54.579128-05:00","created_by":"daemon"}]}
{"id":"sensei-wv9g","title":"Create standalone eval runner CLI","description":"Create `sensei/eval/runner.py` for standalone execution:\n- `python -m sensei.eval` entry point\n- Options: `--dataset NAME`, `--filter TAG`, `--output FILE`\n- Richer report output than pytest (for exploration/debugging)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T09:42:23.286128-05:00","updated_at":"2025-12-12T11:41:51.426866-05:00","closed_at":"2025-12-12T11:41:51.426866-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-wv9g","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.280934-05:00","created_by":"daemon"},{"issue_id":"sensei-wv9g","depends_on_id":"sensei-uuli","type":"blocks","created_at":"2025-12-12T09:42:43.143631-05:00","created_by":"daemon"},{"issue_id":"sensei-wv9g","depends_on_id":"sensei-jq6z","type":"blocks","created_at":"2025-12-12T09:42:43.190643-05:00","created_by":"daemon"}]}
{"id":"sensei-x4g","title":"Create create_kura_server() for PydanticAI integration","description":"Create sensei/tools/kura.py with create_kura_server() function that returns MCPServerStreamableHTTP pointing to /kura/mcp with tool_prefix=\"kura\". Mirror the pattern from create_scout_server().","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.636612-05:00","updated_at":"2025-11-27T09:04:39.288012-05:00","closed_at":"2025-11-27T09:04:39.288012-05:00"}
{"id":"sensei-x5d","title":"Add MCP server tests for tome tools","description":"The tome MCP server (`sensei/tome/server.py`) has no direct tests.\n\n**What to test:**\n- `tome.get()` tool - verify it calls service layer correctly, formats output\n- `tome.search()` tool - verify result formatting with `_format_search_results()`\n- `tome.ingest()` tool - verify it calls crawler and formats summary\n\n**Pattern to follow:**\nLook at how `tests/test_kura.py` or similar tests the kura MCP server tools.\n\n**Files:**\n- `sensei/tome/server.py` - server to test\n- `tests/test_tome_server.py` - new test file to create","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T09:04:15.815641-05:00","updated_at":"2025-12-02T12:41:42.825857-05:00","closed_at":"2025-12-02T12:41:42.825857-05:00","labels":["mcp","testing","tome"]}
{"id":"sensei-xbo9","title":"Deploy Sensei to Fly.io","description":"Deploy the full Sensei stack (API + Scout + Kura + Tome) to Fly.io with persistent volume for Scout repo cache.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-17T16:06:32.459308-05:00","updated_at":"2025-12-17T18:04:08.408038-05:00","closed_at":"2025-12-17T18:04:08.408038-05:00"}
{"id":"sensei-xnl","title":"Create TypeScript configuration","description":"Set up tsconfig.json for the www package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.211914-05:00","updated_at":"2025-11-27T13:27:48.524646-05:00","closed_at":"2025-11-27T13:27:48.524646-05:00","dependencies":[{"issue_id":"sensei-xnl","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.214195-05:00","created_by":"daemon"},{"issue_id":"sensei-xnl","depends_on_id":"sensei-apn","type":"blocks","created_at":"2025-11-27T13:22:35.47179-05:00","created_by":"daemon"}]}
{"id":"sensei-y1tq","title":"Remove ingest() MCP tool","description":"Auto-ingest makes explicit ingest tool redundant","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-02T12:09:02.971177-05:00","updated_at":"2025-12-02T12:12:29.202049-05:00","closed_at":"2025-12-02T12:12:29.202049-05:00","dependencies":[{"issue_id":"sensei-y1tq","depends_on_id":"sensei-k644","type":"parent-child","created_at":"2025-12-02T12:09:02.971972-05:00","created_by":"daemon"}]}
{"id":"sensei-yeq","title":"Update Kura documentation from SQLite to PostgreSQL","description":"kura/__init__.py:7 still references \"SQLite database\" but code uses PostgreSQL:\n```python\n\"\"\"All tools connect to the same SQLite database as the main Sensei app.\"\"\"\n```\nShould say PostgreSQL.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-11-30T10:05:21.494993-05:00","updated_at":"2025-11-30T10:12:40.272084-05:00","closed_at":"2025-11-30T10:12:40.272084-05:00"}
{"id":"sensei-ynz","title":"Create scripts/ingest_domain.py CLI script for crawling","description":"**Context:**\nCurrently, to ingest a domain we need to write Python code inline:\n```python\nDATABASE_URL=\"...\" uv run python -c \"\nimport asyncio\nfrom sensei.tome.crawler import ingest_domain\nasyncio.run(ingest_domain('crawlee.dev'))\n\"\n```\n\n**Need:**\nA proper CLI script in `scripts/ingest_domain.py` that:\n- Takes domain as argument\n- Optionally takes max_depth\n- Handles DATABASE_URL from environment or .env\n- Prints progress and results nicely\n\n**Usage:**\n```bash\nuv run python scripts/ingest_domain.py crawlee.dev\nuv run python scripts/ingest_domain.py crawlee.dev --max-depth 2\nuv run python scripts/ingest_domain.py crawlee.dev --force  # see sensei-xxx for force flag\n```\n\n**Existing scripts for reference:**\n- `scripts/build_plugin.py`\n- `scripts/dump_full_request.py`","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:03:02.631597-05:00","updated_at":"2025-12-02T12:40:31.129514-05:00","closed_at":"2025-12-02T12:40:31.129514-05:00","dependencies":[{"issue_id":"sensei-ynz","depends_on_id":"sensei-mwh","type":"related","created_at":"2025-12-01T13:03:07.534536-05:00","created_by":"daemon"}]}
{"id":"sensei-yonb","title":"Add main() function to sensei/scout/server.py","description":"Move the main() entry point from __main__.py to server.py where the scout FastMCP server is defined. This follows the idiomatic pattern used by pytest/uvicorn.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-02T12:33:57.831588-05:00","updated_at":"2025-12-02T12:36:37.18054-05:00","closed_at":"2025-12-02T12:36:37.18054-05:00"}
{"id":"sensei-ytct","title":"Consider bundled JS instead of raw TS for distribution","description":"Current design downloads raw .ts files from unpkg that Bun runs directly.\n\nAlternative: Bundle/compile to .js before publishing to npm.\n\nTrade-offs to consider:\n- **Raw TS:** Simpler build, readable source, relies on Bun's TS support\n- **Bundled JS:** More portable (works if OpenCode changes runtime), smaller files, dependencies inlined\n\nQuestions:\n- Does OpenCode guarantee Bun runtime long-term?\n- Are there edge cases where raw TS fails?\n- Would bundling allow us to inline the @opencode-ai/plugin types?\n\nResearch OpenCode's expectations and decide on distribution format.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T18:05:59.19853-05:00","updated_at":"2025-12-17T18:05:59.19853-05:00","dependencies":[{"issue_id":"sensei-ytct","depends_on_id":"sensei-gy0d","type":"parent-child","created_at":"2025-12-17T18:06:03.780632-05:00","created_by":"daemon"}]}
{"id":"sensei-z1e","title":"Create tome MCP server exposing tools to agent","description":"Expose tome_get and tome_search as MCP tools for the Sensei agent.\n\n**New file:** `sensei/tome/server.py` (following pattern from kura/server.py, scout/server.py)\n\n**Tools to expose:**\n\n### `tome_get`\n```python\n@server.tool()\nasync def tome_get(domain: str, path: str) -\u003e str:\n    \"\"\"Get documentation from an ingested llms.txt domain.\n    \n    Args:\n        domain: The domain to fetch from (e.g., \"react.dev\")\n        path: Document path, or sentinel values:\n              - \"INDEX\" for /llms.txt (table of contents)\n              - \"FULL\" for /llms-full.txt (complete docs)\n              - Any path like \"/hooks/useState\" for specific doc\n    \n    Returns:\n        Document content as markdown, or error if not found.\n    \"\"\"\n```\n\n### `tome_search`\n```python\n@server.tool()\nasync def tome_search(domain: str, query: str, paths: list[str]) -\u003e str:\n    \"\"\"Search documentation within an ingested llms.txt domain.\n    \n    Args:\n        domain: The domain to search (e.g., \"react.dev\")\n        query: Search query (supports natural language)\n        paths: Path prefixes to search within (empty = all paths)\n               e.g., [\"/hooks\"] searches only /hooks/* docs\n    \n    Returns:\n        Matching snippets with source paths and relevance ranking.\n    \"\"\"\n```\n\n### `tome_ingest` (optional - for on-demand ingestion)\n```python\n@server.tool()\nasync def tome_ingest(domain: str) -\u003e str:\n    \"\"\"Ingest a domain's llms.txt documentation.\n    \n    Fetches /llms.txt, /llms-full.txt (if available), and all linked docs.\n    \"\"\"\n```\n\n**Integration:**\n- Add to agent.py toolsets list\n- Create `create_tome_server()` factory function\n- Follow existing patterns from kura/scout","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.50445-05:00","updated_at":"2025-12-01T08:54:40.448191-05:00","closed_at":"2025-12-01T08:54:40.448191-05:00","labels":["mcp","tome","tools"],"dependencies":[{"issue_id":"sensei-z1e","depends_on_id":"sensei-8qn","type":"blocks","created_at":"2025-12-01T08:38:58.836118-05:00","created_by":"daemon"},{"issue_id":"sensei-z1e","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.767202-05:00","created_by":"daemon"}]}
{"id":"sensei-z2ox","title":"Transform YAML datasets to pydantic-evals schema","description":"Migrate existing YAML files (general.yaml, fastapi.yaml, react.yaml) from current schema to native pydantic-evals format:\n- `id` → `name`\n- `query` → `inputs`\n- `category` + `library` → `metadata.tags` (array)\n- Add `name` at dataset level\n- Generate `_schema.json` for IDE autocomplete","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T09:42:22.953415-05:00","updated_at":"2025-12-12T11:41:51.020007-05:00","closed_at":"2025-12-12T11:41:51.020007-05:00","labels":["eval"],"dependencies":[{"issue_id":"sensei-z2ox","depends_on_id":"sensei-axt2","type":"parent-child","created_at":"2025-12-12T09:42:33.044913-05:00","created_by":"daemon"}]}
{"id":"sensei-z54j","title":"Make DeepEval judge model configuration deterministic and repo-native","description":"Judge/eval model selection currently depends on env vars and DeepEval dotenv autoload, which is brittle for a repeatable harness. It also conflicts with repo guidance to avoid env-based setup instructions.\n\nWork needed: move judge selection into explicit, deterministic configuration for eval runs (e.g., test parameters or config file), avoiding reliance on dotenv or ad-hoc env vars.","acceptance_criteria":"- Eval runs use explicit judge model selection that is stable across machines.\n- No dependency on DeepEval dotenv autoload for core harness behavior.\n- Switching judge models is supported via repo-native configuration/parameters.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T20:42:44.94927-05:00","updated_at":"2025-12-12T11:41:39.226118-05:00","closed_at":"2025-12-12T11:41:39.226118-05:00"}
{"id":"sensei-z563","title":"Improve prompt to get coding agent feedback on sensei responses","description":"Optimize the prompt/instructions that tell the coding agent (e.g., Claude Code) to provide feedback on how well sensei answered their query. Goal: increase feedback frequency so we can iterate on response quality.","design":"## Current State\n- `sensei_feedback` MCP tool exists in `sensei/server.py`\n- Agent prompt (`packages/sensei-claude-code/agents/sensei.md`) has NO feedback instructions\n- Agents only provide feedback if they happen to notice the tool\n\n## Research Findings (from sensei-ellw)\n\n### Claude Code Patterns\n1. **Stop hook** - Could trigger feedback prompt after agent finishes\n2. **Confidence scoring** - Agents output scores, findings below 80% filtered\n3. **Skills** - Methodology extracted into reusable modules that get loaded on demand\n\n### OpenCode Patterns\n1. **Permissions ARE feedback** - No explicit rating system; user approve/deny decisions implicitly train autonomy\n2. **No prompting for ratings** - Feedback is behavioral, not explicit\n3. **Event-driven architecture** - Every action trackable through event bus\n\n### Amp Patterns\n1. **Agent Skills** - Compatible with Claude Code skills format\n2. **Handoff vs compaction** - Different approach to context management\n3. Closed source - limited visibility\n\n## Design Options\n\n### Option A: Prompt-based (simplest)\nAdd a \"Providing Feedback\" section to sensei.md that explicitly asks the agent to call `sensei_feedback` after using results.\n\n**Pros**: Simple, immediate\n**Cons**: Agent may ignore, no enforcement\n\n### Option B: Stop hook\nCreate a hook that fires when sensei agent completes, prompting for feedback.\n\n**Pros**: Systematic, harder to skip\n**Cons**: More infrastructure, may feel intrusive\n\n### Option C: Implicit feedback (OpenCode style)\nTrack which results get used vs ignored. If agent re-queries same topic, previous result was insufficient.\n\n**Pros**: Zero friction, behavioral signal\n**Cons**: Requires tracking infrastructure, indirect signal\n\n### Option D: Confidence-gated feedback\nOnly ask for feedback when sensei's confidence was low OR when the answer came from cache.\n\n**Pros**: Targeted, less noise\n**Cons**: Requires confidence scoring in responses\n\n## Open Questions\n1. What feedback frequency is acceptable? (every query? only cache misses? only low-confidence?)\n2. Should feedback be mandatory or encouraged?\n3. How do we handle feedback fatigue?\n\n## Next Steps\n- [ ] Continue researching how other tools handle feedback\n- [ ] Decide on approach (A/B/C/D or hybrid)\n- [ ] Implement chosen approach","notes":"Updated with research from sensei-ellw. User wants to learn more before implementing.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-17T12:26:11.835004-05:00","updated_at":"2025-12-17T12:36:24.246229-05:00","dependencies":[{"issue_id":"sensei-z563","depends_on_id":"sensei-ellw","type":"related","created_at":"2025-12-17T12:34:40.949723-05:00","created_by":"daemon"}]}
{"id":"sensei-z8c0","title":"Fix version mismatch and automate version sync","description":"Currently versions are out of sync:\n- README.md: 1.0.1\n- pyproject.toml: 1.4.1\n- plugin.json: 1.4.1\n\nNeed to:\n1. Fix README.md to show correct version\n2. Create automation so `uv version` bumps also update:\n   - `packages/sensei-claude-code/.claude-plugin/plugin.json`\n   - README.md badge\n\nOptions:\n- Pre-commit hook that reads pyproject.toml version and updates others\n- Script in `scripts/` to sync versions\n- Single source of truth approach (read from pyproject.toml at runtime if possible)","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-17T12:19:05.747816-05:00","updated_at":"2025-12-17T12:34:02.510269-05:00","closed_at":"2025-12-17T12:34:02.510269-05:00"}
{"id":"sensei-z8x","title":"Setup Neon Postgres database","description":"Create Neon project in us-east-1, get connection string for DATABASE_URL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.848645-05:00","updated_at":"2025-11-27T10:20:12.28555-05:00","closed_at":"2025-11-27T10:20:12.28555-05:00","dependencies":[{"issue_id":"sensei-z8x","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.849367-05:00","created_by":"daemon"}]}
{"id":"sensei-zevl","title":"Add capture_exception() to tome/crawler.py failure handlers","description":"Add `sentry_sdk.capture_exception()` to crawler failure handlers in `sensei/tome/crawler.py`.\n\n## Locations to instrument\n\n1. **UnicodeDecodeError** (line 192) - unexpected encoding failure\n2. **Crawl failed** (line 291) - unexpected crawl error\n3. **Cleanup failed** (line 308) - unexpected cleanup error\n\n## Pattern\n\n```python\nexcept Exception as e:\n    sentry_sdk.capture_exception(e)  # ADD THIS\n    logger.error(f\"Crawl failed for {normalized_domain}: {e}\")\n    ...\n```\n\n## Do NOT instrument\n- ContentTypeWarning, NotFoundWarning - these are expected skips, not errors\n- They're recorded in result.warnings, not failures","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-18T10:16:54.899848-05:00","updated_at":"2025-12-18T10:21:02.6825-05:00","closed_at":"2025-12-18T10:21:02.6825-05:00","dependencies":[{"issue_id":"sensei-zevl","depends_on_id":"sensei-8mqm","type":"blocks","created_at":"2025-12-18T10:16:54.901326-05:00","created_by":"daemon"}]}
{"id":"sensei-zii","title":"Implement TomeCrawler with Crawlee HttpCrawler","description":"Create TomeCrawler in sensei/tome/crawler.py. Subclass or use Crawlee HttpCrawler. Handle: fetch llms.txt, parse links, enqueue same-domain markdown URLs, respect depth limit, store via storage layer.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:04.783373-05:00","updated_at":"2025-11-27T11:27:39.631156-05:00","closed_at":"2025-11-27T11:27:39.631156-05:00","dependencies":[{"issue_id":"sensei-zii","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:04.784816-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-au8","type":"blocks","created_at":"2025-11-27T11:16:18.364351-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-07q","type":"blocks","created_at":"2025-11-27T11:16:18.406751-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-qow","type":"blocks","created_at":"2025-11-27T11:16:18.449108-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-9v0","type":"blocks","created_at":"2025-11-27T11:16:18.491868-05:00","created_by":"daemon"}]}
{"id":"sensei-zujl","title":"Gut database/local.py - remove PostgreSQL lifecycle","description":"Remove all PostgreSQL lifecycle management from database/local.py.\n\n**File:** `sensei/database/local.py`\n\n**Remove:**\n- `_db_ready` module-level flag\n- `check_postgres_installed()`\n- `is_initialized()`\n- `is_running()`\n- `init_db()`\n- `start()`\n- `stop()`\n- `ensure_db_ready()` - the main entry point\n\n**Keep:**\n- `ensure_migrated()` - useful as reference for running migrations manually\n\n**Remove imports:**\n- `shutil` (only used for which())\n- `subprocess` (only used for pg_ctl)\n- Paths imports (get_pg_log, get_pgdata, get_sensei_home)\n- `BrokenInvariant` (only used for PostgreSQL errors)","design":"The file should be reduced to just:\n```python\n\"\"\"Database migration utilities.\n\nMigrations are run explicitly by the user via `alembic upgrade head`.\nThis module provides a helper for programmatic migration if needed.\n\"\"\"\n\nimport asyncio\nimport logging\nfrom functools import partial\n\nfrom alembic import command\nfrom alembic.config import Config\n\nfrom sensei.settings import sensei_settings\n\nlogger = logging.getLogger(__name__)\n\n\nasync def ensure_migrated() -\u003e None:\n    \"\"\"Run alembic migrations.\n    \n    Typically you run migrations via CLI: `alembic upgrade head`\n    This function is for programmatic use if needed.\n    \"\"\"\n    config = Config()\n    config.set_main_option(\"script_location\", \"sensei:migrations\")\n    config.set_main_option(\"sqlalchemy.url\", sensei_settings.database_url)\n\n    logger.info(\"Running database migrations...\")\n    loop = asyncio.get_event_loop()\n    await loop.run_in_executor(None, partial(command.upgrade, config, \"head\"))\n    logger.info(\"Database migrations complete\")\n```","acceptance_criteria":"- All PostgreSQL lifecycle functions removed\n- Only ensure_migrated() remains\n- File is ~30 lines instead of ~170\n- No subprocess, shutil imports","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T09:45:39.512932-05:00","updated_at":"2025-12-21T09:57:16.55317-05:00","closed_at":"2025-12-21T09:57:16.55317-05:00","dependencies":[{"issue_id":"sensei-zujl","depends_on_id":"sensei-8t6h","type":"parent-child","created_at":"2025-12-21T09:45:39.514927-05:00","created_by":"daemon"},{"issue_id":"sensei-zujl","depends_on_id":"sensei-b61g","type":"blocks","created_at":"2025-12-21T09:46:23.010097-05:00","created_by":"daemon"}]}
