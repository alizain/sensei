{"id":"sensei-07q","title":"Implement llms.txt parser","description":"Create parse_llms_txt() function in sensei/tome/parser.py. Extract URLs from llms.txt markdown format (links in H2 sections). Return list of URLs.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:03.962606-05:00","updated_at":"2025-11-27T11:26:20.975411-05:00","closed_at":"2025-11-27T11:26:20.975411-05:00","dependencies":[{"issue_id":"sensei-07q","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:03.964273-05:00","created_by":"daemon"},{"issue_id":"sensei-07q","depends_on_id":"sensei-au8","type":"blocks","created_at":"2025-11-27T11:16:18.327806-05:00","created_by":"daemon"}]}
{"id":"sensei-08s","title":"Tome: llms.txt-focused documentation retrieval for Sensei","description":"Add llms.txt-focused documentation retrieval to Sensei, complementing Context7 for general library lookups.\n\n## Overview\n\nTome is Sensei's llms.txt specialist. While Context7 handles general documentation from any source with its complex pipeline (parse→enrich→vectorize→rerank→cache), Tome focuses exclusively on llms.txt/llms-full.txt sources with a simpler, fresher approach.\n\n## Value Proposition\n\n- **Real-time**: No 10-15 day sync lag like Context7\n- **Authoritative**: llms.txt files are curated by library authors\n- **Simple**: No vector DB, just PostgreSQL FTS\n- **llms.txt-first**: Respects the standard's structure (INDEX/FULL/linked docs)\n\n## API Design\n\n### `tome_get(domain: str, path: str) -\u003e str`\nGet document content with sentinel values:\n- `\"INDEX\"` → `/llms.txt` (table of contents)\n- `\"FULL\"` → `/llms-full.txt` (complete docs)\n- Any path → specific document\n\n### `tome_search(domain: str, query: str, paths: list[str]) -\u003e list[SearchResult]`\nFull-text search within a domain, optionally scoped to path prefixes.\n\n## Implementation Components\n\n1. **Database**: Add PostgreSQL FTS (tsvector + GIN index)\n2. **Crawler**: Also fetch llms-full.txt\n3. **Storage**: Add search_documents_fts()\n4. **Service**: tome_get(), tome_search()\n5. **MCP Server**: Expose tools to agent\n6. **Integration**: Wire into Sensei agent\n\n## When to Use\n\n- **Tome**: Domains you've ingested, want fresh/authoritative llms.txt content\n- **Context7**: General library lookups, unknown domains, semantic search","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-01T08:39:31.779528-05:00","updated_at":"2025-12-01T09:03:15.06944-05:00","closed_at":"2025-12-01T09:03:15.06944-05:00","labels":["epic","tome"]}
{"id":"sensei-0ih","title":"Tome: Create SaveResult enum for storage return type","description":"Replace bool return from save_tome_document() with SaveResult enum (INSERTED, UPDATED, SKIPPED). This upstream fix enables proper tracking of documents_updated in IngestResult.","design":"```python\nclass SaveResult(Enum):\n    INSERTED = \"inserted\"\n    UPDATED = \"updated\"\n    SKIPPED = \"skipped\"  # content unchanged\n```\n\nCrawler can then:\n```python\nmatch await save_tome_document(...):\n    case SaveResult.INSERTED: result.documents_added += 1\n    case SaveResult.UPDATED: result.documents_updated += 1\n    case SaveResult.SKIPPED: result.documents_skipped += 1\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T13:28:47.061712-05:00","updated_at":"2025-11-27T13:42:03.542461-05:00","closed_at":"2025-11-27T13:42:03.542461-05:00","labels":["architecture","important","tome"]}
{"id":"sensei-0nt","title":"Make database init conditional (SQLite FTS5 vs Postgres ILIKE)","description":"Detect database type from URL. Skip FTS5 virtual tables on Postgres, use ILIKE for search instead.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.676313-05:00","updated_at":"2025-11-27T09:16:22.149689-05:00","closed_at":"2025-11-27T09:16:22.149689-05:00","dependencies":[{"issue_id":"sensei-0nt","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.6772-05:00","created_by":"daemon"}]}
{"id":"sensei-0uo","title":"Add kura script entry point to pyproject.toml","description":"Add kura = \"sensei.kura:main\" to [project.scripts] so users can run `uvx --from sensei kura` for stdio MCP.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.142133-05:00","updated_at":"2025-11-27T08:58:06.351994-05:00","closed_at":"2025-11-27T08:58:06.351994-05:00","dependencies":[{"issue_id":"sensei-0uo","depends_on_id":"sensei-fnh","type":"blocks","created_at":"2025-11-27T08:55:36.527213-05:00","created_by":"daemon"}]}
{"id":"sensei-0y5","title":"Create fly.toml configuration","description":"Create fly.toml with: app name, region (iad for US East near Neon), internal port 8080, volume mount at /data, health check endpoint, memory/CPU settings.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.791005-05:00","updated_at":"2025-11-30T05:57:26.643762-05:00","closed_at":"2025-11-30T05:57:26.643762-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-0y5","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.929291-05:00","created_by":"daemon"}]}
{"id":"sensei-10i","title":"Move flatten/save_tree from storage.py to crawler.py","description":"**Current state (storage.py:264-319):**\n`flatten` and `save_tree` are nested functions inside `save_sections` that do tree traversal:\n- Convert SectionData tree → flat Section models\n- Maintain parent_section_id relationships\n- Track position counters\n\n**Problem:** Tree traversal is business logic, not storage logic. Storage should receive flat data and store it.\n\n**Additional issue:** `flatten` function (lines 266-290) is defined but **never called** - only `save_tree` is used. This is dead code.\n\n**Fix:**\n1. Delete the dead `flatten` function\n2. Move `save_tree` logic to crawler.py as:\n```python\ndef flatten_section_tree(\n    root: SectionData, \n    document_id: UUID\n) -\u003e list[Section]:\n    \"\"\"Convert SectionData tree to flat list of Section models with parent relationships.\"\"\"\n    sections = []\n    position = [0]\n    \n    def walk(node: SectionData, parent_id: UUID | None):\n        if node.content or node.children:\n            section = Section(\n                document_id=document_id,\n                parent_section_id=parent_id,\n                heading=node.heading,\n                level=node.level,\n                content=node.content,\n                position=position[0],\n            )\n            position[0] += 1\n            sections.append(section)\n            for child in node.children:\n                walk(child, section.id)\n    \n    walk(root, None)\n    return sections\n```\n3. Storage just does bulk insert of the flat list","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.268775-05:00","updated_at":"2025-12-01T15:17:32.450562-05:00","closed_at":"2025-12-01T15:17:32.450562-05:00","dependencies":[{"issue_id":"sensei-10i","depends_on_id":"sensei-7af","type":"related","created_at":"2025-12-01T13:01:31.231896-05:00","created_by":"daemon"}]}
{"id":"sensei-11w","title":"Remove depth column from Document model","description":"The `depth` column on Document tracks crawl depth from llms.txt (0 = llms.txt itself, 1 = directly linked, etc.). \n\nThis is crawl metadata that doesn't belong on the Document model - it's an artifact of how we discovered the document, not a property of the document itself.\n\n**Changes:**\n- Remove `depth` column from Document model in models.py\n- Remove `depth` from DocumentContent type in types.py  \n- Update migration (sensei-gwq) to not include depth\n- Update crawler to not track/save depth\n\n**Note:** If we need crawl metadata in the future, it belongs in a separate crawl_history or document_source table, not on the document itself.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T10:01:02.296358-05:00","updated_at":"2025-12-01T13:01:45.601173-05:00","closed_at":"2025-12-01T13:01:45.601173-05:00","labels":["cleanup","database","tome"],"dependencies":[{"issue_id":"sensei-11w","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T10:01:02.298897-05:00","created_by":"daemon"}]}
{"id":"sensei-198","title":"Add asyncpg dependency to pyproject.toml","description":"Add asyncpg for async PostgreSQL driver. Also add alembic as dependency.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.48923-05:00","updated_at":"2025-11-27T16:01:50.231857-05:00","closed_at":"2025-11-27T16:01:50.231857-05:00","labels":["database","dependencies"],"dependencies":[{"issue_id":"sensei-198","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:08.065958-05:00","created_by":"daemon"}]}
{"id":"sensei-1c5","title":"Remove init_db() and SQLite-specific code from storage.py","description":"Remove: init_db(), is_postgres(), FTS5 virtual table, SQLite triggers, SQLite search logic in search_queries(). Keep engine/session factory and all query functions.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.42918-05:00","updated_at":"2025-11-27T16:01:50.456783-05:00","closed_at":"2025-11-27T16:01:50.456783-05:00","labels":["cleanup","database"],"dependencies":[{"issue_id":"sensei-1c5","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.982994-05:00","created_by":"daemon"}]}
{"id":"sensei-1ha","title":"Create alembic.ini and alembic/env.py with async support","description":"Initialize Alembic with async engine config. Read DATABASE_URL from env. Import Base.metadata from sensei.database.models. Enable compare_type and compare_server_default.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.259893-05:00","updated_at":"2025-11-27T16:01:50.34949-05:00","closed_at":"2025-11-27T16:01:50.34949-05:00","labels":["database","migrations"],"dependencies":[{"issue_id":"sensei-1ha","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.851521-05:00","created_by":"daemon"}]}
{"id":"sensei-1j9","title":"Create docker-compose.yml for local PostgreSQL","description":"PostgreSQL 16 container with volume persistence. Port 5432, credentials sensei/sensei/sensei.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.375073-05:00","updated_at":"2025-11-27T16:01:50.298951-05:00","closed_at":"2025-11-27T16:01:50.298951-05:00","labels":["database","docker"],"dependencies":[{"issue_id":"sensei-1j9","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.937163-05:00","created_by":"daemon"}]}
{"id":"sensei-1qn","title":"Tome: Add rate limiting to crawler","description":"HttpCrawler configured with max_requests but no rate limiting. Could overwhelm target servers. Add max_requests_per_minute or min_request_delay.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-11-27T11:44:48.829289-05:00","updated_at":"2025-11-27T16:08:51.073521-05:00","closed_at":"2025-11-27T16:08:51.073521-05:00","labels":["minor","tome"]}
{"id":"sensei-225","title":"Tome: Validate domain parameter in ingest_domain()","description":"crawler.py:49 has no validation that domain parameter is actually a domain (not a full URL). User might pass https://react.dev instead of react.dev.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, validation happens automatically when constructing Domain(user_input).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:32.747283-05:00","updated_at":"2025-11-27T13:42:03.743659-05:00","closed_at":"2025-11-27T13:42:03.743659-05:00","labels":["important","tome"],"dependencies":[{"issue_id":"sensei-225","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.941895-05:00","created_by":"daemon"}]}
{"id":"sensei-271","title":"Review and clean up chunker.py code quality issues","description":"**Issues found in chunker.py:**\n\n1. **_start_line hack (line 262):**\n   ```python\n   section._start_line = start_line  # type: ignore[attr-defined]\n   ```\n   Setting an attribute on a dataclass that doesn't have it. This is a code smell - the type ignore is a red flag.\n\n2. **Confusing function relationships:**\n   - `_split_by_top_level_headings` calls `_split_by_headings_at_level(lines, min_level=None)`\n   - `_split_by_subheadings` calls `_split_by_headings_at_level(lines, min_level=parent_level + 1)`\n   - These wrapper functions add indirection without much clarity\n\n3. **Token counting is rough (line 52-54):**\n   ```python\n   return len(content.split())  # Just counts words\n   ```\n   Comment says \"roughly 1.3 tokens per word on average but we use 1:1 for simplicity\". This could under-estimate by 30%.\n\n4. **Algorithm complexity:**\n   The interplay between `chunk_markdown`, `_chunk_section`, `_split_by_top_level_headings`, `_split_by_subheadings`, and `_extract_intro` is hard to follow. Consider simplifying.\n\n5. **Potential issue with intro extraction:**\n   `_extract_intro` relies on `_start_line` attribute which is set via the hack in point 1.\n\n**Recommendations:**\n- Add `start_line: int | None = None` to SectionData dataclass properly\n- Consider flattening the function hierarchy\n- Document the algorithm more clearly\n- Consider if token counting accuracy matters for the use case","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T13:01:21.687116-05:00","updated_at":"2025-12-01T14:45:23.231728-05:00","closed_at":"2025-12-01T14:45:23.231728-05:00","dependencies":[{"issue_id":"sensei-271","depends_on_id":"sensei-7af","type":"related","created_at":"2025-12-01T13:01:31.273264-05:00","created_by":"daemon"}]}
{"id":"sensei-28m","title":"Deps.http_client typed as Any instead of httpx.AsyncClient","description":"deps.py:16 types http_client as Optional[Any] instead of proper type.\n\nProblems:\n1. Loses type safety\n2. tools/common.py:44-49 get_client() uses getattr patterns to work around it\n3. Caller must track whether client was created to close it\n\nFix:\n1. Type properly: `http_client: httpx.AsyncClient | None = None`\n2. Consider using httpx context manager pattern\n3. Or inject client at request start in middleware","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.599112-05:00","updated_at":"2025-11-30T09:22:39.625143-05:00","closed_at":"2025-11-30T09:22:39.625143-05:00","labels":["cleanup","types"]}
{"id":"sensei-28w","title":"Remove depth column from Document model - web is a graph, not a tree","description":"**Current state:**\n- `Document` model has `depth = Column(Integer, nullable=False, server_default=\"0\")` (models.py:101)\n- `save_document_metadata` accepts and saves `depth` (storage.py:191, 208, 223, 234)\n- Crawler passes depth (crawler.py:120)\n\n**Problem:** The web is a graph, not a tree. The same document could be reached at different depths depending on the crawl path. Depth is only relevant for controlling crawl behavior (how many links to follow), not for storage.\n\n**Fix:**\n1. Remove `depth` column from `Document` model (models.py:101)\n2. Remove `depth` parameter from `save_document_metadata` (storage.py)\n3. Create migration to drop the column\n4. Keep depth only in crawler.py for crawl control (already does this correctly)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:20.826228-05:00","updated_at":"2025-12-01T13:34:48.08205-05:00","closed_at":"2025-12-01T13:34:48.08205-05:00"}
{"id":"sensei-2fb","title":"Create sensei/paths.py - centralized path management","description":"Create a new module for centralized path detection and management.\n\n**File:** `sensei/paths.py`\n\n**Design decision:** Use `SENSEI_HOME` environment variable with `~/.sensei` default. No fragile cwd detection.\n\n**Functions to implement:**\n```python\nimport os\nfrom pathlib import Path\n\n\ndef get_sensei_home() -\u003e Path:\n    \"\"\"Get the sensei home directory.\n    \n    Priority:\n    1. SENSEI_HOME env var (explicit override)\n    2. ~/.sensei (default)\n    \n    For development, set SENSEI_HOME=.sensei in .env\n    \"\"\"\n    if env_home := os.environ.get(\"SENSEI_HOME\"):\n        return Path(env_home)\n    return Path.home() / \".sensei\"\n\n\ndef get_pgdata() -\u003e Path:\n    \"\"\"Get PostgreSQL data directory path.\"\"\"\n    return get_sensei_home() / \"pgdata\"\n\n\ndef get_pg_log() -\u003e Path:\n    \"\"\"Get PostgreSQL log file path.\"\"\"\n    return get_sensei_home() / \"pg.log\"\n\n\ndef get_scout_repos() -\u003e Path:\n    \"\"\"Get scout repository cache directory.\"\"\"\n    return get_sensei_home() / \"scout\" / \"repos\"\n\n\ndef get_local_database_url() -\u003e str:\n    \"\"\"Get connection URL for local PostgreSQL using Unix socket.\n    \n    This is the default database URL when DATABASE_URL is not set.\n    Uses Unix socket to avoid port conflicts with system PostgreSQL.\n    \"\"\"\n    return f\"postgresql+asyncpg:///sensei?host={get_pgdata()}\"\n```\n\n**Why this approach:**\n- Simple and predictable\n- No fragile cwd detection\n- Devs set `SENSEI_HOME=.sensei` in `.env` for local data\n- Production uses `~/.sensei` by default\n- Follows \"explicit over implicit\" principle\n\n**Also update:**\n1. `sensei/config.py` - add `sensei_home` field (for documentation/visibility, won't be used by paths.py to avoid import cycle)\n2. `.env` - add `SENSEI_HOME=.sensei` for development\n\n**Data flow:**\n```\npaths.py (reads SENSEI_HOME directly from os.environ)\n    │\n    └──► config.py (has sensei_home field for visibility)\n             │\n             └──► storage.py (uses settings.database_url)\n```","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:05.63198-05:00","updated_at":"2025-12-02T06:36:35.601562-05:00","dependencies":[{"issue_id":"sensei-2fb","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:05.633609-05:00","created_by":"daemon"}]}
{"id":"sensei-2r4","title":"Fix module-level database initialization in storage.py","description":"storage.py:24-28 creates engine at module import time, causing test failures when DATABASE_URL is empty. Use lazy initialization or context-based engine creation.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:29.394024-05:00","updated_at":"2025-11-27T16:01:55.594075-05:00","closed_at":"2025-11-27T16:01:55.594075-05:00","labels":["database","important"]}
{"id":"sensei-2vw","title":"Remove tools/cache.py backwards-compat re-export module","description":"sensei/tools/cache.py is just a backwards-compatibility re-export from kura.tools:\n\n```python\nfrom sensei.kura.tools import _compute_age_days, get_cached_response, search_cache\n__all__ = [\"search_cache\", \"get_cached_response\", \"_compute_age_days\"]\n```\n\nOnly used by tests:\n- tests/test_cache_integration.py:11\n- tests/test_cache.py:66, 85, 109, 127\n\nChanges needed:\n1. Update test imports from `sensei.tools.cache` → `sensei.kura.tools`\n2. Delete sensei/tools/cache.py","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:16:36.907406-05:00","updated_at":"2025-11-30T09:17:09.110672-05:00","closed_at":"2025-11-30T09:17:09.110672-05:00","labels":["cleanup"]}
{"id":"sensei-2ym","title":"Timezone coercion logic duplicated across modules","description":"Multiple places handle timezone coercion differently:\n- kura/tools.py:19-30 (_compute_age_days)\n- storage.py:173-179 (search_queries)\n\nBoth do:\n```python\nif created_at.tzinfo is None:\n    created_at = created_at.replace(tzinfo=UTC)\n```\n\nFix options:\n1. Centralize in _compute_age_days() and use everywhere\n2. Ensure DB always returns timezone-aware datetimes (server_default with timezone)\n3. Add utility function in types.py for timezone normalization","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.5016-05:00","updated_at":"2025-11-30T09:19:19.825069-05:00","closed_at":"2025-11-30T09:19:19.825069-05:00","labels":["cleanup","database"]}
{"id":"sensei-35k","title":"Improve MCP tool descriptions in server files","description":"Improve the docstrings/descriptions for each MCP tool in their respective server files (scout/server.py, kura/server.py, tools/tavily.py, tools/context7.py). These descriptions are what Sensei sees via MCP, so they should be high-quality and guide proper usage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T22:15:09.963837-05:00","updated_at":"2025-12-01T07:09:47.855866-05:00","closed_at":"2025-12-01T07:09:47.855866-05:00"}
{"id":"sensei-37t","title":"Consolidate get_section_subtree_by_heading to single query with JOIN","description":"**Current state (storage.py:360-420):**\nTwo separate queries:\n```python\n# Query 1: Find active document\ndoc_result = await session.execute(\n    select(Document).where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n)\ndoc = doc_result.scalar_one_or_none()\nif not doc:\n    return []\n\n# Query 2: Recursive CTE with doc.id\nsql = \"\"\"\n    WITH RECURSIVE subtree AS (\n        SELECT * FROM sections\n        WHERE document_id = :doc_id AND heading = :heading\n        ...\n    )\n\"\"\"\n```\n\n**Problem:** Two round-trips to database when one suffices.\n\n**Fix:** Inline document lookup into the CTE's base case:\n```sql\nWITH RECURSIVE subtree AS (\n    -- Base case: find section by heading in active document\n    SELECT s.* FROM sections s\n    JOIN documents d ON s.document_id = d.id\n    WHERE d.domain = :domain\n      AND d.path = :path\n      AND d.generation_active = true\n      AND s.heading = :heading\n    UNION ALL\n    -- Recursive case: get children\n    SELECT s.* FROM sections s\n    JOIN subtree t ON s.parent_section_id = t.id\n)\nSELECT * FROM subtree ORDER BY position\n```\n\n**Benefits:**\n- Single query instead of two\n- Reduces latency\n- Same pattern as sensei-sp6\n\n**Related:** sensei-sp6 (same pattern for get_sections_by_document)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:23:59.684392-05:00","updated_at":"2025-12-01T15:35:56.996079-05:00","closed_at":"2025-12-01T15:35:56.996079-05:00","labels":["performance","query-optimization","storage"],"dependencies":[{"issue_id":"sensei-37t","depends_on_id":"sensei-sp6","type":"related","created_at":"2025-12-01T15:24:11.729151-05:00","created_by":"daemon"}]}
{"id":"sensei-3ao","title":"Expose tome module via MCP tool","description":"The tome module (ingest_domain) exists but nothing exposes it. No MCP tool, no API endpoint, no CLI command. It's dead code from a system perspective.\n\nCreate `sensei/tools/tome.py` that wraps `ingest_domain` as a tool callable via MCP.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-01T07:30:35.674535-05:00","updated_at":"2025-12-01T08:43:58.31512-05:00","closed_at":"2025-12-01T08:43:58.31512-05:00","labels":["mcp","p0","tome"]}
{"id":"sensei-3c9","title":"Add TomeDocument model to database","description":"Add SQLAlchemy model for TomeDocument with fields: id, domain, url, path, content, content_hash, depth, fetched_at. Add to sensei/database/models.py","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:02.272288-05:00","updated_at":"2025-11-27T11:26:10.678093-05:00","closed_at":"2025-11-27T11:26:10.678093-05:00","dependencies":[{"issue_id":"sensei-3c9","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:02.275146-05:00","created_by":"daemon"}]}
{"id":"sensei-3f9","title":"Add stateless_http=True to MCP server mounts","description":"Enable stateless mode for all three MCP servers (mcp, scout, kura) in __main__.py for horizontal scaling.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.731053-05:00","updated_at":"2025-11-27T09:16:32.361168-05:00","closed_at":"2025-11-27T09:16:32.361168-05:00","dependencies":[{"issue_id":"sensei-3f9","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.731773-05:00","created_by":"daemon"}]}
{"id":"sensei-3na","title":"Add scout script entry point to pyproject.toml","description":"Add [project.scripts] with scout = \"sensei.scout:main\" and create main() function","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.291703-05:00","updated_at":"2025-11-27T06:18:27.225929-05:00","closed_at":"2025-11-27T06:18:27.225929-05:00","dependencies":[{"issue_id":"sensei-3na","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.292305-05:00","created_by":"daemon"}]}
{"id":"sensei-3oe","title":"Fix test fixture asyncio.run() conflict with Alembic migrations","description":"The test_db fixture called run_migrations() which used command.upgrade() synchronously. Since the fixture is async and pytest-asyncio runs it in an event loop, calling asyncio.run() inside Alembic failed with \"cannot be called from a running event loop\". Fixed by running migrations in a ThreadPoolExecutor via run_in_executor().","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-27T16:12:07.24009-05:00","updated_at":"2025-11-27T16:12:11.275195-05:00","closed_at":"2025-11-27T16:12:11.275195-05:00","labels":["async","testing"]}
{"id":"sensei-3zm","title":"Verify dev server runs and builds successfully","description":"Test that npm run dev and npm run build work in the www package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.339278-05:00","updated_at":"2025-11-27T16:14:31.592256-05:00","closed_at":"2025-11-27T16:14:31.592256-05:00","dependencies":[{"issue_id":"sensei-3zm","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.340618-05:00","created_by":"daemon"},{"issue_id":"sensei-3zm","depends_on_id":"sensei-dbo","type":"blocks","created_at":"2025-11-27T13:22:35.607426-05:00","created_by":"daemon"}]}
{"id":"sensei-4mz","title":"Consolidate get_sections_for_toc to single query with JOIN","description":"**Current state (storage.py:575-615):**\nTwo separate queries:\n```python\n# Query 1: Find active document\ndoc_result = await session.execute(\n    select(Document).where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n)\ndoc = doc_result.scalar_one_or_none()\nif not doc:\n    return []\n\n# Query 2: Get section hierarchy data\nresult = await session.execute(\n    select(Section.id, Section.parent_section_id, Section.heading, Section.level)\n    .where(Section.document_id == doc.id)\n    .order_by(Section.position)\n)\n```\n\n**Problem:** Two round-trips to database when one suffices.\n\n**Fix:** Single query with JOIN:\n```python\nresult = await session.execute(\n    select(Section.id, Section.parent_section_id, Section.heading, Section.level)\n    .join(Document, Document.id == Section.document_id)\n    .where(\n        Document.domain == domain,\n        Document.path == path,\n        Document.generation_active == True,\n    )\n    .order_by(Section.position)\n)\nreturn [(row.id, row.parent_section_id, row.heading, row.level) for row in result]\n```\n\n**Benefits:**\n- Single query instead of two\n- Reduces latency\n- Cleaner code\n\n**Related:** sensei-sp6 (same pattern for get_sections_by_document)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:24:00.032606-05:00","updated_at":"2025-12-01T15:35:56.932049-05:00","closed_at":"2025-12-01T15:35:56.932049-05:00","labels":["performance","query-optimization","storage"],"dependencies":[{"issue_id":"sensei-4mz","depends_on_id":"sensei-sp6","type":"related","created_at":"2025-12-01T15:24:11.768197-05:00","created_by":"daemon"}]}
{"id":"sensei-4ok","title":"Deploy Sensei to Cloud Run with Neon Postgres","description":"Deploy full Sensei HTTP server to Google Cloud Run with Neon Postgres database. Stateless MCP for horizontal scaling.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T09:00:25.904011-05:00","updated_at":"2025-11-27T15:59:45.972035-05:00","closed_at":"2025-11-27T15:59:45.972035-05:00"}
{"id":"sensei-4pt","title":"Create sensei/database/local.py - PostgreSQL lifecycle","description":"Create the PostgreSQL lifecycle management module.\n\n**File:** `sensei/database/local.py`\n\n**This module handles:**\n- PostgreSQL lifecycle (init/start/stop) for sensei-managed mode\n- Migration running for local mode only (thin wrapper on alembic)\n\n**Functions to implement:**\n```python\nimport logging\nimport shutil\nimport subprocess\n\nfrom sensei.paths import get_pgdata, get_pg_log, get_sensei_home\nfrom sensei.types import BrokenInvariant\n\nlogger = logging.getLogger(__name__)\n\n# Idempotency flag - ensure_db_ready() may be called multiple times\n# (e.g., from combined_lifespan AND individual service lifespans)\n_db_ready = False\n\n# ─────────────────────────────────────────────────────────────────────────────\n# PostgreSQL Lifecycle (sensei-managed mode only)\n# ─────────────────────────────────────────────────────────────────────────────\n\ndef check_postgres_installed() -\u003e bool:\n    \"\"\"Check if PostgreSQL binaries are available on PATH.\"\"\"\n    return shutil.which(\"pg_ctl\") is not None\n\n\ndef is_initialized() -\u003e bool:\n    \"\"\"Check if data directory exists and is initialized.\"\"\"\n    return (get_pgdata() / \"PG_VERSION\").exists()\n\n\ndef is_running() -\u003e bool:\n    \"\"\"Check if PostgreSQL is running.\"\"\"\n    result = subprocess.run(\n        [\"pg_ctl\", \"-D\", str(get_pgdata()), \"status\"],\n        capture_output=True,\n    )\n    return result.returncode == 0\n\n\ndef init_db() -\u003e None:\n    \"\"\"Initialize the data directory + create sensei database.\"\"\"\n    logger.info(\"Initializing PostgreSQL data directory...\")\n    get_sensei_home().mkdir(parents=True, exist_ok=True)\n    \n    subprocess.run(\n        [\"initdb\", \"-D\", str(get_pgdata()), \"--auth=trust\", \"--no-instructions\"],\n        check=True,\n    )\n    \n    start()\n    \n    # Create database (uses Unix socket)\n    subprocess.run(\n        [\"createdb\", \"-h\", str(get_pgdata()), \"sensei\"],\n        check=True,\n    )\n    logger.info(\"PostgreSQL initialized and sensei database created\")\n\n\ndef start() -\u003e None:\n    \"\"\"Start PostgreSQL (blocking until ready).\"\"\"\n    logger.info(\"Starting PostgreSQL...\")\n    subprocess.run(\n        [\"pg_ctl\", \"-D\", str(get_pgdata()), \n         \"-l\", str(get_pg_log()), \n         \"-o\", f\"-k {get_pgdata()}\",  # Unix socket in pgdata\n         \"start\", \"-w\"],\n        check=True,\n    )\n    logger.info(\"PostgreSQL started\")\n\n\ndef stop() -\u003e None:\n    \"\"\"Stop PostgreSQL.\"\"\"\n    if is_running():\n        logger.info(\"Stopping PostgreSQL...\")\n        subprocess.run(\n            [\"pg_ctl\", \"-D\", str(get_pgdata()), \"stop\", \"-m\", \"fast\"],\n            check=True,\n        )\n        logger.info(\"PostgreSQL stopped\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# Migrations (local mode only) - thin wrapper on alembic\n# ─────────────────────────────────────────────────────────────────────────────\n\nasync def ensure_migrated() -\u003e None:\n    \"\"\"Run alembic migrations.\"\"\"\n    import asyncio\n    from functools import partial\n    from alembic import command\n    from alembic.config import Config\n    from sensei.config import settings\n    \n    # Build alembic config programmatically (works when installed as package)\n    config = Config()\n    config.set_main_option(\"script_location\", \"sensei:migrations\")\n    # Alembic needs sync URL (no +asyncpg)\n    sync_url = settings.database_url.replace(\"+asyncpg\", \"\")\n    config.set_main_option(\"sqlalchemy.url\", sync_url)\n    \n    # Run migrations (sync operation, run in thread pool)\n    logger.info(\"Running database migrations...\")\n    loop = asyncio.get_event_loop()\n    await loop.run_in_executor(None, partial(command.upgrade, config, \"head\"))\n    logger.info(\"Database migrations complete\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# Main Entry Point\n# ─────────────────────────────────────────────────────────────────────────────\n\nasync def ensure_db_ready() -\u003e None:\n    \"\"\"Ensure database is ready for use.\n    \n    **Idempotent** - safe to call multiple times. Uses module-level flag\n    to skip work on subsequent calls (e.g., when called from both\n    combined_lifespan and individual service lifespans).\n    \n    Behavior:\n    - External DB (DATABASE_URL set): Do nothing (user's responsibility)\n    - Local DB (DATABASE_URL not set): Start PG if needed, run migrations\n    \"\"\"\n    global _db_ready\n    \n    if _db_ready:\n        logger.debug(\"ensure_db_ready() already completed, skipping\")\n        return\n    \n    from sensei.config import settings\n    \n    if settings.is_external_database:\n        # External DB - user is responsible for setup and migrations\n        _db_ready = True\n        return\n    \n    # Sensei-managed mode: ensure PostgreSQL is running\n    if not check_postgres_installed():\n        raise BrokenInvariant(\n            \"PostgreSQL not found. Please install PostgreSQL 17+:\\n\"\n            \"  macOS:   brew install postgresql@17\\n\"\n            \"  Ubuntu:  sudo apt install postgresql-17\\n\"\n            \"  Windows: https://www.postgresql.org/download/windows/\"\n        )\n    \n    if not is_initialized():\n        try:\n            init_db()\n        except subprocess.CalledProcessError:\n            # Another process may have initialized - check again\n            if not is_initialized():\n                raise  # Real failure\n    elif not is_running():\n        start()  # pg_ctl start is safe to call concurrently\n    \n    # Run migrations for local DB\n    await ensure_migrated()\n    \n    _db_ready = True\n    logger.info(\"Database ready\")\n```\n\n**Idempotency:**\n- Module-level `_db_ready` flag ensures work is done only once per process\n- Safe to call from multiple lifespans (combined + individual services)\n- Logs skip on subsequent calls for visibility\n\n**Simplified logic:**\n- External DB → do nothing (user handles everything)\n- Local DB → start PG + migrate (always)\n\n**Mode matrix:**\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No |","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:05.722136-05:00","updated_at":"2025-12-02T06:57:36.191759-05:00","dependencies":[{"issue_id":"sensei-4pt","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:05.722999-05:00","created_by":"daemon"},{"issue_id":"sensei-4pt","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:10:20.55718-05:00","created_by":"daemon"}]}
{"id":"sensei-4wa","title":"Local PostgreSQL lifecycle management (epic)","description":"Enable sensei to manage a local PostgreSQL instance automatically, eliminating the need for users to manually set up and run PostgreSQL.\n\n## User-facing requirements\n\n1. User installs PostgreSQL 17+ (via Homebrew, apt, etc.) - we document how\n2. Sensei automatically initializes, starts, and migrates the database\n3. Works identically for:\n   - Claude Code plugins: `uvx --from sensei-ai scout` (stdio MCP)\n   - Full server: `uvicorn sensei:app` (HTTP server)\n\n## Key design decisions\n\n- **SENSEI_HOME env var** - defaults to `~/.sensei`, set `SENSEI_HOME=.sensei` in `.env` for development\n- **Unix socket connection** - no port conflicts with system PostgreSQL\n- **`trust` auth** for local socket (it's a local dev DB)\n- **Auto-start on first DB access** (lazy, via `ensure_db_ready()`)\n- **No filelock** - race conditions handled gracefully (initdb failure → retry, pg_ctl is idempotent)\n- **No CLI** - fully automatic, no manual `sensei db *` commands\n- **Idempotent `ensure_db_ready()`** - safe to call multiple times (module-level flag)\n\n## Folder structure\n\n```\n~/.sensei/                      # or SENSEI_HOME if set\n├── pgdata/                     # PostgreSQL data directory\n├── pg.log                      # PostgreSQL log\n└── scout/\n    └── repos/                  # Cloned GitHub repos\n```\n\n## Mode matrix\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No (user's responsibility) |\n\n## Subtasks\n\n1. `sensei-2fb` - Create sensei/paths.py (SENSEI_HOME, get_pgdata, etc.)\n2. `sensei-4pt` - Create sensei/database/local.py (lifecycle + migrations)\n3. `sensei-gvh` - Update sensei/config.py (dynamic database_url)\n4. `sensei-6ul` - Update pyproject.toml (add tome entry point)\n5. `sensei-7t4` - Restructure alembic for package distribution\n6. `sensei-agi` - Add ensure_db_ready() to MCP server lifespans\n7. `sensei-6iu` - Update scout/manager.py to use centralized paths\n8. `sensei-dnj` - Verify storage.py uses settings.database_url\n9. `sensei-asv` - Add unit tests (no integration tests)\n10. `sensei-sop` - Update README with PostgreSQL instructions\n\n## Closed subtasks\n\n- `sensei-czf` - CLI not needed (auto-managed)","status":"open","priority":0,"issue_type":"epic","created_at":"2025-12-02T06:09:40.48943-05:00","updated_at":"2025-12-02T07:06:17.295245-05:00","labels":["database","dx","infrastructure"]}
{"id":"sensei-4wt","title":"Tome: Fix misleading link count logging","description":"crawler.py:90 logs \"Found {len(links)} links\" but links is already filtered at that point. Should extract total before filtering for accurate logging.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:44.144582-05:00","updated_at":"2025-11-27T16:08:50.891579-05:00","closed_at":"2025-11-27T16:08:50.891579-05:00","labels":["minor","tome"]}
{"id":"sensei-505","title":"Refactor import cycles and lazy imports in sensei package","description":"Multiple lazy imports inside functions (e.g., crawler.py importing save_tome_document inside ingest_domain) suggest poor layering. Cycles are a code smell indicating unclear boundaries. Need to analyze import graph, identify cycles, and restructure modules for clean dependency flow.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:44:08.3768-05:00","updated_at":"2025-11-27T13:52:52.722924-05:00","closed_at":"2025-11-27T13:52:52.722924-05:00","labels":["architecture","important","tech-debt"]}
{"id":"sensei-51u","title":"Create Fly volume and set secrets","description":"Run fly commands to: 1) fly launch (or fly apps create), 2) fly volumes create sensei_data --size 10, 3) fly secrets set for DATABASE_URL, ANTHROPIC_API_KEY, etc.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.914964-05:00","updated_at":"2025-11-30T05:57:26.737998-05:00","closed_at":"2025-11-30T05:57:26.737998-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-51u","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:18.011101-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-0y5","type":"blocks","created_at":"2025-11-27T16:00:24.494254-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-iid","type":"blocks","created_at":"2025-11-27T16:00:24.53964-05:00","created_by":"daemon"},{"issue_id":"sensei-51u","depends_on_id":"sensei-n82","type":"blocks","created_at":"2025-11-27T16:00:24.57999-05:00","created_by":"daemon"}]}
{"id":"sensei-543","title":"Tome: Normalize subdomains in is_same_domain()","description":"parser.py:38-50 treats www.example.com and example.com as different domains. Should strip www. prefix for comparison to avoid filtering legitimate docs.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, this becomes trivial - just use Domain() for comparison.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:44:01.949301-05:00","updated_at":"2025-11-27T13:42:03.611821-05:00","closed_at":"2025-11-27T13:42:03.611821-05:00","labels":["critical","tome"],"dependencies":[{"issue_id":"sensei-543","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.845224-05:00","created_by":"daemon"}]}
{"id":"sensei-56t","title":"Decide on UUID generation strategy for primary keys","description":"TomeDocument uses String primary key with UUID generated in Python (storage.py). Need to decide: (1) Generate UUID in Python at storage layer (current), (2) Use PostgreSQL gen_random_uuid() as server_default, (3) Use SQLAlchemy's uuid type with default. Also affects: should queries table use UUID instead of caller-provided query_id?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T16:12:15.997504-05:00","updated_at":"2025-11-30T08:36:02.969798-05:00","closed_at":"2025-11-30T08:36:02.969798-05:00","labels":["database","migrations"]}
{"id":"sensei-5a1","title":"SQL injection vulnerability in document search","description":"storage.py:268 uses direct string interpolation in ILIKE query:\n```python\nstmt = stmt.where(Document.content.ilike(f\"%{query}%\"))\n```\nUse SQLAlchemy's parameterized binding instead.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-30T10:05:21.45224-05:00","updated_at":"2025-11-30T10:12:40.232206-05:00","closed_at":"2025-11-30T10:12:40.232206-05:00"}
{"id":"sensei-5i5","title":"Separate warnings from failures in IngestResult","description":"Currently all issues go into a single `errors` list. There's no distinction between:\n- Content we deliberately skipped (wrong content-type) - expected behavior\n- Actual failures (decode error, network error) - unexpected behavior\n\nFix: Split into `warnings: list[str]` (expected skips) and `failures: list[str]` (unexpected errors) for better observability.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-01T07:30:35.95418-05:00","updated_at":"2025-12-01T07:30:35.95418-05:00","labels":["observability","p2","tome"],"dependencies":[{"issue_id":"sensei-5i5","depends_on_id":"sensei-qm8","type":"blocks","created_at":"2025-12-01T07:30:44.021966-05:00","created_by":"daemon"}]}
{"id":"sensei-5wd","title":"Add asyncpg dependency to pyproject.toml","description":"Add asyncpg for Postgres async support. SQLAlchemy auto-selects driver based on DATABASE_URL prefix.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.619244-05:00","updated_at":"2025-11-27T09:10:01.260401-05:00","closed_at":"2025-11-27T09:10:01.260401-05:00","dependencies":[{"issue_id":"sensei-5wd","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.620306-05:00","created_by":"daemon"}]}
{"id":"sensei-656","title":"Re-evaluate str vs UUID type for query_id across layers","description":"Currently query_id uses different types at different layers:\n\n**Database layer (UUID):**\n- `storage.save_query()` returns `UUID`\n- `storage.get_query()` takes `UUID`\n- Models use `UUID(as_uuid=True)`\n\n**Application layer (str):**\n- `Deps.query_id: Optional[str]`\n- `Deps.parent_id: Optional[str]`\n- `types.QueryResult.query_id: str`\n- `types.Rating.query_id: str`\n- `types.CacheHit.query_id: str`\n- `types.SubSenseiResult.query_id: str`\n\n**Conversion points:**\n- `str(query_id)` - storage.py:167, core.py:119, 175, 178, sub_agent.py:86\n- `UUID(query_id)` - kura/tools.py:103, sub_agent.py:81, 158\n\n**Options to evaluate:**\n1. Keep current (str at edges, UUID in DB) - simpler JSON serialization\n2. Use UUID throughout - better type safety, Pydantic handles serialization\n3. Create a QueryId newtype that wraps UUID with str serialization\n\nDiscovered from sensei-56t review.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T08:37:42.887713-05:00","updated_at":"2025-11-30T09:31:38.454461-05:00","closed_at":"2025-11-30T09:31:38.454461-05:00","labels":["architecture","types"]}
{"id":"sensei-67v","title":"Implement generation-based crawls with atomic swap","description":"**Problem:**\nDocuments removed from source are never detected/deleted. Queries may see partial state during crawl.\n\n**Solution:** Generation-based crawls with atomic visibility swap.\n\n**Schema changes:**\n```sql\nALTER TABLE documents ADD COLUMN generation_id UUID NOT NULL;\nALTER TABLE documents ADD COLUMN generation_active BOOLEAN NOT NULL DEFAULT false;\nCREATE INDEX idx_documents_domain_active ON documents (domain) WHERE generation_active = true;\nCREATE VIEW documents_active AS SELECT * FROM documents WHERE generation_active = true;\n```\n\n**Crawl flow:**\n1. `gen_id = uuid4()` at crawl start\n2. Insert all docs with `generation_id=gen_id, generation_active=false`\n3. Insert all sections (FK to docs)\n4. Atomic swap: `UPDATE documents SET generation_active = (generation_id = $gen_id) WHERE domain = $domain`\n5. Cleanup: `DELETE FROM documents WHERE domain = $domain AND NOT generation_active` (cascades to sections)\n\n**Query changes:**\n- Use `documents_active` view instead of `documents` table\n- Or add `WHERE generation_active = true` to queries\n\n**Benefits:**\n- Queries always see complete set (old OR new, never partial)\n- Failed crawl = orphan generation, no impact on queries\n- Swap is instant (single UPDATE)\n- Can rollback by flipping generation_active back\n- Cleanup is separate, non-blocking\n\n**Note:** This replaces the skip-if-unchanged logic. Every crawl is a full re-process.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-01T13:22:59.958866-05:00","updated_at":"2025-12-01T13:34:48.025928-05:00","closed_at":"2025-12-01T13:34:48.025928-05:00"}
{"id":"sensei-6br","title":"Configure Vite and React Router for Framework Mode","description":"Create vite.config.ts and react-router.config.ts for Framework Mode setup","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.092242-05:00","updated_at":"2025-11-27T13:27:48.421694-05:00","closed_at":"2025-11-27T13:27:48.421694-05:00","dependencies":[{"issue_id":"sensei-6br","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.094102-05:00","created_by":"daemon"},{"issue_id":"sensei-6br","depends_on_id":"sensei-apn","type":"blocks","created_at":"2025-11-27T13:22:35.39033-05:00","created_by":"daemon"}]}
{"id":"sensei-6iu","title":"Update sensei/scout/manager.py - use centralized paths","description":"Update the scout RepoManager to use centralized paths instead of hardcoded `.scout/repos`.\n\n**File:** `sensei/scout/manager.py`\n\n**Current (line 41):**\n```python\nself.cache_dir = cache_dir or Path(\".\") / \".scout\" / \"repos\"\n```\n\n**Change to:**\n```python\nfrom sensei.paths import get_scout_repos\n\nself.cache_dir = cache_dir or get_scout_repos()\n```\n\n**Why:**\n- Currently uses cwd, which is wrong for `uvx --from sensei-ai scout`\n- With centralized paths, repos cache goes to:\n  - Development: `./.sensei/scout/repos/`\n  - Production: `~/.sensei/scout/repos/`\n- Repos are shared across all invocations (not per-cwd)","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:32.667055-05:00","updated_at":"2025-12-02T06:10:32.667055-05:00","dependencies":[{"issue_id":"sensei-6iu","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:10:32.668714-05:00","created_by":"daemon"},{"issue_id":"sensei-6iu","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:32.669535-05:00","created_by":"daemon"}]}
{"id":"sensei-6sm","title":"Remove dead code from tools/common.py","description":"tools/common.py has unused functions that should be removed after exec_plan refactor:\n\nDead code:\n- format_entries (line 15) - never imported or used\n- get_client (line 44) - never imported or used  \n- DEFAULT_TIMEOUT (line 12) - only used by get_client\n\nKeep:\n- wrap_tool (line 52) - will be used after exec_plan refactor\n\nAfter exec_plan refactor is complete, audit again and remove any remaining dead code.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:14:46.657517-05:00","updated_at":"2025-11-30T06:17:58.148726-05:00","closed_at":"2025-11-30T06:17:58.148726-05:00","labels":["cleanup"]}
{"id":"sensei-6ul","title":"Update pyproject.toml - add tome entry point","description":"Update pyproject.toml with missing entry point.\n\n**File:** `pyproject.toml`\n\n**Update [project.scripts]:**\n```toml\n[project.scripts]\nscout = \"sensei.scout:scout.run\"        # existing\nkura = \"sensei.kura:kura.run\"           # existing  \ntome = \"sensei.tome.server:tome.run\"    # NEW: tome was missing!\n```\n\n**Why:**\n- `tome` entry point was missing (only scout and kura existed)\n- Enables `uvx --from sensei-ai tome` to work\n\n**No CLI needed** - sensei-czf was closed. HTTP server can be started via uvicorn directly if needed.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:55.930428-05:00","updated_at":"2025-12-02T07:02:53.708832-05:00","dependencies":[{"issue_id":"sensei-6ul","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:55.931283-05:00","created_by":"daemon"},{"issue_id":"sensei-6ul","depends_on_id":"sensei-czf","type":"blocks","created_at":"2025-12-02T06:11:08.411426-05:00","created_by":"daemon"}]}
{"id":"sensei-6wo","title":"Add new types for section-based storage","description":"Add new domain types to `sensei/types.py` for section-based storage.\n\n**New types:**\n\n```python\n@dataclass\nclass SectionData:\n    \"\"\"Intermediate type for chunking algorithm output.\"\"\"\n    heading: str | None\n    level: int\n    content: str\n    children: list[SectionData]\n\n@dataclass  \nclass TOCEntry:\n    \"\"\"Table of contents entry for tome_toc().\"\"\"\n    heading: str\n    level: int\n    children: list[TOCEntry]\n```\n\n**Update existing:**\n\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n    heading_path: str  # NEW: breadcrumb like \"API \u003e Hooks \u003e useState\"\n```\n\n**Remove:**\n- `DocumentContent.depth` field (per sensei-cij)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T10:00:36.794866-05:00","updated_at":"2025-12-01T10:10:33.327875-05:00","closed_at":"2025-12-01T10:10:33.327875-05:00","labels":["tome","types"],"dependencies":[{"issue_id":"sensei-6wo","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T10:00:36.797037-05:00","created_by":"daemon"}]}
{"id":"sensei-718","title":"Move search_queries transformation logic to service layer","description":"**Current state (storage.py:131-183):**\n`search_queries` builds `CacheHit` objects with calculated/derived fields:\n\n```python\nnow = datetime.now(UTC)\nhits = []\nfor row in rows:\n    query_id, query_text, lib, ver, inserted_at = row\n    age_days = (now - inserted_at).days if inserted_at else 0\n    hits.append(\n        CacheHit(\n            query_id=query_id,\n            query_truncated=query_text[:100] if query_text else \"\",\n            age_days=age_days,\n            library=lib,\n            version=ver,\n        )\n    )\nreturn hits\n```\n\n**Problems:**\n1. `query_truncated=query_text[:100]` - presentation logic (truncation)\n2. `age_days` calculation - derived field computation\n3. Building `CacheHit` domain objects - transformation logic\n\n**Storage should:** Return raw `Query` objects (or minimal row data)\n\n**Service layer should:** Handle truncation, age calculation, CacheHit construction\n\n**Fix:**\n1. Storage returns `list[Query]` from simple SELECT\n2. Create/use service function that transforms to `list[CacheHit]`\n\n**Note:** This function is used by cache tools. Check `sensei/tools/cache.py` for the caller.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:19:45.597114-05:00","updated_at":"2025-12-01T15:40:19.385005-05:00","closed_at":"2025-12-01T15:40:19.385005-05:00","labels":["refactoring","service-layer","storage"]}
{"id":"sensei-73d","title":"Add edge case tests for Domain value object","description":"The `Domain` value object in `sensei/types.py` handles normalization but edge cases aren't tested with tome.\n\n**Edge cases to test:**\n- Domain with trailing slash: `example.com/`\n- Domain with path: `example.com/docs` (should extract just domain)\n- Domain with port: `example.com:8080`\n- Domain with protocol: `https://example.com`\n- Domain with www: `www.example.com`\n- Mixed case: `Example.COM`\n\n**Files:**\n- `sensei/types.py` - Domain class\n- `tests/test_tome_parser.py` - could add tests here or new file","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-01T09:04:15.877467-05:00","updated_at":"2025-12-01T09:04:15.877467-05:00","labels":["testing","tome"]}
{"id":"sensei-775","title":"Precompute heading_path in sections table to eliminate recursive CTE in FTS","description":"**Current state (storage.py:488-558):**\n`search_sections_fts` uses a recursive CTE on every search to build heading breadcrumbs:\n```sql\nWITH RECURSIVE ancestors AS (\n    SELECT s.id, s.parent_section_id, s.heading, ...\n           ARRAY[s.heading] as path_array, 1 as depth\n    FROM sections s\n    JOIN documents d ON s.document_id = d.id\n    WHERE ... AND s.search_vector @@ websearch_to_tsquery(...)\n    UNION ALL\n    SELECT a.id, p.parent_section_id, p.heading, ...\n           p.heading || a.path_array, a.depth + 1\n    FROM ancestors a\n    JOIN sections p ON a.parent_section_id = p.id\n    WHERE a.depth \u003c 10\n),\nfull_paths AS (\n    SELECT DISTINCT ON (id) id, path_array, ...\n    FROM ancestors ORDER BY id, depth DESC\n)\nSELECT array_to_string(array_remove(fp.path_array, NULL), ' \u003e ') as heading_path\n...\n```\n\n**Problem:** Recursive CTE runs on every search query. For deep heading hierarchies, this is expensive. The heading path doesn't change after crawl.\n\n**Fix:** Precompute `heading_path` at crawl time:\n\n1. **Add column to Section model:**\n```python\n# sensei/database/models.py\nheading_path = Column(String, nullable=True)  # e.g., \"API \u003e Hooks \u003e useState\"\n```\n\n2. **Compute in flatten_section_tree() (crawler.py):**\n```python\ndef flatten_section_tree(root: SectionData, document_id: UUID) -\u003e list[Section]:\n    sections: list[Section] = []\n    position = [0]\n\n    def walk(node: SectionData, parent_id: UUID | None, path_parts: list[str]) -\u003e None:\n        if node.content or node.children:\n            # Build heading path from ancestors\n            current_path = path_parts + ([node.heading] if node.heading else [])\n            heading_path = \" \u003e \".join(p for p in current_path if p) or None\n            \n            section = Section(\n                document_id=document_id,\n                parent_section_id=parent_id,\n                heading=node.heading,\n                level=node.level,\n                content=node.content or \"\",\n                position=position[0],\n                heading_path=heading_path,  # NEW\n            )\n            position[0] += 1\n            sections.append(section)\n\n            for child in node.children:\n                walk(child, section.id, current_path)\n\n    walk(root, None, [])\n    return sections\n```\n\n3. **Simplify FTS query:**\n```sql\nSELECT d.url, d.path, \n       ts_headline(...) as snippet,\n       ts_rank(...) as rank,\n       s.heading_path  -- Direct column read, no CTE!\nFROM sections s\nJOIN documents d ON s.document_id = d.id\nWHERE d.domain = :domain AND d.generation_active = true\n  AND s.search_vector @@ websearch_to_tsquery('english', :query)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Migration:**\n```python\ndef upgrade():\n    op.add_column(\"sections\", sa.Column(\"heading_path\", sa.String(), nullable=True))\n\ndef downgrade():\n    op.drop_column(\"sections\", \"heading_path\")\n```\n\n**Benefits:**\n- Eliminates expensive recursive CTE on every search\n- O(1) lookup instead of O(depth) tree traversal\n- Search queries become much simpler and faster\n\n**Trade-off:** Small storage increase (~50 bytes per section avg). Worth it for search performance.\n\n**Note:** Existing sections will have NULL heading_path until re-crawled. FTS query should fall back to recursive CTE or return empty string for NULL.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T15:24:00.783924-05:00","updated_at":"2025-12-01T15:33:09.945079-05:00","closed_at":"2025-12-01T15:33:09.945079-05:00","labels":["database","denormalization","fts","performance"]}
{"id":"sensei-7af","title":"Chunker should always parse structure, not just when content is large","description":"**Current state (chunker.py:74-76):**\n```python\nif count_tokens(content) \u003c= max_tokens:\n    # Content fits - return as single section\n    return SectionData(heading=None, level=0, content=content, children=[])\n```\n\n**Problem:** If content is small, it returns a single flat SectionData with no structure. This loses the heading hierarchy even when it exists.\n\n**Current behavior:**\n- Small doc with h1, h2, h3 → single flat SectionData (structure lost)\n- Large doc → parsed into tree structure\n\n**Desired behavior:**\n- All docs → parsed into tree structure by headings\n- Token limit only affects whether we need to recursively split further\n\n**Fix:**\nRemove the early return. Always parse headings and build the structure. The max_tokens limit should only trigger additional splitting when a section is too large, not skip structure parsing entirely.\n\n```python\ndef chunk_markdown(content: str, max_tokens: int = DEFAULT_MAX_TOKENS) -\u003e SectionData:\n    lines = content.split(\"\\n\")\n    sections = _split_by_top_level_headings(lines)\n    \n    if not sections:\n        # No headings - return as single root section\n        return SectionData(heading=None, level=0, content=content, children=[])\n    \n    # Build tree from sections, recursively splitting if any are too large\n    # ...\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.550148-05:00","updated_at":"2025-12-01T14:40:07.466015-05:00","closed_at":"2025-12-01T14:40:07.466015-05:00"}
{"id":"sensei-7ao","title":"Update tests to run migrations instead of create_all","description":"Tests run alembic upgrade head against test database. No create_all() shortcut. Ensures migrations are always tested.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.547898-05:00","updated_at":"2025-11-27T16:01:50.511515-05:00","closed_at":"2025-11-27T16:01:50.511515-05:00","labels":["database","testing"],"dependencies":[{"issue_id":"sensei-7ao","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:08.142222-05:00","created_by":"daemon"},{"issue_id":"sensei-7ao","depends_on_id":"sensei-ke1","type":"blocks","created_at":"2025-11-27T13:39:08.322769-05:00","created_by":"daemon"}]}
{"id":"sensei-7d0","title":"Tome: Fix depth semantics documentation","description":"crawler.py:40,86 comment says \"0 = llms.txt only\" but code with max_depth=0 still crawls llms.txt. Documentation doesn't match behavior.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:36.082348-05:00","updated_at":"2025-11-27T16:08:50.831049-05:00","closed_at":"2025-11-27T16:08:50.831049-05:00","labels":["minor","tome"]}
{"id":"sensei-7f4","title":"Tome: Track documents_updated correctly in IngestResult","description":"crawler.py:71-83 only tracks documents_added and documents_skipped. documents_updated always stays at 0. Modify save_tome_document() to distinguish new/updated/skipped.","notes":"System thinking review: This is a symptom of bool return type from storage. Once sensei-0ih (SaveResult enum) is implemented, tracking becomes trivial with pattern matching.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:21.836762-05:00","updated_at":"2025-11-27T13:42:03.811644-05:00","closed_at":"2025-11-27T13:42:03.811644-05:00","labels":["important","tome"],"dependencies":[{"issue_id":"sensei-7f4","depends_on_id":"sensei-0ih","type":"blocks","created_at":"2025-11-27T13:28:54.991289-05:00","created_by":"daemon"}]}
{"id":"sensei-7go","title":"Add kura to plugin mcpServers config","description":"Update packages/marketplace/sensei/.claude-plugin/plugin.json to include kura MCP server alongside scout.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.252771-05:00","updated_at":"2025-11-27T08:58:06.459188-05:00","closed_at":"2025-11-27T08:58:06.459188-05:00","dependencies":[{"issue_id":"sensei-7go","depends_on_id":"sensei-k1i","type":"blocks","created_at":"2025-11-27T08:55:36.594533-05:00","created_by":"daemon"}]}
{"id":"sensei-7gz","title":"Implement adaptive recursive markdown chunker","description":"Create the chunking algorithm that splits markdown by headings based on token count.\n\n**Location:** `sensei/tome/chunker.py` (new file)\n\n**Algorithm:**\n```python\ndef chunk_markdown(content: str, max_tokens: int = 8000) -\u003e list[SectionData]:\n    if count_tokens(content) \u003c= max_tokens:\n        return [SectionData(content=content, level=0, heading=None)]\n    \n    children = split_by_top_level_headings(content)\n    if not children:\n        raise BrokenInvariant(\"Content exceeds limit with no heading boundaries\")\n    \n    results = []\n    for child in children:\n        if count_tokens(child.content) \u003c= max_tokens:\n            results.append(child)\n        else:\n            results.extend(chunk_markdown(child.content, max_tokens))\n    return results\n```\n\n**Functions needed:**\n- `count_tokens(content: str) -\u003e int` - use tiktoken or simple word count\n- `split_by_top_level_headings(content: str) -\u003e list[SectionData]` - parse markdown AST, extract sections\n- `chunk_markdown(content: str, max_tokens: int) -\u003e list[SectionData]` - main recursive function\n\n**SectionData type:**\n```python\n@dataclass\nclass SectionData:\n    heading: str | None\n    level: int\n    content: str\n    children: list[SectionData]  # For building parent relationships\n```\n\n**Uses existing:** marko parser from parser.py","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:37.763826-05:00","updated_at":"2025-12-01T10:10:33.275686-05:00","closed_at":"2025-12-01T10:10:33.275686-05:00","labels":["parser","tome"],"dependencies":[{"issue_id":"sensei-7gz","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T09:56:37.766473-05:00","created_by":"daemon"}]}
{"id":"sensei-7jw","title":"Add search_documents_fts() to storage layer","description":"Add PostgreSQL full-text search function to storage layer for tome_search.\n\n**Function signature:**\n```python\nasync def search_documents_fts(\n    domain: str,\n    query: str,\n    paths: list[str] | None = None,\n    limit: int = 10,\n) -\u003e list[SearchResult]\n```\n\n**Behavior:**\n- Use `websearch_to_tsquery()` for natural language query parsing\n- Filter by domain (required)\n- Optionally filter by path prefixes (e.g., paths=[\"/hooks\"] matches \"/hooks/useState\")\n- Return ranked results with:\n  - url, path\n  - snippet (ts_headline for context)\n  - relevance score\n\n**SQL approach:**\n```sql\nSELECT url, path, \n       ts_headline('english', content, query, 'MaxWords=50, MinWords=20') as snippet,\n       ts_rank(search_vector, query) as rank\nFROM documents\nWHERE domain = :domain\n  AND search_vector @@ websearch_to_tsquery('english', :query)\n  AND (path LIKE ANY(:path_patterns) OR :path_patterns IS NULL)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Domain model:**\nAdd `SearchResult` to `sensei/types.py`:\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n```\n\n**Depends on:** sensei-ip4 (FTS migration must run first)","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:50.4462-05:00","updated_at":"2025-12-01T08:50:29.453276-05:00","closed_at":"2025-12-01T08:50:29.453276-05:00","labels":["fts","storage","tome"],"dependencies":[{"issue_id":"sensei-7jw","depends_on_id":"sensei-ip4","type":"blocks","created_at":"2025-12-01T08:38:58.748207-05:00","created_by":"daemon"},{"issue_id":"sensei-7jw","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.678492-05:00","created_by":"daemon"}]}
{"id":"sensei-7t4","title":"Restructure alembic for package distribution","description":"Restructure alembic migrations to work when sensei is installed as a package.\n\n**Problem:**\nWhen running via `uvx --from sensei tome`, the cwd is NOT the sensei repo. The `alembic.ini` and `alembic/` directory won't be found.\n\n**Solution:** Move migrations into the package and update alembic.ini.\n\n**Steps:**\n\n### 1. Create sensei/migrations/ directory structure\n```\nsensei/\n├── migrations/\n│   ├── __init__.py\n│   ├── env.py\n│   ├── script.py.mako\n│   └── versions/\n│       ├── __init__.py\n│       └── 40de6c7e554f_initial_schema.py\n```\n\n### 2. Move files\n- `alembic/versions/40de6c7e554f_initial_schema.py` → `sensei/migrations/versions/`\n- `alembic/env.py` → `sensei/migrations/env.py` (adapt as needed)\n- `alembic/script.py.mako` → `sensei/migrations/script.py.mako`\n\n### 3. Update alembic.ini\n```ini\n[alembic]\nscript_location = sensei/migrations\n```\n\nThis keeps the CLI working for development (`alembic revision --autogenerate`).\n\n### 4. Update sensei/migrations/env.py\n```python\nfrom alembic import context\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import create_engine\n\nfrom sensei.database.models import Base\nfrom sensei.config import settings\n\ntarget_metadata = Base.metadata\n\ndef run_migrations_online():\n    # Use sync URL (no +asyncpg)\n    sync_url = settings.database_url.replace(\"+asyncpg\", \"\")\n    \n    connectable = create_engine(sync_url, poolclass=pool.NullPool)\n    \n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n        )\n        with context.begin_transaction():\n            context.run_migrations()\n\nrun_migrations_online()\n```\n\n### 5. Update pyproject.toml\n```toml\n[tool.setuptools.package-data]\nsensei = [\"migrations/*.py\", \"migrations/*.mako\", \"migrations/versions/*.py\"]\n```\n\n### 6. Delete old alembic/ directory\nAfter verifying everything works, remove the old `alembic/` directory (keep `alembic.ini` at root for dev CLI).\n\n**Why this works:**\n- `alembic.ini` at root → CLI works for development\n- Migrations in package → works when installed via `uvx`\n- Programmatic config in `local.py` uses `script_location = \"sensei:migrations\"` (colon notation for package)","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:24:35.441879-05:00","updated_at":"2025-12-02T06:53:38.791946-05:00","dependencies":[{"issue_id":"sensei-7t4","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:24:35.443717-05:00","created_by":"daemon"},{"issue_id":"sensei-7t4","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:24:35.444464-05:00","created_by":"daemon"}]}
{"id":"sensei-7uw","title":"Update sub_agent.py to use Kura via MCP toolset","description":"Add create_kura_server() to sub-agent toolsets for consistency with main agent. Sub-agents should have same tool access patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.732217-05:00","updated_at":"2025-11-27T09:06:08.523371-05:00","closed_at":"2025-11-27T09:06:08.523371-05:00","dependencies":[{"issue_id":"sensei-7uw","depends_on_id":"sensei-x4g","type":"blocks","created_at":"2025-11-27T09:03:51.34974-05:00","created_by":"daemon"}]}
{"id":"sensei-7za","title":"Add FTS index on queries table for cache search","description":"**Current state (storage.py:131-183):**\n`search_queries` uses ILIKE with wildcards:\n```python\nconditions = \" AND \".join([f\"query ILIKE :term{i}\" for i in range(len(terms))])\nparams = {f\"term{i}\": f\"%{term}%\" for i, term in enumerate(terms)}\n\nsql = f\"\"\"\n    SELECT id, query, library, version, inserted_at\n    FROM queries\n    WHERE {conditions}\n    LIMIT :limit\n\"\"\"\n```\n\n**Problem:** `ILIKE '%term%'` cannot use indexes - it's a full table scan. As the queries table grows, cache search gets slower.\n\n**Fix:** Add tsvector column with GIN index for proper FTS:\n\n1. **Add to Query model:**\n```python\n# sensei/database/models.py\nclass Query(TimestampMixin, Base):\n    ...\n    search_vector = Column(\n        TSVECTOR,\n        Computed(\"to_tsvector('english', query)\", persisted=True),\n        nullable=True,\n    )\n```\n\n2. **Migration:**\n```python\ndef upgrade():\n    op.execute(\"\"\"\n        ALTER TABLE queries\n        ADD COLUMN search_vector tsvector\n        GENERATED ALWAYS AS (to_tsvector('english', query)) STORED\n    \"\"\")\n    op.execute(\"\"\"\n        CREATE INDEX idx_queries_search_vector\n        ON queries USING GIN(search_vector)\n    \"\"\")\n\ndef downgrade():\n    op.execute(\"DROP INDEX IF EXISTS idx_queries_search_vector\")\n    op.drop_column(\"queries\", \"search_vector\")\n```\n\n3. **Update search_queries:**\n```python\nasync def search_queries(terms: list[str], limit: int = 10) -\u003e list[Query]:\n    if not terms:\n        return []\n    \n    # Convert terms to tsquery\n    query_str = \" \u0026 \".join(terms)  # AND semantics\n    \n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(Query)\n            .where(Query.search_vector.op(\"@@\")(func.plainto_tsquery(\"english\", query_str)))\n            .limit(limit)\n        )\n        return list(result.scalars().all())\n```\n\n**Benefits:**\n- Index-backed search instead of table scan\n- Scales with table size\n- Supports stemming, ranking, etc.\n\n**Trade-off:**\n- Small storage overhead for tsvector column\n- Slightly different search semantics (FTS vs substring match)\n\n**Priority:** P3 - Cache search is not the hot path. Only implement if cache usage grows significantly.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-01T15:24:01.136428-05:00","updated_at":"2025-12-01T15:42:17.345074-05:00","closed_at":"2025-12-01T15:42:17.345074-05:00","labels":["cache","database","fts","performance"]}
{"id":"sensei-8ec","title":"Tome: Document migration strategy for TomeDocument schema","description":"models.py:64-76 uses Base.metadata.create_all() which doesn't handle schema evolution. Document that schema changes require manual migration or Alembic setup.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T11:44:12.648564-05:00","updated_at":"2025-11-27T13:39:08.377067-05:00","closed_at":"2025-11-27T13:39:08.377067-05:00","labels":["important","tome"]}
{"id":"sensei-8mm","title":"Remove Cloud Run artifacts","description":"Remove .gcloudignore and Procfile since we're switching to Fly.io. Keep Dockerfile but update it for Fly.io.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.62305-05:00","updated_at":"2025-11-30T05:57:26.548242-05:00","closed_at":"2025-11-30T05:57:26.548242-05:00","labels":["cleanup","deployment"],"dependencies":[{"issue_id":"sensei-8mm","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.861005-05:00","created_by":"daemon"}]}
{"id":"sensei-8no","title":"Remove debug logging from crawler.py","description":"Debug logging was added for link analysis during a debugging session:\n\n```python\n# Debug logging for link analysis\nlogger.debug(f\"=== Link analysis for {url} ===\")\nlogger.debug(f\"Same-domain links ({len(same_domain_links)}):\")\nfor link in same_domain_links:\n    logger.debug(f\"  ✓ {link}\")\nlogger.debug(f\"Other-domain links ({len(other_domain_links)}):\")\nfor link in other_domain_links:\n    logger.debug(f\"  ✗ {link}\")\n```\n\nThis should be removed or converted to a proper verbose/trace level option if needed for future debugging.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-12-01T13:01:31.137042-05:00","updated_at":"2025-12-01T13:01:31.137042-05:00"}
{"id":"sensei-8qn","title":"Create tome service layer with tome_get and tome_search","description":"Create the middle layer that exposes tome functionality to tools.\n\n**New file:** `sensei/tome/service.py`\n\n**Functions:**\n\n### `tome_get(domain: str, path: str) -\u003e str | None`\nGet document content by domain and path.\n\nSentinel values:\n- `\"INDEX\"` → `/llms.txt`\n- `\"FULL\"` → `/llms-full.txt`\n- Any other path → literal path lookup\n\n```python\nasync def tome_get(domain: str, path: str) -\u003e Success[str] | NoResults:\n    actual_path = {\n        \"INDEX\": \"/llms.txt\",\n        \"FULL\": \"/llms-full.txt\",\n    }.get(path, path)\n    \n    url = f\"https://{domain}{actual_path}\"\n    doc = await storage.get_document_by_url(url)\n    if doc:\n        return Success(doc.content)\n    return NoResults()\n```\n\n### `tome_search(domain: str, query: str, paths: list[str]) -\u003e list[SearchResult]`\nFull-text search within a domain, optionally filtered by path prefixes.\n\n```python\nasync def tome_search(\n    domain: str, \n    query: str, \n    paths: list[str],\n) -\u003e Success[list[SearchResult]] | NoResults:\n    results = await storage.search_documents_fts(domain, query, paths)\n    if results:\n        return Success(results)\n    return NoResults()\n```\n\n**Error handling:**\n- Domain not ingested → return NoResults (agent can try Context7)\n- Empty query → raise ToolError","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.00595-05:00","updated_at":"2025-12-01T08:53:41.332262-05:00","closed_at":"2025-12-01T08:53:41.332262-05:00","labels":["service","tome"],"dependencies":[{"issue_id":"sensei-8qn","depends_on_id":"sensei-7jw","type":"blocks","created_at":"2025-12-01T08:38:58.794598-05:00","created_by":"daemon"},{"issue_id":"sensei-8qn","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.720479-05:00","created_by":"daemon"}]}
{"id":"sensei-9hb","title":"Fix search_cache_tool test mock - expects string but gets list","description":"**Test:** `test_search_cache_tool` in `tests/test_cache.py:78`\n\n**Issue:** Test mocks `search_queries` expecting a string argument:\n```python\nmock_search.assert_called_once_with(\"React hooks\", limit=5)\n```\n\nBut actual implementation passes a list:\n```python\nsearch_queries(['React', 'hooks'], limit=5)\n```\n\n**Fix:** Update the mock expectation to match the actual behavior.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-01T13:34:12.904674-05:00","updated_at":"2025-12-01T13:35:02.80021-05:00","closed_at":"2025-12-01T13:35:02.80021-05:00"}
{"id":"sensei-9lu","title":"Update tome MCP server with new tools","description":"Update server.py to expose new API and update existing tools.\n\n**Update existing:**\n- `tome_get` - add optional `heading` parameter\n- `tome_search` - return heading_path in results\n\n**Add new:**\n- `tome_toc` - expose table of contents\n\n**Tool definitions:**\n\n```python\n@server.tool()\nasync def tome_toc(domain: str, path: str) -\u003e str:\n    \\\"\\\"\\\"Get table of contents for a document.\n    \n    Returns the heading structure so you can navigate to specific sections.\n    Use with tome_get(domain, path, heading) to retrieve specific sections.\n    \\\"\\\"\\\"\n\n@server.tool()\nasync def tome_get(\n    domain: str, \n    path: str, \n    heading: str | None = None\n) -\u003e str:\n    \\\"\\\"\\\"Get documentation content.\n    \n    Args:\n        domain: The domain (e.g., \"react.dev\")\n        path: Document path, or \"INDEX\"/\"FULL\" sentinels\n        heading: Optional - get only this section and its children\n    \n    Returns:\n        Full document content, or specific section if heading provided.\n    \\\"\\\"\\\"\n```","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-01T09:57:15.005994-05:00","updated_at":"2025-12-01T09:57:15.005994-05:00","labels":["mcp","tome"],"dependencies":[{"issue_id":"sensei-9lu","depends_on_id":"sensei-m7g","type":"blocks","created_at":"2025-12-01T09:57:15.008075-05:00","created_by":"daemon"},{"issue_id":"sensei-9lu","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.88693-05:00","created_by":"daemon"}]}
{"id":"sensei-9pf","title":"Build Dojo: DSPy prompt optimization from feedback","description":"DSPy plugin that optimizes Sensei's prompts using collected user feedback ratings. Uses MIPROv2 optimizer with 4-5 star rated examples as training data.","design":"## Stack\n- DSPy framework\n- MIPROv2 optimizer (joint instruction + few-shot optimization)\n\n## Input\n- User feedback ratings from existing ratings table\n- Filter to 4-5 star examples\n- Need 50-100 diverse examples\n\n## Output\n- Optimized system prompts\n- Curated few-shot examples\n\n## Integration\n- Periodic retraining when new feedback accumulates\n- Save optimized prompts for production use\n\n## Status: Stub for now\nCreate folder with README describing objective. Build after Tome.","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-11-27T11:12:59.177268-05:00","updated_at":"2025-11-27T16:00:28.806615-05:00","closed_at":"2025-11-27T16:00:28.806615-05:00"}
{"id":"sensei-9v0","title":"Add crawlee dependency to pyproject.toml","description":"Add crawlee[httpx] to dependencies in pyproject.toml","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:06.395559-05:00","updated_at":"2025-11-27T11:26:10.763828-05:00","closed_at":"2025-11-27T11:26:10.763828-05:00","dependencies":[{"issue_id":"sensei-9v0","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:06.39726-05:00","created_by":"daemon"}]}
{"id":"sensei-9xe","title":"Tome: Add timeout configuration to HttpCrawler","description":"crawler.py:51-53 HttpCrawler created without timeout settings. Could hang indefinitely on slow/dead servers. Add request_handler_timeout_secs.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T11:44:26.124788-05:00","updated_at":"2025-11-27T13:42:05.769835-05:00","closed_at":"2025-11-27T13:42:05.769835-05:00","labels":["important","tome"]}
{"id":"sensei-afa","title":"Prepare pyproject.toml for PyPI publishing","description":"Add required metadata: authors, license, classifiers, urls, etc.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.330399-05:00","updated_at":"2025-11-27T06:20:07.826302-05:00","closed_at":"2025-11-27T06:20:07.826302-05:00","dependencies":[{"issue_id":"sensei-afa","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.331062-05:00","created_by":"daemon"}]}
{"id":"sensei-agi","title":"Add ensure_db_ready() to MCP server lifespans","description":"Add database initialization to MCP server startup paths.\n\n**Key insight:** Services that USE the database should call `ensure_db_ready()` in their lifespan. The function is idempotent, so multiple calls are safe.\n\n**Service matrix:**\n| Service | Needs DB? | Add lifespan? |\n|---------|-----------|---------------|\n| Scout | No | No |\n| Kura | Yes | Yes |\n| Tome | Yes | Yes |\n| Combined HTTP | Yes | Yes (calls before nesting) |\n\n**Files to update:**\n\n### 1. sensei/scout/server.py\n**NO CHANGES** - Scout doesn't use the database.\n\n### 2. sensei/kura/server.py\n```python\nfrom contextlib import asynccontextmanager\nfrom fastmcp import FastMCP\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\nkura = FastMCP(name=\"kura\", lifespan=lifespan)\n```\n\n### 3. sensei/tome/server.py\n```python\nfrom contextlib import asynccontextmanager\nfrom fastmcp import FastMCP\n\n@asynccontextmanager\nasync def lifespan(server):\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    yield\n\ntome = FastMCP(name=\"tome\", lifespan=lifespan)\n```\n\n### 4. sensei/__main__.py\n```python\n@asynccontextmanager\nasync def combined_lifespan(app: FastAPI):\n    # Ensure database is ready (idempotent - will be called again in kura/tome lifespans but will no-op)\n    from sensei.database.local import ensure_db_ready\n    await ensure_db_ready()\n    \n    async with mcp_app.lifespan(app):\n        async with scout_mcp_app.lifespan(app):\n            async with kura_mcp_app.lifespan(app):\n                async with tome_mcp_app.lifespan(app):\n                    yield\n```\n\n**Why call in both combined AND individual lifespans?**\n- Standalone stdio (`uvx --from sensei kura`): Only individual lifespan runs\n- Combined HTTP (`uvx sensei`): Combined lifespan runs first, individual lifespans no-op\n\n**Idempotency guarantee:** `ensure_db_ready()` uses a module-level flag to skip work on subsequent calls within the same process.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:11:08.460139-05:00","updated_at":"2025-12-02T06:57:36.299717-05:00","dependencies":[{"issue_id":"sensei-agi","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:08.461179-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:11:21.240209-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-gvh","type":"blocks","created_at":"2025-12-02T06:11:21.283815-05:00","created_by":"daemon"},{"issue_id":"sensei-agi","depends_on_id":"sensei-7t4","type":"blocks","created_at":"2025-12-02T06:24:41.868741-05:00","created_by":"daemon"}]}
{"id":"sensei-aky","title":"Refactor exec_plan tools to use wrap_tool and proper result types","description":"tools/exec_plan.py returns error strings directly instead of using the established pattern:\n- Core functions should raise ToolError for errors, return Success[str] for success\n- wrap_tool decorator converts rich types to strings at PydanticAI boundary\n\nCurrent (lines 83-113):\n```python\nasync def add_exec_plan(ctx: RunContext[Deps]) -\u003e str:\n    if not ctx.deps or not ctx.deps.query_id:\n        return \"Error: missing query_id; cannot create ExecPlan.\"  # BAD\n    ...\n    return \"ExecPlan template added...\"\n```\n\nShould be:\n```python\nasync def _add_exec_plan(ctx: RunContext[Deps]) -\u003e Success[str]:\n    if not ctx.deps or not ctx.deps.query_id:\n        raise ToolError(\"Missing query_id; cannot create ExecPlan.\")\n    ...\n    return Success(\"ExecPlan template added...\")\n\nadd_exec_plan = wrap_tool(_add_exec_plan)\n```\n\nFiles affected:\n- sensei/tools/exec_plan.py\n- sensei/tools/common.py (wrap_tool will be used)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:14:46.602674-05:00","updated_at":"2025-11-30T07:55:27.570545-05:00","closed_at":"2025-11-30T07:55:27.570545-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-anj","title":"Create Procfile for Cloud Run","description":"Add Procfile with: web: uvicorn sensei.__main__:app --host 0.0.0.0 --port $PORT","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.789268-05:00","updated_at":"2025-11-27T09:16:41.222625-05:00","closed_at":"2025-11-27T09:16:41.222625-05:00","dependencies":[{"issue_id":"sensei-anj","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.78984-05:00","created_by":"daemon"}]}
{"id":"sensei-apn","title":"Initialize packages/www directory and package.json","description":"Create the www package directory and configure package.json with React Router v7 Framework Mode dependencies","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.022327-05:00","updated_at":"2025-11-27T13:27:48.360399-05:00","closed_at":"2025-11-27T13:27:48.360399-05:00","dependencies":[{"issue_id":"sensei-apn","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.024215-05:00","created_by":"daemon"}]}
{"id":"sensei-asv","title":"Add tests for database lifecycle management","description":"Add unit tests for the new database lifecycle code. **No integration tests** - only unit tests with mocking.\n\n**Files to create:** `tests/test_paths.py`, `tests/test_database_local.py`\n\n### test_paths.py\n```python\nimport os\nfrom pathlib import Path\nfrom unittest.mock import patch\n\ndef test_get_sensei_home_default():\n    \"\"\"Returns ~/.sensei when SENSEI_HOME not set.\"\"\"\n    with patch.dict(os.environ, {}, clear=True):\n        # Need to reload module to pick up env change\n        from sensei import paths\n        import importlib\n        importlib.reload(paths)\n        assert paths.get_sensei_home() == Path.home() / \".sensei\"\n\ndef test_get_sensei_home_from_env(tmp_path):\n    \"\"\"Respects SENSEI_HOME env var.\"\"\"\n    with patch.dict(os.environ, {\"SENSEI_HOME\": str(tmp_path)}):\n        from sensei import paths\n        import importlib\n        importlib.reload(paths)\n        assert paths.get_sensei_home() == tmp_path\n\ndef test_get_pgdata():\n    \"\"\"Returns sensei_home/pgdata.\"\"\"\n    from sensei.paths import get_sensei_home, get_pgdata\n    assert get_pgdata() == get_sensei_home() / \"pgdata\"\n\ndef test_get_scout_repos():\n    \"\"\"Returns sensei_home/scout/repos.\"\"\"\n    from sensei.paths import get_sensei_home, get_scout_repos\n    assert get_scout_repos() == get_sensei_home() / \"scout\" / \"repos\"\n\ndef test_get_local_database_url():\n    \"\"\"Returns Unix socket URL pointing to pgdata.\"\"\"\n    from sensei.paths import get_pgdata, get_local_database_url\n    url = get_local_database_url()\n    assert url.startswith(\"postgresql+asyncpg:///sensei?host=\")\n    assert str(get_pgdata()) in url\n```\n\n### test_database_local.py\n```python\nimport shutil\nfrom unittest.mock import patch, MagicMock\n\ndef test_check_postgres_installed_true():\n    \"\"\"Returns True when pg_ctl found.\"\"\"\n    with patch.object(shutil, \"which\", return_value=\"/usr/bin/pg_ctl\"):\n        from sensei.database.local import check_postgres_installed\n        assert check_postgres_installed() is True\n\ndef test_check_postgres_not_installed():\n    \"\"\"Returns False when pg_ctl not found.\"\"\"\n    with patch.object(shutil, \"which\", return_value=None):\n        from sensei.database.local import check_postgres_installed\n        assert check_postgres_installed() is False\n\ndef test_is_initialized_false_when_no_pgdata(tmp_path):\n    \"\"\"Returns False when PG_VERSION doesn't exist.\"\"\"\n    with patch(\"sensei.database.local.get_pgdata\", return_value=tmp_path / \"pgdata\"):\n        from sensei.database.local import is_initialized\n        assert is_initialized() is False\n\ndef test_is_initialized_true_when_pg_version_exists(tmp_path):\n    \"\"\"Returns True when PG_VERSION exists.\"\"\"\n    pgdata = tmp_path / \"pgdata\"\n    pgdata.mkdir()\n    (pgdata / \"PG_VERSION\").write_text(\"17\")\n    \n    with patch(\"sensei.database.local.get_pgdata\", return_value=pgdata):\n        from sensei.database.local import is_initialized\n        assert is_initialized() is True\n\n@pytest.mark.asyncio\nasync def test_ensure_db_ready_idempotent():\n    \"\"\"Second call is a no-op due to _db_ready flag.\"\"\"\n    import sensei.database.local as local\n    \n    # Reset the flag\n    local._db_ready = False\n    \n    with patch.object(local, \"check_postgres_installed\", return_value=True), \\\n         patch.object(local, \"is_initialized\", return_value=True), \\\n         patch.object(local, \"is_running\", return_value=True), \\\n         patch.object(local, \"ensure_migrated\", new_callable=MagicMock) as mock_migrate, \\\n         patch(\"sensei.database.local.settings\") as mock_settings:\n        \n        mock_settings.is_external_database = False\n        mock_migrate.return_value = None  # async mock\n        \n        # First call does work\n        await local.ensure_db_ready()\n        assert local._db_ready is True\n        \n        # Second call skips\n        mock_migrate.reset_mock()\n        await local.ensure_db_ready()\n        mock_migrate.assert_not_called()\n```\n\n**No integration tests** - we don't test actual PostgreSQL startup/shutdown. Unit tests with mocking are sufficient.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-02T06:11:35.024518-05:00","updated_at":"2025-12-02T07:04:58.191133-05:00","dependencies":[{"issue_id":"sensei-asv","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:35.033361-05:00","created_by":"daemon"},{"issue_id":"sensei-asv","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:11:40.061786-05:00","created_by":"daemon"},{"issue_id":"sensei-asv","depends_on_id":"sensei-2fb","type":"blocks","created_at":"2025-12-02T06:11:40.093271-05:00","created_by":"daemon"}]}
{"id":"sensei-au8","title":"Create sensei/tome/ module structure","description":"Create sensei/tome/ with __init__.py, crawler.py, parser.py. Basic module structure for Tome.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:03.087901-05:00","updated_at":"2025-11-27T11:26:10.723597-05:00","closed_at":"2025-11-27T11:26:10.723597-05:00","dependencies":[{"issue_id":"sensei-au8","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:03.089268-05:00","created_by":"daemon"}]}
{"id":"sensei-azo","title":"Improve instructions for calling agent (Claude Code)","description":"Improve the instructions that tell Claude Code how to invoke Sensei. Currently in packages/marketplace/sensei/README.md. The calling agent needs to provide rich context (the problem/outcome, tech stack, constraints) so Sensei can do effective research. A weak question leads to weak research.","status":"open","priority":2,"issue_type":"task","created_at":"2025-11-30T22:15:17.863029-05:00","updated_at":"2025-11-30T22:15:17.863029-05:00"}
{"id":"sensei-b9k","title":"Fix test_cache_search_and_retrieve_flow integration test","description":"The test_cache_integration.py::test_cache_search_and_retrieve_flow test is failing because search_cache returns NoResults even though a query was just saved. This appears to be a pre-existing issue with FTS search not finding recently saved queries.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-30T09:49:27.618471-05:00","updated_at":"2025-12-01T07:30:18.433911-05:00","closed_at":"2025-12-01T07:30:18.433911-05:00","dependencies":[{"issue_id":"sensei-b9k","depends_on_id":"sensei-vrp","type":"discovered-from","created_at":"2025-11-30T09:49:27.62088-05:00","created_by":"daemon"}]}
{"id":"sensei-ben","title":"Add composite index on documents(domain, path) WHERE active","description":"**Current indexes on documents:**\n- `domain` - single column index\n- `url` - unique constraint (implicit index)\n- `idx_documents_domain_active` - partial index on (domain) WHERE generation_active=true\n\n**Problem:** Multiple functions filter by `(domain, path, generation_active=true)`:\n- `get_sections_by_document`\n- `get_section_subtree_by_heading`\n- `get_sections_for_toc`\n\nThe current partial index only covers `domain`. After filtering by domain, PostgreSQL still needs to scan for matching `path`.\n\n**Fix:** Create composite partial index:\n```sql\nCREATE INDEX idx_documents_domain_path_active\nON documents (domain, path)\nWHERE generation_active = true\n```\n\n**Migration:**\n```python\ndef upgrade():\n    op.execute(\"\"\"\n        CREATE INDEX idx_documents_domain_path_active\n        ON documents (domain, path)\n        WHERE generation_active = true\n    \"\"\")\n    # Optionally drop the old single-column partial index\n    # op.execute(\"DROP INDEX IF EXISTS idx_documents_domain_active\")\n\ndef downgrade():\n    op.execute(\"DROP INDEX IF EXISTS idx_documents_domain_path_active\")\n```\n\n**Benefits:**\n- Faster lookups for (domain, path) combination\n- Index-only scans possible for document existence checks\n- Partial index keeps it small (only active docs)\n\n**Note:** Evaluate whether to keep or drop `idx_documents_domain_active` - it may still be useful for domain-only queries like `search_sections_fts` and `cleanup_old_generations`.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:24:00.384681-05:00","updated_at":"2025-12-01T15:36:46.531948-05:00","closed_at":"2025-12-01T15:36:46.531948-05:00","labels":["database","index","performance"]}
{"id":"sensei-bff","title":"Fix is_markdown_content test - test expects None to return True but function returns False","description":"**Test:** `test_is_markdown_content_accepts_none` in `tests/test_tome_parser.py:213`\n\n**Issue:** Test asserts `is_markdown_content(None)` should return `True` with comment \"trust the URL\", but the function in `sensei/tome/crawler.py` returns `False` for `None`:\n\n```python\ndef is_markdown_content(content_type: str | None) -\u003e bool:\n    if not content_type:\n        return False  # \u003c-- returns False for None\n```\n\n**Decision needed:** Should we:\n1. Trust the URL when content-type is missing (change function to return True for None)\n2. Not trust, require explicit content-type (fix the test)\n\nCurrently the crawler skips documents with no content-type header, which may be overly restrictive.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-01T13:34:12.844313-05:00","updated_at":"2025-12-01T14:01:55.528723-05:00","closed_at":"2025-12-01T14:01:55.528723-05:00"}
{"id":"sensei-bgr","title":"Decide on created_at server_default strategy","description":"Migration has created_at columns without server_default. Need to decide: (1) Use server_default=sa.func.now() so DB handles timestamps even for raw SQL inserts, (2) Keep Python-only default and ensure all inserts go through ORM, (3) Use timezone-aware now() function. Also consider: should models.py Column definitions match migration server_defaults?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T16:10:31.669066-05:00","updated_at":"2025-11-30T08:39:17.995198-05:00","closed_at":"2025-11-30T08:39:17.995198-05:00","labels":["database","migrations"]}
{"id":"sensei-bti","title":"Extract cache into kura module","description":"Create sensei/kura/ module mirroring Scout's structure. Extract existing cache functionality from sensei/tools/cache.py. Connect to same SQLite database as main Sensei app.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T08:55:29.016538-05:00","updated_at":"2025-11-27T08:57:25.865367-05:00","closed_at":"2025-11-27T08:57:25.865367-05:00"}
{"id":"sensei-cfg","title":"Tome: Use proper public suffix resolution for domain comparison","description":"Current Domain value object only strips www. prefix, but subdomains like api.example.com, docs.example.com should resolve to the same root domain (example.com). Need proper public suffix handling (e.g., tldextract library) to correctly identify registrable domain. Example: foo.bar.co.uk should resolve to bar.co.uk, not co.uk.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-27T13:44:08.328063-05:00","updated_at":"2025-11-27T13:58:32.321651-05:00","closed_at":"2025-11-27T13:58:32.321651-05:00","labels":["architecture","important","tome"]}
{"id":"sensei-cij","title":"Remove depth column from Document model","description":"The `depth` column on Document tracks crawl depth from llms.txt (0 = llms.txt itself, 1 = directly linked, etc.). \n\nThis is crawl metadata that doesn't belong on the Document model - it's an artifact of how we discovered the document, not a property of the document itself.\n\n**Changes:**\n- Remove `depth` column from Document model\n- Remove `depth` from DocumentContent type\n- Update migration to not include depth\n- Update crawler to not track/save depth\n\n**Note:** If we need crawl metadata in the future, it belongs in a separate crawl_history or document_source table, not on the document itself.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T09:59:01.100177-05:00","updated_at":"2025-12-01T10:00:41.774917-05:00","closed_at":"2025-12-01T10:00:41.774917-05:00","labels":["cleanup","database","tome"],"dependencies":[{"issue_id":"sensei-cij","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T09:59:01.102535-05:00","created_by":"daemon"}]}
{"id":"sensei-cn3","title":"Deploy to Cloud Run","description":"Run: gcloud run deploy sensei --source . --region us-east1. Configure secrets for API keys and DATABASE_URL.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.914569-05:00","updated_at":"2025-11-27T15:59:46.012164-05:00","closed_at":"2025-11-27T15:59:46.012164-05:00","dependencies":[{"issue_id":"sensei-cn3","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.915243-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-5wd","type":"blocks","created_at":"2025-11-27T09:00:51.527228-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-0nt","type":"blocks","created_at":"2025-11-27T09:00:51.568133-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-3f9","type":"blocks","created_at":"2025-11-27T09:00:51.6073-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-anj","type":"blocks","created_at":"2025-11-27T09:00:51.645743-05:00","created_by":"daemon"},{"issue_id":"sensei-cn3","depends_on_id":"sensei-z8x","type":"blocks","created_at":"2025-11-27T09:00:51.68622-05:00","created_by":"daemon"}]}
{"id":"sensei-cox","title":"Set up TailwindCSS v4","description":"Configure TailwindCSS v4 with Vite plugin and base styles","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.152555-05:00","updated_at":"2025-11-27T13:27:48.472576-05:00","closed_at":"2025-11-27T13:27:48.472576-05:00","dependencies":[{"issue_id":"sensei-cox","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.153763-05:00","created_by":"daemon"},{"issue_id":"sensei-cox","depends_on_id":"sensei-6br","type":"blocks","created_at":"2025-11-27T13:22:35.425621-05:00","created_by":"daemon"}]}
{"id":"sensei-czf","title":"Create sensei/cli.py - CLI with subcommands","description":"Create a CLI module with subcommands for server and database management.\n\n**File:** `sensei/cli.py`\n\n**Commands:**\n```\nsensei                    # With no args: start HTTP server\nsensei serve              # Explicit: start HTTP server  \nsensei db init            # Initialize PostgreSQL data directory\nsensei db start           # Start PostgreSQL\nsensei db stop            # Stop PostgreSQL\nsensei db status          # Show PostgreSQL status\nsensei db migrate         # Run pending migrations\nsensei db destroy         # Remove data directory (with confirmation)\n```\n\n**Implementation using argparse (or click):**\n```python\nimport argparse\nimport asyncio\nimport sys\n\ndef main():\n    parser = argparse.ArgumentParser(prog=\"sensei\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n    \n    # sensei serve\n    subparsers.add_parser(\"serve\", help=\"Start HTTP server\")\n    \n    # sensei db \u003csubcommand\u003e\n    db_parser = subparsers.add_parser(\"db\", help=\"Database management\")\n    db_sub = db_parser.add_subparsers(dest=\"db_command\")\n    db_sub.add_parser(\"init\", help=\"Initialize database\")\n    db_sub.add_parser(\"start\", help=\"Start PostgreSQL\")\n    db_sub.add_parser(\"stop\", help=\"Stop PostgreSQL\")\n    db_sub.add_parser(\"status\", help=\"Show status\")\n    db_sub.add_parser(\"migrate\", help=\"Run migrations\")\n    db_sub.add_parser(\"destroy\", help=\"Remove data directory\")\n    \n    args = parser.parse_args()\n    \n    if args.command is None or args.command == \"serve\":\n        run_server()\n    elif args.command == \"db\":\n        handle_db_command(args.db_command)\n\ndef run_server():\n    import uvicorn\n    from sensei.__main__ import app\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\ndef handle_db_command(cmd: str):\n    from sensei.database import local\n    # ... dispatch to local.* functions\n```\n\n**Note:** Keep it simple with argparse. No need for click unless complexity grows.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:46.501223-05:00","updated_at":"2025-12-02T07:02:43.34575-05:00","closed_at":"2025-12-02T07:02:43.34575-05:00","dependencies":[{"issue_id":"sensei-czf","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:46.502747-05:00","created_by":"daemon"},{"issue_id":"sensei-czf","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:10:55.891069-05:00","created_by":"daemon"}]}
{"id":"sensei-dbo","title":"Create root.tsx entry point and routes","description":"Create src/root.tsx (HTML shell), src/routes.ts, and src/routes/home.tsx placeholder","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.277674-05:00","updated_at":"2025-11-27T13:27:48.582151-05:00","closed_at":"2025-11-27T13:27:48.582151-05:00","dependencies":[{"issue_id":"sensei-dbo","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.278804-05:00","created_by":"daemon"},{"issue_id":"sensei-dbo","depends_on_id":"sensei-6br","type":"blocks","created_at":"2025-11-27T13:22:35.508318-05:00","created_by":"daemon"},{"issue_id":"sensei-dbo","depends_on_id":"sensei-cox","type":"blocks","created_at":"2025-11-27T13:22:35.554226-05:00","created_by":"daemon"}]}
{"id":"sensei-ddo","title":"Tome: Handle port numbers in domain comparison","description":"parser.py:48-50 netloc comparison includes port numbers - example.com:443 != example.com. Should strip default ports (80/443) during comparison.","notes":"System thinking review: This is a symptom of missing Domain value object. Once sensei-kt2 is implemented, port stripping happens automatically in Domain._normalize().","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:44:07.128291-05:00","updated_at":"2025-11-27T13:42:03.676023-05:00","closed_at":"2025-11-27T13:42:03.676023-05:00","labels":["critical","tome"],"dependencies":[{"issue_id":"sensei-ddo","depends_on_id":"sensei-kt2","type":"blocks","created_at":"2025-11-27T13:28:54.887605-05:00","created_by":"daemon"}]}
{"id":"sensei-die","title":"Build and publish sensei to PyPI","description":"Use uv build and uv publish to publish package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:14:56.447842-05:00","updated_at":"2025-11-27T06:15:23.12644-05:00","closed_at":"2025-11-27T06:15:23.12644-05:00","dependencies":[{"issue_id":"sensei-die","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:14:56.448451-05:00","created_by":"daemon"}]}
{"id":"sensei-dnj","title":"Update sensei/database/storage.py - use effective_database_url","description":"Update storage.py to use `settings.database_url` (which now always has a value).\n\n**File:** `sensei/database/storage.py`\n\n**Current (line 27-31):**\n```python\ndef _get_engine():\n    global _engine\n    if _engine is None:\n        _engine = create_async_engine(\n            settings.database_url,\n            ...\n        )\n```\n\n**NO CHANGES NEEDED to the engine creation!**\n\nThe `settings.database_url` field now:\n- Uses `default_factory=get_local_database_url` so it's never None\n- Gets overridden by `DATABASE_URL` env var if set\n\n**What to verify:**\n- `settings.database_url` is used (not `settings.effective_database_url`)\n- No None checks needed (it's always a string)\n\n**Data flow:**\n```\nsettings.database_url (always a valid string)\n    │\n    ├── DATABASE_URL env var set? → uses that value\n    │\n    └── DATABASE_URL not set? → uses get_local_database_url()\n                                 → \"postgresql+asyncpg:///sensei?host=~/.sensei/pgdata\"\n```\n\n**Actual change needed:** Just verify the existing code is correct. If it already uses `settings.database_url`, no changes required.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:24:35.355757-05:00","updated_at":"2025-12-02T06:26:12.637722-05:00","dependencies":[{"issue_id":"sensei-dnj","depends_on_id":"sensei-gvh","type":"blocks","created_at":"2025-12-02T06:24:35.357211-05:00","created_by":"daemon"},{"issue_id":"sensei-dnj","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:24:35.358118-05:00","created_by":"daemon"}]}
{"id":"sensei-ec9","title":"Update crawler to use chunker and save sections","description":"Modify crawler.py to chunk documents and save sections instead of raw content.\n\n**Changes to ingest flow:**\n1. Fetch document content (unchanged)\n2. Compute content_hash (unchanged)\n3. Check if document exists and hash matches (unchanged)\n4. **NEW:** Chunk content using `chunk_markdown()`\n5. Save Document metadata (no content)\n6. Save Sections with parent relationships and positions\n\n**Key changes:**\n- Import chunker module\n- After fetching content, call `chunk_markdown(content)`\n- Build Section objects with parent_section_id tracking\n- Use position counter for global ordering\n- Call `save_sections()` instead of `save_document()`\n\n**Handle parent relationships:**\n- Track current parent as we recurse\n- Assign position incrementally in document order","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:55.507402-05:00","updated_at":"2025-12-01T10:16:27.986935-05:00","closed_at":"2025-12-01T10:16:27.986935-05:00","labels":["crawler","tome"],"dependencies":[{"issue_id":"sensei-ec9","depends_on_id":"sensei-7gz","type":"blocks","created_at":"2025-12-01T09:56:55.509738-05:00","created_by":"daemon"},{"issue_id":"sensei-ec9","depends_on_id":"sensei-v52","type":"blocks","created_at":"2025-12-01T09:56:55.510465-05:00","created_by":"daemon"},{"issue_id":"sensei-ec9","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.803889-05:00","created_by":"daemon"}]}
{"id":"sensei-edz","title":"Tome: Section-based storage for large documents","description":"Refactor tome storage model from monolithic documents to section-based storage.\n\n**Core insight:** Documents are containers, sections are content. Markdown headings provide natural boundaries for chunking.\n\n**Benefits:**\n- FTS always works (sections fit within tsvector limit)\n- Better search results (return relevant section, not 1MB blob)\n- Granular retrieval (full doc OR specific section subtree)\n- Natural TOC derived from section tree\n\n**Design doc:** docs/plans/2025-12-01-tome-section-based-storage.md","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-01T09:56:20.361081-05:00","updated_at":"2025-12-01T10:25:39.054492-05:00","closed_at":"2025-12-01T10:25:39.054492-05:00","labels":["architecture","epic","tome"]}
{"id":"sensei-eg1","title":"RatingRequest duplicates Rating model - violates single source of truth","description":"server/models.py duplicates domain models from types.py, violating single source of truth:\n\n**Duplications found:**\n1. `RatingRequest` duplicates `Rating` (7 fields)\n2. `QueryResponse` duplicates `QueryResult` (2 fields)\n\nThe ONLY difference is edge models add `json_schema_extra` for OpenAPI examples.\n\n**Fix: Use inheritance**\n- `RatingRequest(Rating)` - inherits fields, adds only `model_config` with examples\n- `QueryResponse(QueryResult)` - inherits fields, adds only `model_config` with examples\n\n**Files affected:**\n- sensei/server/models.py\n- sensei/types.py (may need Field descriptions moved here)\n\n**Conversion points to verify:**\n- api.py:180 - `Rating(**request.model_dump())` → can use `request` directly if RatingRequest inherits Rating\n- api.py:53 - `QueryResponse(query_id=..., markdown=...)` → can return result directly","design":"Use Pydantic inheritance: edge models inherit from domain models, adding only json_schema_extra for OpenAPI examples. This keeps field definitions in types.py (single source of truth) while allowing API-specific presentation at the edge.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:03:17.207517-05:00","updated_at":"2025-11-30T08:57:47.739968-05:00","closed_at":"2025-11-30T08:57:47.739968-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-fdc","title":"Update tome crawler to fetch llms-full.txt","description":"Currently the crawler only fetches `/llms.txt` and follows its links. We need to also fetch `/llms-full.txt` when available.\n\n**Current behavior:**\n1. GET /llms.txt (depth=0)\n2. Parse links, crawl linked docs (depth=1+)\n\n**New behavior:**\n1. GET /llms.txt (depth=0) - the INDEX\n2. GET /llms-full.txt (depth=0) - the FULL content (404 is fine, not all sites have it)\n3. Parse links from llms.txt\n4. Crawl linked docs (depth=1+)\n\n**Implementation:**\n- Modify `ingest_domain()` in `sensei/tome/crawler.py`\n- Start crawl with TWO initial URLs instead of one\n- Handle 404 gracefully for llms-full.txt (many sites won't have it)\n\n**Testing:**\n- Test domain with both files\n- Test domain with only llms.txt (llms-full.txt 404)\n- Verify both stored correctly with proper paths","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:36.15785-05:00","updated_at":"2025-12-01T08:48:54.937877-05:00","closed_at":"2025-12-01T08:48:54.937877-05:00","labels":["crawler","tome"],"dependencies":[{"issue_id":"sensei-fdc","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.640039-05:00","created_by":"daemon"}]}
{"id":"sensei-fnh","title":"Add kura MCP server with stdio mode","description":"Create server.py with FastMCP wrapping cache functions (search_cache, get_cached_response). Add __main__.py for stdio entry point. Mirror Scout's server pattern.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.090543-05:00","updated_at":"2025-11-27T08:57:25.907756-05:00","closed_at":"2025-11-27T08:57:25.907756-05:00","dependencies":[{"issue_id":"sensei-fnh","depends_on_id":"sensei-bti","type":"blocks","created_at":"2025-11-27T08:55:36.496058-05:00","created_by":"daemon"}]}
{"id":"sensei-gc1","title":"Remove dead code from tools/common.py","description":"tools/common.py has unused functions that should be removed after exec_plan refactor:\n\nDead code:\n- format_entries (line 15) - never imported or used\n- get_client (line 44) - never imported or used  \n- DEFAULT_TIMEOUT (line 12) - only used by get_client\n\nKeep:\n- wrap_tool (line 52) - will be used after exec_plan refactor\n\nAfter exec_plan refactor is complete, audit again and remove any remaining dead code.","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-11-30T06:14:52.27395-05:00","updated_at":"2025-11-30T08:32:57.613046-05:00","closed_at":"2025-11-30T08:32:57.613046-05:00","labels":["cleanup"],"dependencies":[{"issue_id":"sensei-gc1","depends_on_id":"sensei-aky","type":"blocks","created_at":"2025-11-30T06:14:52.275468-05:00","created_by":"daemon"}]}
{"id":"sensei-gcu","title":"Update tests for section-based storage","description":"Update test_tome_service.py and add new tests for section-based functionality.\n\n**Update existing tests:**\n- Fixtures need to create Document + Sections instead of Documents with content\n- Search tests should verify heading_path in results\n\n**Add new tests:**\n\n**Chunker tests (test_tome_chunker.py):**\n- Small content returns single section\n- Large content splits by headings\n- Recursive splitting works at multiple levels\n- Raises error when content too large with no headings\n- Position ordering is correct\n- Parent relationships are correct\n\n**Service tests:**\n- `tome_toc` returns correct structure\n- `tome_get` without heading returns full doc\n- `tome_get` with heading returns subtree only\n- `tome_search` returns heading_path breadcrumb\n\n**Integration tests:**\n- Ingest real domain, verify sections created\n- Search returns sections with context\n- TOC structure matches document","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T09:57:24.666195-05:00","updated_at":"2025-12-01T10:25:32.187954-05:00","closed_at":"2025-12-01T10:25:32.187954-05:00","labels":["testing","tome"],"dependencies":[{"issue_id":"sensei-gcu","depends_on_id":"sensei-m7g","type":"blocks","created_at":"2025-12-01T09:57:24.669272-05:00","created_by":"daemon"},{"issue_id":"sensei-gcu","depends_on_id":"sensei-7gz","type":"blocks","created_at":"2025-12-01T09:57:24.670269-05:00","created_by":"daemon"},{"issue_id":"sensei-gcu","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.933641-05:00","created_by":"daemon"}]}
{"id":"sensei-gdv","title":"Add ingest_domain() public API","description":"Create async def ingest_domain(domain: str, max_depth: int = 3) -\u003e IngestResult in sensei/tome/__init__.py. This is the main entry point for crawling a domain's llms.txt.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:05.602081-05:00","updated_at":"2025-11-27T11:27:50.74934-05:00","closed_at":"2025-11-27T11:27:50.74934-05:00","dependencies":[{"issue_id":"sensei-gdv","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:05.603335-05:00","created_by":"daemon"},{"issue_id":"sensei-gdv","depends_on_id":"sensei-zii","type":"blocks","created_at":"2025-11-27T11:16:18.536978-05:00","created_by":"daemon"}]}
{"id":"sensei-gg1","title":"Remove unused documents_active view from migration","description":"**Current state:**\nMigration 004 creates a `documents_active` view that is never used:\n```python\n# alembic/versions/004_generation_based_crawls.py lines 77-80\nop.execute(\"\"\"\n    CREATE VIEW documents_active AS\n    SELECT * FROM documents WHERE generation_active = true\n\"\"\")\n```\n\n**Problem:** Dead code. All queries use `Document.generation_active == True` directly via SQLAlchemy ORM. The view adds no value and clutters the schema.\n\n**Fix:** Remove view creation from migration 004:\n\n1. Delete lines 77-80 (CREATE VIEW in upgrade)\n2. Delete line 96 (DROP VIEW in downgrade)\n3. Update docstring to remove mention of view\n\n**Note:** Do NOT create a new migration. We'll wipe the DB and start clean.\n\n**Why not use the view:**\n- SQLAlchemy ORM doesn't naturally map to views\n- `generation_active == True` filter is explicit and clear\n- Partial index `idx_documents_domain_active` already optimizes these queries\n- Keeping dead abstractions adds confusion","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-12-01T15:25:39.526155-05:00","updated_at":"2025-12-01T15:31:09.586521-05:00","closed_at":"2025-12-01T15:31:09.586521-05:00","labels":["cleanup","database","dead-code"]}
{"id":"sensei-ggd","title":"Deploy to Fly.io and verify","description":"Run fly deploy, verify app is running, test /health endpoint, test MCP endpoints, verify Scout can clone repos to /data volume.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.981935-05:00","updated_at":"2025-11-30T05:57:26.787413-05:00","closed_at":"2025-11-30T05:57:26.787413-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-ggd","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:18.052622-05:00","created_by":"daemon"},{"issue_id":"sensei-ggd","depends_on_id":"sensei-51u","type":"blocks","created_at":"2025-11-27T16:00:24.630788-05:00","created_by":"daemon"}]}
{"id":"sensei-gun","title":"Tome: Move UUID generation to storage layer","description":"crawler.py:72 caller generates UUID instead of storage layer defaulting it. Make id parameter optional with default in save_tome_document().","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:47.570845-05:00","updated_at":"2025-11-27T16:08:50.950692-05:00","closed_at":"2025-11-27T16:08:50.950692-05:00","labels":["minor","tome"]}
{"id":"sensei-gvh","title":"Update sensei/config.py - dynamic database_url","description":"Update config.py with dynamic database_url.\n\n**File:** `sensei/config.py`\n\n**Current:**\n```python\ndatabase_url: str = Field(\n    default=\"postgresql+asyncpg://sensei:sensei@localhost:5432/sensei\",\n    description=\"Database connection URL\",\n)\n```\n\n**Change to:**\n```python\nimport os\nfrom sensei.paths import get_local_database_url\n\nclass Settings(BaseSettings):\n    # ... other fields ...\n    \n    database_url: str = Field(\n        default_factory=get_local_database_url,\n        description=\"Database connection URL (defaults to local PostgreSQL via Unix socket)\",\n    )\n    \n    @property\n    def is_external_database(self) -\u003e bool:\n        \"\"\"Check if using an external (user-provided) database.\n        \n        Returns True if DATABASE_URL env var was explicitly set.\n        If external, sensei won't start PostgreSQL and won't run migrations\n        (user is responsible for their own DB).\n        \"\"\"\n        return os.environ.get(\"DATABASE_URL\") is not None\n```\n\n**Simplifications from review:**\n- No `sensei_home` field (paths.py handles it directly)\n- No `database_auto_migrate` field - simpler rule:\n  - Local DB → always migrate\n  - External DB → never migrate (user's responsibility)\n\n**Behavior:**\n\n| DATABASE_URL env var | is_external_database | Starts PG? | Migrates? |\n|---------------------|---------------------|------------|-----------|\n| Not set | False | Yes | Yes |\n| Set | True | No | No |","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-02T06:10:20.601992-05:00","updated_at":"2025-12-02T06:39:06.777839-05:00","dependencies":[{"issue_id":"sensei-gvh","depends_on_id":"sensei-4pt","type":"blocks","created_at":"2025-12-02T06:10:20.602946-05:00","created_by":"daemon"},{"issue_id":"sensei-gvh","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:10:20.605029-05:00","created_by":"daemon"}]}
{"id":"sensei-gwq","title":"Create Section model and migration","description":"Create the new Section table and modify Document table.\n\n**Section table:**\n- id (UUID, PK)\n- document_id (FK → Document, indexed)\n- parent_section_id (FK → Section, nullable)\n- heading (String, nullable)\n- level (Integer)\n- content (Text)\n- position (Integer)\n- search_vector (TSVECTOR, computed on content)\n- inserted_at, updated_at\n\n**Document table changes:**\n- Remove: content, search_vector columns\n- Keep: id, domain, url, path, content_hash, depth, timestamps\n\n**Migration:**\n- Create sections table with all columns and indexes\n- Add GIN index on search_vector\n- Alter documents table to drop content/search_vector\n- Add FK constraints","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:27.900632-05:00","updated_at":"2025-12-01T10:07:58.108514-05:00","closed_at":"2025-12-01T10:07:58.108514-05:00","labels":["database","migration","tome"],"dependencies":[{"issue_id":"sensei-gwq","depends_on_id":"sensei-edz","type":"blocks","created_at":"2025-12-01T09:56:27.903439-05:00","created_by":"daemon"}]}
{"id":"sensei-hkk","title":"save_query takes individual fields instead of domain model","description":"storage.py:53-64 save_query() takes 10 individual parameters while save_rating() correctly accepts the Rating domain model.\n\nThis violates: \"Pass models, not individual fields\"\n\nFix: Create QueryInput model in types.py and refactor save_query to accept it:\n```python\nasync def save_query(query: QueryInput) -\u003e None:\n    ...\n```\n\nFiles affected:\n- sensei/database/storage.py\n- sensei/types.py (add QueryInput model)\n- sensei/core.py (update call sites)","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-30T06:03:17.402667-05:00","updated_at":"2025-11-30T09:19:54.039708-05:00","closed_at":"2025-11-30T09:19:54.039708-05:00","labels":["architecture","cleanup"]}
{"id":"sensei-hon","title":"Tome: Return Success[IngestResult] | NoResults instead of raw IngestResult","description":"ingest_domain() returns IngestResult directly. Per CLAUDE.md Result Types, tools should return Success[T] | NoResults pattern.","notes":"System thinking review: Part of error boundary pattern. Should be done together with typed exceptions (sensei-rhx) as they're both about proper edge handling.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:43:46.263262-05:00","updated_at":"2025-11-27T13:42:04.653563-05:00","closed_at":"2025-11-27T13:42:04.653563-05:00","labels":["critical","tome"]}
{"id":"sensei-iid","title":"Update Dockerfile for Fly.io deployment","description":"Update existing Dockerfile: add gcc/g++ for numpy build, use PORT env var, ensure uv sync works correctly. Reference /data mount point for Scout cache.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.696172-05:00","updated_at":"2025-11-30T05:57:26.599434-05:00","closed_at":"2025-11-30T05:57:26.599434-05:00","labels":["deployment","fly.io"],"dependencies":[{"issue_id":"sensei-iid","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.892747-05:00","created_by":"daemon"},{"issue_id":"sensei-iid","depends_on_id":"sensei-8mm","type":"blocks","created_at":"2025-11-27T16:00:24.428165-05:00","created_by":"daemon"}]}
{"id":"sensei-ip4","title":"Add PostgreSQL full-text search to documents table","description":"Add FTS capability to the documents table for tome_search.\n\n**Changes:**\n1. Create Alembic migration that adds:\n   - `search_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED`\n   - GIN index: `CREATE INDEX idx_documents_search ON documents USING GIN(search_vector)`\n\n2. Update SQLAlchemy model in `sensei/database/models.py`:\n   - Add `search_vector` column (computed, so read-only in SQLAlchemy)\n\n**Why GENERATED ALWAYS AS STORED:**\n- Postgres auto-updates vector when content changes\n- No triggers needed\n- Zero application code to maintain it\n\n**Testing:**\n- Verify migration runs cleanly\n- Verify search_vector is populated for existing docs\n- Verify new docs get vector automatically","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:35.707324-05:00","updated_at":"2025-12-01T08:48:08.142603-05:00","closed_at":"2025-12-01T08:48:08.142603-05:00","labels":["database","fts","tome"],"dependencies":[{"issue_id":"sensei-ip4","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.605586-05:00","created_by":"daemon"}]}
{"id":"sensei-ipx","title":"Separate save_sections into explicit delete + insert operations","description":"**Current state (storage.py:243-324):**\n`save_sections` implicitly deletes all sections then re-inserts. The caller has no visibility into what's happening.\n\n```python\nasync def save_sections(document_id, sections):\n    async with AsyncSessionLocal() as session:\n        await session.execute(delete(Section)...)  # Implicit delete!\n        # ... complex recursive save logic\n```\n\n**Problem:**\n1. Implicit delete is surprising - function name suggests \"save\" not \"replace all\"\n2. Caller doesn't control the transaction boundary\n3. If insert fails after delete, data is lost\n\n**Fix:**\n- Keep `delete_sections_by_document(document_id)` - already exists at line 327\n- Create `insert_sections(document_id, sections: list[Section])` - simple bulk insert, no tree logic\n- Crawler explicitly calls both:\n```python\nawait delete_sections_by_document(doc_id)\nawait insert_sections(doc_id, flat_sections)\n```\n\nThis makes the replace-all behavior explicit and gives caller control.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:21.133062-05:00","updated_at":"2025-12-01T15:34:29.68655-05:00","closed_at":"2025-12-01T15:34:29.68655-05:00","dependencies":[{"issue_id":"sensei-ipx","depends_on_id":"sensei-10i","type":"blocks","created_at":"2025-12-01T13:01:31.187763-05:00","created_by":"daemon"}]}
{"id":"sensei-izi","title":"Set up Alembic for PostgreSQL migrations","description":"Replace init_db() with Alembic. PostgreSQL-only (drop SQLite). CLI-driven migrations, not at app startup.","design":"## Decisions\n- Migrations run via CLI only (avoids race conditions with Cloud Run replicas)\n- DATABASE_URL from environment variable (standard Alembic pattern)\n- Fresh start (data is disposable)\n- Full downgrade support (useful for branch switching)\n- Tests run migrations (no create_all shortcut)\n- PostgreSQL only (drop SQLite support)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T13:38:39.292935-05:00","updated_at":"2025-11-27T16:01:50.570063-05:00","closed_at":"2025-11-27T16:01:50.570063-05:00","labels":["database","migrations"]}
{"id":"sensei-ja0","title":"Tome: Clean up Crawlee storage directory after crawl","description":"Crawlee creates storage/ directory with request queues and state. No cleanup after crawl. Accumulates disk space and could interfere with subsequent crawls.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-11-27T11:44:40.144904-05:00","updated_at":"2025-11-27T16:08:51.011648-05:00","closed_at":"2025-11-27T16:08:51.011648-05:00","labels":["minor","tome"]}
{"id":"sensei-jmk","title":"Tome: Add content-type checking for responses","description":"Design doc mentions is_markdown_content() but not implemented. Could prevent crawling non-markdown responses (HTML, JSON). Consider adding content-type validation.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-11-27T11:44:46.408882-05:00","updated_at":"2025-11-27T16:10:14.644596-05:00","closed_at":"2025-11-27T16:10:14.644596-05:00","labels":["minor","tome"]}
{"id":"sensei-jwz","title":"Remove search_queries_fts alias and library/version params from search_queries","description":"storage.py:312 has `search_queries_fts = search_queries` as backwards-compat alias that should be removed.\n\nAdditionally, search_queries() should NOT accept library/version parameters - the cache search should be broad, letting the agent decide relevance from results.\n\nChanges needed:\n1. Remove the alias `search_queries_fts = search_queries` from storage.py:312\n2. Remove library/version params from search_queries() signature\n3. Update all call sites to use `search_queries` directly:\n   - core.py:12, 94, 155\n   - kura/tools.py:81\n   - sub_agent.py:12, 77\n4. Remove library/version from kura/tools.py search_cache() and kura/server.py search()\n\nThis supersedes sensei-rpo which only addressed the naming issue.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-30T06:12:30.344411-05:00","updated_at":"2025-11-30T07:53:07.226884-05:00","closed_at":"2025-11-30T07:53:07.226884-05:00","labels":["breaking-change","cleanup","database"]}
{"id":"sensei-k1i","title":"Mount kura MCP server in main Sensei app","description":"Add HTTP endpoint for kura MCP at /kura/mcp in sensei/__main__.py, similar to how Scout is mounted at /scout/mcp.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.197141-05:00","updated_at":"2025-11-27T08:58:06.405082-05:00","closed_at":"2025-11-27T08:58:06.405082-05:00","dependencies":[{"issue_id":"sensei-k1i","depends_on_id":"sensei-fnh","type":"blocks","created_at":"2025-11-27T08:55:36.561192-05:00","created_by":"daemon"}]}
{"id":"sensei-k7u","title":"Move search_sections_fts presentation logic to service layer","description":"**Current state (storage.py:460-572):**\n`search_sections_fts` mixes query logic with presentation/transformation:\n\n1. **Path normalization** (lines 514-515):\n```python\nprefix = path if path.startswith(\"/\") else f\"/{path}\"\n```\n\n2. **Snippet formatting** embedded in SQL (line 547-548):\n```python\nts_headline('english', fp.content, ..., 'MaxWords=50, MinWords=20, StartSel=**, StopSel=**')\n```\n\n3. **SearchResult construction** (lines 563-572):\n```python\nreturn [\n    SearchResult(\n        url=row.url,\n        path=row.path,\n        snippet=row.snippet,\n        rank=row.rank,\n        heading_path=row.heading_path or \"\",\n    )\n    for row in rows\n]\n```\n\n**Problem:** Storage layer should be a thin persistence layer. Path normalization and result transformation are business/presentation logic that belongs in the service layer.\n\n**Fix:**\n1. Storage returns raw row data (namedtuple or dataclass with url, path, content, rank, heading_path)\n2. Service layer handles:\n   - Path prefix normalization before calling storage\n   - Snippet generation (or pass snippet config to storage)\n   - SearchResult construction\n\n**Pattern to follow:** `get_sections_for_toc` returns raw tuples, `_build_toc_tree` in service.py does the transformation. Same pattern should apply here.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T15:19:45.516871-05:00","updated_at":"2025-12-01T15:39:06.149685-05:00","closed_at":"2025-12-01T15:39:06.149685-05:00","labels":["refactoring","service-layer","storage"]}
{"id":"sensei-kcp","title":"Extract crawler business logic from save_document_metadata","description":"**Current state (storage.py:186-240):**\nThe function does too much:\n1. Checks if document exists by URL\n2. Compares content hashes to decide if update needed\n3. Decides to skip/update/insert based on business logic\n4. Actually saves\n\n**Problem:** Storage layer is making decisions about crawl behavior. This couples storage to crawler logic.\n\n**New context:** With generation-based crawls (sensei-67v), we no longer need skip logic. Every crawl is a full re-process - we insert new documents with the new generation_id, then atomically swap.\n\n**Fix:** Storage becomes simple CRUD:\n- `insert_document(domain, url, path, content_hash, generation_id) -\u003e Document` - just insert, no upsert logic\n- Remove `save_document_metadata` entirely\n- Remove `SaveResult` enum (no longer needed)\n\nCrawler handles orchestration:\n```python\ngen_id = uuid4()\n# For each document:\ndoc = await insert_document(\n    domain=domain, url=url, path=path,\n    content_hash=hash_value, generation_id=gen_id\n)\nsections = flatten_section_tree(chunk_markdown(content), doc.id)\nawait insert_sections(sections)\n\n# After crawl completes:\nawait activate_generation(domain, gen_id)\nawait cleanup_old_generations(domain)\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:01:20.971057-05:00","updated_at":"2025-12-01T13:34:48.13617-05:00","closed_at":"2025-12-01T13:34:48.13617-05:00","dependencies":[{"issue_id":"sensei-kcp","depends_on_id":"sensei-67v","type":"related","created_at":"2025-12-01T13:23:12.191412-05:00","created_by":"daemon"}]}
{"id":"sensei-ke1","title":"Create initial migration with all 3 tables","description":"Create 001_initial_schema.py with queries, ratings, tome_documents tables. Include all constraints, indexes, foreign keys. Full downgrade support.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:38:55.319483-05:00","updated_at":"2025-11-27T16:01:50.401331-05:00","closed_at":"2025-11-27T16:01:50.401331-05:00","labels":["database","migrations"],"dependencies":[{"issue_id":"sensei-ke1","depends_on_id":"sensei-izi","type":"parent-child","created_at":"2025-11-27T13:39:07.894251-05:00","created_by":"daemon"},{"issue_id":"sensei-ke1","depends_on_id":"sensei-1ha","type":"blocks","created_at":"2025-11-27T13:39:08.212697-05:00","created_by":"daemon"},{"issue_id":"sensei-ke1","depends_on_id":"sensei-198","type":"blocks","created_at":"2025-11-27T13:39:08.264434-05:00","created_by":"daemon"}]}
{"id":"sensei-kt2","title":"Tome: Create Domain value object with normalization","description":"Create a Domain value object that normalizes on construction: strips protocol, removes www. prefix, removes default ports (80/443), lowercases. This upstream fix simplifies issues sensei-543 (subdomain), sensei-ddo (ports), sensei-225 (validation).","design":"```python\n@dataclass(frozen=True)\nclass Domain:\n    value: str\n    \n    def __post_init__(self):\n        normalized = self._normalize(self.value)\n        object.__setattr__(self, 'value', normalized)\n    \n    @staticmethod\n    def _normalize(raw: str) -\u003e str:\n        if \"://\" in raw:\n            raw = raw.split(\"://\", 1)[1]\n        raw = raw.split(\"/\")[0]\n        raw = raw.removeprefix(\"www.\")\n        raw = re.sub(r':(?:80|443)$', '', raw)\n        return raw.lower()\n```\n\nOnce this exists:\n- is_same_domain() becomes: `Domain(url1) == Domain(url2)`\n- ingest_domain() validates by constructing Domain\n- No scattered normalization logic","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-11-27T13:28:46.143648-05:00","updated_at":"2025-11-27T13:42:03.480346-05:00","closed_at":"2025-11-27T13:42:03.480346-05:00","labels":["architecture","critical","tome"]}
{"id":"sensei-kte","title":"Integrate tome server into Sensei agent","description":"Wire up the tome MCP server to the main Sensei agent.\n\n**Changes to `sensei/agent.py`:**\n\n1. Import tome server factory:\n```python\nfrom sensei.tools.tome import create_tome_server\n```\n\n2. Add to toolsets in `create_agent()`:\n```python\ntoolsets=[\n    create_context7_server(settings.context7_api_key),\n    create_tavily_server(settings.tavily_api_key),\n    create_scout_server(),\n    create_kura_server(),\n    create_tome_server(),  # NEW\n],\n```\n\n**Update prompts (optional):**\nConsider adding guidance to `sensei/prompts.py` about when to use tome vs context7:\n- **tome**: For domains you've explicitly ingested, want fresh/authoritative llms.txt content\n- **context7**: For general library lookups, unknown domains, semantic search\n\n**Testing:**\n- Verify agent can call tome_get and tome_search\n- Test with real ingested domain\n- Verify graceful handling when domain not ingested","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.998726-05:00","updated_at":"2025-12-01T08:56:20.092922-05:00","closed_at":"2025-12-01T08:56:20.092922-05:00","labels":["agent","integration","tome"],"dependencies":[{"issue_id":"sensei-kte","depends_on_id":"sensei-z1e","type":"blocks","created_at":"2025-12-01T08:38:58.878001-05:00","created_by":"daemon"},{"issue_id":"sensei-kte","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.811449-05:00","created_by":"daemon"}]}
{"id":"sensei-kvi","title":"Fix test_search_cache_tool test for search_queries signature change","description":"test_cache.py::test_search_cache_tool fails because it expects `search_queries(string)` but the function now takes `search_queries(list[str])`.\n\nThe mock assertion expects:\n```\nsearch_queries('React hooks', limit=5)\n```\n\nBut actual call is:\n```\nsearch_queries(['React', 'hooks'], limit=5)\n```\n\nUpdate the test to match the new signature.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-01T07:37:26.099678-05:00","updated_at":"2025-12-01T14:01:55.481507-05:00","closed_at":"2025-12-01T14:01:55.481507-05:00","labels":["bug","testing"]}
{"id":"sensei-l5w","title":"Global mutable state in exec_plan.py is not request-scoped","description":"tools/exec_plan.py:18 uses module-level mutable dict `_exec_plans: Dict[str, str] = {}` for storing ExecPlans.\n\nProblems:\n- Not thread-safe for concurrent requests\n- No cleanup mechanism if clear_plan() isn't called\n- State persists across requests in same process\n\nFix options:\n1. Use contextvars for request-scoped state\n2. Move to database or Redis for persistence\n3. Attach to Deps object which is per-request","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-30T06:03:17.301718-05:00","updated_at":"2025-11-30T09:13:01.94712-05:00","closed_at":"2025-11-30T09:13:01.94712-05:00","labels":["architecture","concurrency"]}
{"id":"sensei-ljd","title":"Write CLAUDE.md snippet for plugin README","description":"Create instructions for users to copy into their CLAUDE.md. Prioritize spawning sensei agent for ANY research task. Document MCP tools (scout, kura) as secondary/direct access options.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T08:55:29.317689-05:00","updated_at":"2025-11-27T08:58:45.077401-05:00","closed_at":"2025-11-27T08:58:45.077401-05:00"}
{"id":"sensei-ljg","title":"Add search_documents_fts() to storage layer","description":"Add PostgreSQL full-text search function to storage layer for tome_search.\n\n**Function signature:**\n```python\nasync def search_documents_fts(\n    domain: str,\n    query: str,\n    paths: list[str] | None = None,\n    limit: int = 10,\n) -\u003e list[SearchResult]\n```\n\n**Behavior:**\n- Use `plainto_tsquery()` or `websearch_to_tsquery()` for query parsing\n- Filter by domain (required)\n- Optionally filter by path prefixes (e.g., paths=[\"/hooks\"] matches \"/hooks/useState\")\n- Return ranked results with:\n  - url, path, title (first H1 or path)\n  - snippet (ts_headline for context)\n  - relevance score\n\n**SQL approach:**\n```sql\nSELECT url, path, \n       ts_headline('english', content, query) as snippet,\n       ts_rank(search_vector, query) as rank\nFROM documents\nWHERE domain = :domain\n  AND search_vector @@ websearch_to_tsquery('english', :query)\n  AND (path LIKE ANY(:path_patterns) OR :path_patterns IS NULL)\nORDER BY rank DESC\nLIMIT :limit\n```\n\n**Domain model:**\nAdd `SearchResult` to `sensei/types.py`:\n```python\nclass SearchResult(BaseModel):\n    url: str\n    path: str\n    snippet: str\n    rank: float\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T08:38:36.641793-05:00","updated_at":"2025-12-01T08:43:58.268791-05:00","closed_at":"2025-12-01T08:43:58.268791-05:00","labels":["fts","storage","tome"]}
{"id":"sensei-lnu","title":"Build and publish sensei to PyPI","description":"Use uv build and uv publish to publish package","notes":"Package built successfully. Waiting for PyPI account recovery to publish. Run `uv publish` when ready.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T06:15:01.305215-05:00","updated_at":"2025-11-27T06:33:16.918807-05:00","closed_at":"2025-11-27T06:33:16.918807-05:00","dependencies":[{"issue_id":"sensei-lnu","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:15:01.305847-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-u5g","type":"blocks","created_at":"2025-11-27T06:15:07.495691-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-3na","type":"blocks","created_at":"2025-11-27T06:15:07.520515-05:00","created_by":"daemon"},{"issue_id":"sensei-lnu","depends_on_id":"sensei-afa","type":"blocks","created_at":"2025-11-27T06:15:07.548945-05:00","created_by":"daemon"}]}
{"id":"sensei-lzn","title":"Stub out sensei/dojo/ module","description":"Create sensei/dojo/ with __init__.py and README.md describing the objective: DSPy prompt optimization using MIPROv2 with user feedback ratings.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T11:16:07.203856-05:00","updated_at":"2025-11-27T11:28:17.946563-05:00","closed_at":"2025-11-27T11:28:17.946563-05:00","dependencies":[{"issue_id":"sensei-lzn","depends_on_id":"sensei-9pf","type":"parent-child","created_at":"2025-11-27T11:16:07.205529-05:00","created_by":"daemon"}]}
{"id":"sensei-m7g","title":"Update tome service layer for section-based retrieval","description":"Update service.py with new section-based functions.\n\n**Update existing:**\n- `tome_get(domain, path)` - concatenate all sections by position\n- `tome_get(domain, path, heading)` - NEW param: get section subtree\n- `tome_search()` - use section search, return heading_path in results\n\n**Add new:**\n- `tome_toc(domain, path) -\u003e TOC` - return heading structure derived from sections\n\n**TOC structure:**\n```python\n@dataclass\nclass TOCEntry:\n    heading: str\n    level: int\n    children: list[TOCEntry]\n\ndef tome_toc(domain: str, path: str) -\u003e Success[list[TOCEntry]] | NoResults:\n    # Query sections, build tree from parent_section_id relationships\n```\n\n**tome_get with heading:**\n```python\nasync def tome_get(\n    domain: str, \n    path: str, \n    heading: str | None = None\n) -\u003e Success[str] | NoResults:\n    if heading:\n        # Find section, get subtree, concatenate\n        sections = await storage.get_section_subtree_by_heading(domain, path, heading)\n    else:\n        # Get all sections for document\n        sections = await storage.get_sections_by_document(domain, path)\n    \n    if not sections:\n        return NoResults()\n    \n    content = \"\\n\\n\".join(s.content for s in sections)\n    return Success(content)\n```","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:57:05.474973-05:00","updated_at":"2025-12-01T10:13:43.543094-05:00","closed_at":"2025-12-01T10:13:43.543094-05:00","labels":["service","tome"],"dependencies":[{"issue_id":"sensei-m7g","depends_on_id":"sensei-v52","type":"blocks","created_at":"2025-12-01T09:57:05.477867-05:00","created_by":"daemon"},{"issue_id":"sensei-m7g","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.844978-05:00","created_by":"daemon"}]}
{"id":"sensei-mci","title":"Make scout installable via uvx --from sensei scout","description":"Add script entry point to pyproject.toml and publish sensei to PyPI so users can run `uvx --from sensei scout` for stdio MCP","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T06:14:48.004237-05:00","updated_at":"2025-11-27T06:33:16.956555-05:00","closed_at":"2025-11-27T06:33:16.956555-05:00"}
{"id":"sensei-mwh","title":"Add --force flag to crawler to overwrite existing data","description":"**Context:**\nCurrently the crawler skips documents if content_hash matches (SaveResult.SKIPPED). This is good for incremental updates but sometimes we want to force a full re-crawl.\n\n**Need:**\nA `force: bool = False` parameter on `ingest_domain()` that:\n- When `force=True`: Skip the content_hash check, always overwrite document and sections\n- When `force=False` (default): Current behavior - skip unchanged documents\n\n**Use cases:**\n1. Debugging chunker changes - need to re-process all docs with new chunking logic\n2. Schema changes - need to re-save sections with new structure\n3. Testing - ensure fresh data\n\n**Implementation:**\nAfter sensei-kcp (extract business logic from storage), the crawler will have the skip logic. Add:\n```python\nif not force:\n    existing = await get_document_by_url(url)\n    if existing and existing.content_hash == content_hash:\n        result.documents_skipped += 1\n        return\n# Always proceed if force=True or content changed\n```\n\n**CLI:**\n```bash\nuv run python scripts/ingest_domain.py crawlee.dev --force\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-01T13:03:02.680725-05:00","updated_at":"2025-12-01T13:23:18.237716-05:00","closed_at":"2025-12-01T13:23:18.237716-05:00","dependencies":[{"issue_id":"sensei-mwh","depends_on_id":"sensei-kcp","type":"blocks","created_at":"2025-12-01T13:03:02.681854-05:00","created_by":"daemon"}]}
{"id":"sensei-n82","title":"Configure Scout to use /data volume for git cache","description":"Update Scout configuration to use /data directory for cloning and caching git repositories. This path will be mounted as a persistent Fly volume.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:00:09.854244-05:00","updated_at":"2025-11-30T05:57:26.690525-05:00","closed_at":"2025-11-30T05:57:26.690525-05:00","labels":["deployment","scout"],"dependencies":[{"issue_id":"sensei-n82","depends_on_id":"sensei-t0t","type":"parent-child","created_at":"2025-11-27T16:00:17.967425-05:00","created_by":"daemon"}]}
{"id":"sensei-nbu","title":"Extend link parsing to handle all markdown link formats","description":"`parse_llms_txt_links` only handles `[text](url)` style links.\n\nMissing formats in valid markdown:\n- Reference links: `[text][ref]` with `[ref]: url`\n- Auto-links: `\u003chttps://example.com\u003e`\n- Bare URLs (some llms.txt files use these)\n\nConsider using a proper markdown parser for link extraction or extending the regex patterns.","status":"closed","priority":3,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:36.027082-05:00","updated_at":"2025-12-01T07:48:02.813748-05:00","closed_at":"2025-12-01T07:48:02.813748-05:00","labels":["p3","parser","tome"]}
{"id":"sensei-nc7","title":"Pass DocumentContent model instead of individual fields to save_document","description":"Per CLAUDE.md guidelines: \"Pass models, not individual fields\"\n\nCurrent code in crawler.py:116-123 passes 6 individual fields to save_document(). This violates the domain model pattern.\n\nFix:\n1. Create `DocumentContent` Pydantic model in `types.py`\n2. Update `save_document()` signature to accept `DocumentContent`\n3. Update crawler to construct and pass the model","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:35.744653-05:00","updated_at":"2025-12-01T07:37:26.144902-05:00","closed_at":"2025-12-01T07:37:26.144902-05:00","labels":["architecture","p1","tome"]}
{"id":"sensei-nd9","title":"Fix Crawlee storage race condition in integration tests","description":"Integration tests for tome crawler fail due to Crawlee storage race condition.\n\n**Problem:**\nWhen running multiple integration tests sequentially, Crawlee's file-based storage gets into a bad state:\n1. First test runs and creates `storage/` directory\n2. Cleanup runs after first test\n3. Second test starts but Crawlee's internal state still references deleted files\n4. `FileNotFoundError` when trying to write to deleted directory\n\n**Error:**\n```\nFileNotFoundError: [Errno 2] No such file or directory: \n'/Users/alizain/Experiments/sensei/storage/request_queues/default/...json.tmp'\n```\n\n**Solution options:**\n1. Use `MemoryStorageClient` instead of file-based storage for testing\n2. Configure unique storage directories per test\n3. Use Crawlee's `purge_request_queue=True` properly\n\n**References:**\n- https://crawlee.dev/python/docs/guides/storage-clients.md\n- Crawlee Python v1 API uses `storage_client` parameter in crawler init\n\n**Files:**\n- `sensei/tome/crawler.py` - crawler implementation\n- `tests/test_tome_service.py` - integration tests","notes":"Created as blocker for second integration test. First integration test (test_ingest_real_domain_llmstxt_org) passes.","status":"closed","priority":1,"issue_type":"bug","assignee":"claude","created_at":"2025-12-01T09:03:06.503042-05:00","updated_at":"2025-12-01T09:31:13.353195-05:00","closed_at":"2025-12-01T09:31:13.353195-05:00","labels":["crawlee","testing","tome"]}
{"id":"sensei-neo","title":"Update agent.py to use Kura via MCP toolset","description":"Replace direct cache function imports (search_cache, get_cached_response) with create_kura_server() in toolsets. Remove cache tools from tools=[] list. This makes Kura consistent with Scout - both accessed via MCP.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.683223-05:00","updated_at":"2025-11-27T09:06:08.470345-05:00","closed_at":"2025-11-27T09:06:08.470345-05:00","dependencies":[{"issue_id":"sensei-neo","depends_on_id":"sensei-x4g","type":"blocks","created_at":"2025-11-27T09:03:51.319716-05:00","created_by":"daemon"}]}
{"id":"sensei-nym","title":"Build Tome: Documentation repository from llms.txt","description":"Canonical documentation repository built by crawling llms.txt files. Uses Crawlee HttpCrawler for orchestration (queue, dedup, retries, depth control) with plain httpx fetch. No Playwright needed since llms.txt and linked files are markdown.","design":"## Stack\n- Crawlee HttpCrawler for orchestration\n- httpx under the hood (no browser)\n- Sensei's existing database\n\n## Core function\n```python\nasync def ingest_domain(domain: str, max_depth: int = 3) -\u003e IngestResult\n```\n\n## Flow\n1. Fetch https://{domain}/llms.txt\n2. Parse markdown, extract links\n3. Enqueue same-domain links (respect depth)\n4. Store in tome_documents table\n\n## Why Crawlee HttpCrawler\n- Automatic RequestQueue with deduplication\n- Visited URL tracking\n- Retry logic with exponential backoff\n- Concurrency control\n- State persistence (resume interrupted crawls)\n- No parsing overhead (we're fetching markdown)\n\n## What we implement\n- parse_llms_txt_links() - extract URLs from llms.txt\n- is_same_domain() - domain filtering\n- is_markdown_content() - content type check\n- Storage layer integration","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T11:12:59.092712-05:00","updated_at":"2025-11-27T11:27:57.277048-05:00","closed_at":"2025-11-27T11:27:57.277048-05:00"}
{"id":"sensei-on8","title":"Create packages/www marketing site with React Router v7 Framework Mode","description":"Single-page product landing page for Sensei using React Router v7 Framework Mode, TailwindCSS, and React. Minimal/clean design, CTA links to GitHub README for setup instructions.","design":"## Technical Stack\n- React Router v7 Framework Mode (Vite-based)\n- TailwindCSS for styling\n- TypeScript\n- SSR enabled for SEO\n\n## Package Structure\npackages/www/\n├── package.json\n├── react-router.config.ts\n├── vite.config.ts\n├── tsconfig.json\n├── tailwind.config.ts\n├── src/\n│   ├── root.tsx (HTML shell)\n│   ├── routes.ts (route definitions)\n│   ├── routes/\n│   │   └── home.tsx (landing page)\n│   └── app.css (Tailwind imports)\n\n## Integration\n- Integrates with existing turbo monorepo\n- Uses npm workspaces (packages/www)\n- Follows existing biome/typescript conventions","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-11-27T13:22:14.763677-05:00","updated_at":"2025-11-27T16:14:31.647865-05:00","closed_at":"2025-11-27T16:14:31.647865-05:00"}
{"id":"sensei-ori","title":"Re-enable aider-chat and repo_map functionality","description":"aider-chat was disabled due to dependency conflict with pydantic-ai:\n- aider-chat pins openai==1.99.1\n- pydantic-ai requires openai\u003e=1.107.2\n\nWhen aider-chat updates to support openai\u003e=1.107.2, uncomment:\n1. pyproject.toml - aider-chat dependency (line 17)\n2. sensei/scout/repomap.py - entire file\n3. sensei/scout/server.py - repo_map tool and imports (lines 28-92)\n4. sensei/scout/__init__.py - repomap exports (lines 41-42, 63-65)\n5. tests/test_scout.py - TestRepoMap class (lines 448-488)","status":"open","priority":3,"issue_type":"chore","created_at":"2025-11-30T10:35:49.916993-05:00","updated_at":"2025-11-30T10:35:49.916993-05:00","labels":["blocked-upstream","dependencies"]}
{"id":"sensei-psb","title":"Add integration tests for tome crawler","description":"Only `test_tome_parser.py` exists - tests parsing logic but not:\n- The crawler behavior with mocked HTTP responses\n- Database integration (save_document flow)\n- Error scenarios (network failures, invalid content)\n\nAdd integration tests using httpx mock or similar to test the full crawl flow.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-01T07:30:36.182239-05:00","updated_at":"2025-12-01T07:30:36.182239-05:00","labels":["p3","testing","tome"],"dependencies":[{"issue_id":"sensei-psb","depends_on_id":"sensei-3ao","type":"related","created_at":"2025-12-01T07:30:44.050631-05:00","created_by":"daemon"}]}
{"id":"sensei-qm8","title":"Convert IngestResult from dataclass to Pydantic model","description":"IngestResult is currently a dataclass while the rest of the codebase uses Pydantic models. This breaks consistency and loses validation benefits.\n\nConvert to Pydantic BaseModel for consistency with other domain models.","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T07:30:35.813873-05:00","updated_at":"2025-12-01T07:39:27.481527-05:00","closed_at":"2025-12-01T07:39:27.481527-05:00","labels":["consistency","p1","tome"]}
{"id":"sensei-qow","title":"Add Tome storage functions","description":"Add storage functions to sensei/database/storage.py: save_tome_document(), get_tome_document_by_url(), search_tome_documents(), delete_tome_documents_by_domain(). Upsert logic based on url, update if content_hash changed.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:02.359116-05:00","updated_at":"2025-11-27T11:26:50.761285-05:00","closed_at":"2025-11-27T11:26:50.761285-05:00","dependencies":[{"issue_id":"sensei-qow","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:02.361933-05:00","created_by":"daemon"},{"issue_id":"sensei-qow","depends_on_id":"sensei-3c9","type":"blocks","created_at":"2025-11-27T11:16:18.290789-05:00","created_by":"daemon"}]}
{"id":"sensei-r0g","title":"Add retry logic to tome crawler","description":"A single network hiccup currently fails the entire crawl. Crawlee supports request retries but they're not configured.\n\nFix: Configure `max_request_retries=3` (or similar) in HttpCrawler initialization.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-01T07:30:35.883197-05:00","updated_at":"2025-12-01T07:30:35.883197-05:00","labels":["p2","robustness","tome"]}
{"id":"sensei-r4h","title":"Handle PostgreSQL tsvector size limit for large documents","description":"PostgreSQL's tsvector has a hard limit of ~1MB (1048575 bytes). Large llms-full.txt files (e.g., crawlee.dev/python at 1.1MB) exceed this limit, causing errors:\n\n```\nERROR: string is too long for tsvector (1111118 bytes, max 1048575 bytes)\n```\n\n**Current behavior:** The error is silently ignored somewhere, but documents aren't being saved properly.\n\n**Solution:** Truncate content for FTS indexing while keeping full content for retrieval. Change the computed column to truncate at 1MB:\n\n```sql\nto_tsvector('english', left(content, 1000000))\n```\n\nThis ensures:\n- Full content is stored and retrievable via tome_get\n- Search works on first ~1MB of content (sufficient for most use cases)\n- No errors on large documents","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-01T09:32:06.018975-05:00","updated_at":"2025-12-01T09:56:06.984668-05:00","closed_at":"2025-12-01T09:56:06.984668-05:00","labels":["database","fts","p0","tome"]}
{"id":"sensei-rhx","title":"Tome: Use typed exceptions instead of generic Exception","description":"crawler.py:100-101 catches generic Exception and appends string to errors list. Should use TransientError for network issues, BrokenInvariant for config issues per CLAUDE.md.","notes":"System thinking review: Part of error boundary confusion. Errors should be typed internally, only stringified at API edge. Consider grouping with Result type fix (sensei-hon).","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-11-27T11:43:44.722869-05:00","updated_at":"2025-11-27T13:42:03.880413-05:00","closed_at":"2025-11-27T13:42:03.880413-05:00","labels":["critical","tome"]}
{"id":"sensei-rpo","title":"Resolve search_queries_fts usage - remove or rename consistently","description":"Multiple files import search_queries_fts from storage.py. Instead of adding a backwards-compat alias, decide: rename all usages to search_queries, or keep search_queries_fts as the canonical name. Files affected: sensei/core.py, sensei/kura/tools.py, sensei/tools/sub_agent.py, tests/test_cache.py","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T16:10:31.621192-05:00","updated_at":"2025-11-30T06:17:58.10903-05:00","closed_at":"2025-11-30T06:17:58.10903-05:00","labels":["cleanup","database"]}
{"id":"sensei-sop","title":"Update README with PostgreSQL installation instructions","description":"Update documentation to explain the PostgreSQL requirement.\n\n**File:** `README.md`\n\n**Content to add:**\n\n### Prerequisites section\n```markdown\n## Prerequisites\n\n### PostgreSQL 17+\n\nSensei requires PostgreSQL to be installed on your system. Install it using your package manager:\n\n**macOS:**\n```bash\nbrew install postgresql@17\n```\n\n**Ubuntu/Debian:**\n```bash\nsudo apt install postgresql-17\n```\n\n**Windows:**\nDownload from https://www.postgresql.org/download/windows/\n\n**Note:** You don't need to configure PostgreSQL or create databases manually. Sensei automatically manages a local PostgreSQL instance in `~/.sensei/pgdata/` using a Unix socket (no port conflicts with system PostgreSQL).\n```\n\n### For development\n```markdown\n## Development Setup\n\nSet `SENSEI_HOME=.sensei` in your `.env` file to keep development data local to the repo instead of `~/.sensei/`.\n```\n\n**No CLI commands section** - Sensei auto-manages PostgreSQL, no manual commands needed.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-02T06:11:21.329657-05:00","updated_at":"2025-12-02T07:05:28.728215-05:00","dependencies":[{"issue_id":"sensei-sop","depends_on_id":"sensei-4wa","type":"parent-child","created_at":"2025-12-02T06:11:21.331393-05:00","created_by":"daemon"},{"issue_id":"sensei-sop","depends_on_id":"sensei-6ul","type":"blocks","created_at":"2025-12-02T06:11:34.975338-05:00","created_by":"daemon"}]}
{"id":"sensei-sp6","title":"Simplify get_sections_by_document with a JOIN","description":"**Current state (storage.py:342-371):**\nTwo separate queries:\n```python\nasync def get_sections_by_document(domain, path):\n    # Query 1: Find document\n    doc_result = await session.execute(\n        select(Document).where(Document.domain == domain, Document.path == path)\n    )\n    doc = doc_result.scalar_one_or_none()\n    if not doc:\n        return []\n    # Query 2: Get sections\n    result = await session.execute(\n        select(Section).where(Section.document_id == doc.id).order_by(Section.position)\n    )\n```\n\n**Problem:** Two queries when one JOIN would do.\n\n**Fix:**\n```python\nasync def get_sections_by_document(domain: str, path: str) -\u003e list[Section]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(Section)\n            .join(Document, Document.id == Section.document_id)\n            .where(Document.domain == domain, Document.path == path)\n            .order_by(Section.position)\n        )\n        return list(result.scalars().all())\n```\n\nSingle query, same result. Also simpler to read.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-01T13:01:21.405961-05:00","updated_at":"2025-12-01T15:35:00.784237-05:00","closed_at":"2025-12-01T15:35:00.784237-05:00"}
{"id":"sensei-t0t","title":"Deploy Sensei to Fly.io with persistent volume","description":"Deploy Sensei HTTP server to Fly.io with Neon Postgres and persistent volume for Scout's git repo caching. Replaces Cloud Run approach due to filesystem persistence requirements.","design":"## Why Fly.io over Cloud Run?\n- Scout needs persistent filesystem for git repo caching\n- Cloud Run filesystem is ephemeral (resets on restart/scale)\n- Fly.io volumes provide real persistent disk at $0.15/GB/month\n\n## Architecture\n- Single Fly Machine with 1GB RAM\n- 10GB persistent volume mounted at /data for Scout cache\n- Neon Postgres for database (already configured)\n- Stateless MCP endpoints for horizontal scaling potential\n\n## Components\n- fly.toml configuration\n- Dockerfile (already exists, needs gcc for numpy)\n- Volume mount for /data\n- Environment secrets for API keys and DATABASE_URL","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-11-27T15:59:54.321949-05:00","updated_at":"2025-11-30T05:57:26.462284-05:00","closed_at":"2025-11-30T05:57:26.462284-05:00","labels":["deployment","fly.io"]}
{"id":"sensei-u5g","title":"Remove hardcoded API keys from config.py","description":"Replace hardcoded API keys with empty defaults before publishing to PyPI - SECURITY CRITICAL","status":"closed","priority":0,"issue_type":"task","created_at":"2025-11-27T06:15:01.270024-05:00","updated_at":"2025-11-27T06:17:25.346124-05:00","closed_at":"2025-11-27T06:17:25.346124-05:00","dependencies":[{"issue_id":"sensei-u5g","depends_on_id":"sensei-mci","type":"parent-child","created_at":"2025-11-27T06:15:01.270764-05:00","created_by":"daemon"}]}
{"id":"sensei-u6r","title":"Add rate limiting / 429 handling to tome crawler","description":"The crawler doesn't handle HTTP 429 (Too Many Requests) or respect `Retry-After` headers. Some documentation sites will throttle aggressive crawlers.\n\nAdd request rate limiting or handle 429 responses with exponential backoff.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-01T07:30:36.103623-05:00","updated_at":"2025-12-01T07:30:36.103623-05:00","labels":["p3","politeness","tome"]}
{"id":"sensei-v52","title":"Update storage layer for Section-based model","description":"Update storage.py to work with the new Section-based model.\n\n**Remove/modify:**\n- `save_document()` - no longer saves content, just metadata\n- `get_document_by_url()` - keep for metadata lookup\n- `search_documents_fts()` - replace with section-based search\n\n**Add:**\n- `save_sections(document_id: UUID, sections: list[SectionData]) -\u003e None` - bulk insert sections\n- `get_sections_by_document(document_id: UUID) -\u003e list[Section]` - ordered by position\n- `get_section_subtree(section_id: UUID) -\u003e list[Section]` - recursive CTE for heading param\n- `search_sections_fts(domain, query, paths, limit) -\u003e list[SearchResult]` - search with heading_path\n- `get_heading_path(section_id: UUID) -\u003e str` - derive breadcrumb via recursive CTE\n\n**SearchResult update:**\n- Add `heading_path: str` field for breadcrumb context","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-01T09:56:46.544518-05:00","updated_at":"2025-12-01T10:12:45.227035-05:00","closed_at":"2025-12-01T10:12:45.227035-05:00","labels":["database","storage","tome"],"dependencies":[{"issue_id":"sensei-v52","depends_on_id":"sensei-gwq","type":"blocks","created_at":"2025-12-01T09:56:46.546785-05:00","created_by":"daemon"},{"issue_id":"sensei-v52","depends_on_id":"sensei-edz","type":"parent-child","created_at":"2025-12-01T09:57:33.759238-05:00","created_by":"daemon"}]}
{"id":"sensei-v7g","title":"Add tests for tome service and FTS","description":"Comprehensive tests for the new tome functionality.\n\n**New test file:** `tests/test_tome_service.py`\n\n**Test cases:**\n\n### tome_get tests\n- `test_tome_get_index_returns_llms_txt` - INDEX sentinel\n- `test_tome_get_full_returns_llms_full_txt` - FULL sentinel  \n- `test_tome_get_specific_path` - regular path lookup\n- `test_tome_get_not_found_returns_no_results` - missing doc\n- `test_tome_get_domain_not_ingested` - domain never crawled\n\n### tome_search tests\n- `test_tome_search_finds_matching_content` - basic FTS\n- `test_tome_search_respects_path_filter` - paths=[\"/hooks\"] works\n- `test_tome_search_empty_paths_searches_all` - paths=[] searches everything\n- `test_tome_search_ranks_by_relevance` - better matches first\n- `test_tome_search_returns_snippets` - ts_headline works\n- `test_tome_search_no_results` - query matches nothing\n\n### Integration tests\n- `test_ingest_then_search` - full flow: ingest domain, search it\n- `test_ingest_fetches_both_index_and_full` - crawler change works\n\n**Fixtures needed:**\n- Test documents with known content for FTS testing\n- Mock HTTP responses for crawler tests","status":"closed","priority":2,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:38.496488-05:00","updated_at":"2025-12-01T09:03:06.564384-05:00","closed_at":"2025-12-01T09:03:06.564384-05:00","labels":["testing","tome"],"dependencies":[{"issue_id":"sensei-v7g","depends_on_id":"sensei-8qn","type":"blocks","created_at":"2025-12-01T08:38:58.919769-05:00","created_by":"daemon"},{"issue_id":"sensei-v7g","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.856582-05:00","created_by":"daemon"}]}
{"id":"sensei-vae","title":"Add error handling tests for tome service","description":"The tome service layer needs tests for error conditions.\n\n**Error cases to test:**\n- `tome_search` with empty query raises `ToolError`\n- `tome_search` with whitespace-only query raises `ToolError`\n- `tome_get` with invalid domain format\n- Database connection errors (mock storage layer failures)\n\n**Note:** Some of these are already in `test_tome_service.py` but should verify the full error chain.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-01T09:04:15.939836-05:00","updated_at":"2025-12-01T09:04:15.939836-05:00","labels":["testing","tome"]}
{"id":"sensei-vrp","title":"Refactor core.py and sub_agent.py to reduce duplication","description":"Comprehensive cleanup of core/sub-agent code:\n1. Extract shared query preparation in core.py (_prepare_query, _save_query_result)\n2. Unify agent creation in agent.py with create_agent() factory\n3. Simplify sub_agent.py to use shared agent factory\n4. Standardize import style (module imports)","design":"## Design\n\n### 1. Extract shared query preparation (core.py)\n- `_prepare_query()` - builds enhanced query and deps with cache hits\n- `_save_query_result()` - saves query result to storage\n\n### 2. Unify agent creation (agent.py)\n- `create_agent(include_spawn, include_exec_plan)` - configurable factory\n- `create_sub_agent()` - wrapper for sub-agents (no spawn, no exec_plan)\n- Same model (gemini) for all agents\n\n### 3. Simplify sub_agent.py\n- Remove agent creation (use agent.py factory)\n- Remove grok model setup\n- Standardize imports to module style\n\n### 4. Import standardization\n- Use `from sensei.database import storage` pattern\n- Call as `storage.search_queries()` etc.\n\n## Acceptance\n- No duplicate query preparation logic in core.py\n- Single agent factory in agent.py\n- sub_agent.py uses shared factory\n- Consistent import style across files\n- All tests pass","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-30T09:44:29.41995-05:00","updated_at":"2025-11-30T09:49:35.975193-05:00","closed_at":"2025-11-30T09:49:35.975193-05:00"}
{"id":"sensei-vxc","title":"Remove hardcoded API keys from config.py","description":"Replace hardcoded API keys with empty defaults before publishing to PyPI","status":"closed","priority":0,"issue_type":"task","created_at":"2025-11-27T06:14:56.372621-05:00","updated_at":"2025-11-27T06:15:23.094068-05:00","closed_at":"2025-11-27T06:15:23.094068-05:00"}
{"id":"sensei-x4g","title":"Create create_kura_server() for PydanticAI integration","description":"Create sensei/tools/kura.py with create_kura_server() function that returns MCPServerStreamableHTTP pointing to /kura/mcp with tool_prefix=\"kura\". Mirror the pattern from create_scout_server().","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:03:43.636612-05:00","updated_at":"2025-11-27T09:04:39.288012-05:00","closed_at":"2025-11-27T09:04:39.288012-05:00"}
{"id":"sensei-x5d","title":"Add MCP server tests for tome tools","description":"The tome MCP server (`sensei/tome/server.py`) has no direct tests.\n\n**What to test:**\n- `tome.get()` tool - verify it calls service layer correctly, formats output\n- `tome.search()` tool - verify result formatting with `_format_search_results()`\n- `tome.ingest()` tool - verify it calls crawler and formats summary\n\n**Pattern to follow:**\nLook at how `tests/test_kura.py` or similar tests the kura MCP server tools.\n\n**Files:**\n- `sensei/tome/server.py` - server to test\n- `tests/test_tome_server.py` - new test file to create","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-01T09:04:15.815641-05:00","updated_at":"2025-12-01T09:04:15.815641-05:00","labels":["mcp","testing","tome"]}
{"id":"sensei-xnl","title":"Create TypeScript configuration","description":"Set up tsconfig.json for the www package","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T13:22:28.211914-05:00","updated_at":"2025-11-27T13:27:48.524646-05:00","closed_at":"2025-11-27T13:27:48.524646-05:00","dependencies":[{"issue_id":"sensei-xnl","depends_on_id":"sensei-on8","type":"parent-child","created_at":"2025-11-27T13:22:28.214195-05:00","created_by":"daemon"},{"issue_id":"sensei-xnl","depends_on_id":"sensei-apn","type":"blocks","created_at":"2025-11-27T13:22:35.47179-05:00","created_by":"daemon"}]}
{"id":"sensei-yeq","title":"Update Kura documentation from SQLite to PostgreSQL","description":"kura/__init__.py:7 still references \"SQLite database\" but code uses PostgreSQL:\n```python\n\"\"\"All tools connect to the same SQLite database as the main Sensei app.\"\"\"\n```\nShould say PostgreSQL.","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-11-30T10:05:21.494993-05:00","updated_at":"2025-11-30T10:12:40.272084-05:00","closed_at":"2025-11-30T10:12:40.272084-05:00"}
{"id":"sensei-ynz","title":"Create scripts/ingest_domain.py CLI script for crawling","description":"**Context:**\nCurrently, to ingest a domain we need to write Python code inline:\n```python\nDATABASE_URL=\"...\" uv run python -c \"\nimport asyncio\nfrom sensei.tome.crawler import ingest_domain\nasyncio.run(ingest_domain('crawlee.dev'))\n\"\n```\n\n**Need:**\nA proper CLI script in `scripts/ingest_domain.py` that:\n- Takes domain as argument\n- Optionally takes max_depth\n- Handles DATABASE_URL from environment or .env\n- Prints progress and results nicely\n\n**Usage:**\n```bash\nuv run python scripts/ingest_domain.py crawlee.dev\nuv run python scripts/ingest_domain.py crawlee.dev --max-depth 2\nuv run python scripts/ingest_domain.py crawlee.dev --force  # see sensei-xxx for force flag\n```\n\n**Existing scripts for reference:**\n- `scripts/build_plugin.py`\n- `scripts/dump_full_request.py`","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-01T13:03:02.631597-05:00","updated_at":"2025-12-01T13:03:02.631597-05:00","dependencies":[{"issue_id":"sensei-ynz","depends_on_id":"sensei-mwh","type":"related","created_at":"2025-12-01T13:03:07.534536-05:00","created_by":"daemon"}]}
{"id":"sensei-z1e","title":"Create tome MCP server exposing tools to agent","description":"Expose tome_get and tome_search as MCP tools for the Sensei agent.\n\n**New file:** `sensei/tome/server.py` (following pattern from kura/server.py, scout/server.py)\n\n**Tools to expose:**\n\n### `tome_get`\n```python\n@server.tool()\nasync def tome_get(domain: str, path: str) -\u003e str:\n    \"\"\"Get documentation from an ingested llms.txt domain.\n    \n    Args:\n        domain: The domain to fetch from (e.g., \"react.dev\")\n        path: Document path, or sentinel values:\n              - \"INDEX\" for /llms.txt (table of contents)\n              - \"FULL\" for /llms-full.txt (complete docs)\n              - Any path like \"/hooks/useState\" for specific doc\n    \n    Returns:\n        Document content as markdown, or error if not found.\n    \"\"\"\n```\n\n### `tome_search`\n```python\n@server.tool()\nasync def tome_search(domain: str, query: str, paths: list[str]) -\u003e str:\n    \"\"\"Search documentation within an ingested llms.txt domain.\n    \n    Args:\n        domain: The domain to search (e.g., \"react.dev\")\n        query: Search query (supports natural language)\n        paths: Path prefixes to search within (empty = all paths)\n               e.g., [\"/hooks\"] searches only /hooks/* docs\n    \n    Returns:\n        Matching snippets with source paths and relevance ranking.\n    \"\"\"\n```\n\n### `tome_ingest` (optional - for on-demand ingestion)\n```python\n@server.tool()\nasync def tome_ingest(domain: str) -\u003e str:\n    \"\"\"Ingest a domain's llms.txt documentation.\n    \n    Fetches /llms.txt, /llms-full.txt (if available), and all linked docs.\n    \"\"\"\n```\n\n**Integration:**\n- Add to agent.py toolsets list\n- Create `create_tome_server()` factory function\n- Follow existing patterns from kura/scout","status":"closed","priority":1,"issue_type":"task","assignee":"claude","created_at":"2025-12-01T08:38:37.50445-05:00","updated_at":"2025-12-01T08:54:40.448191-05:00","closed_at":"2025-12-01T08:54:40.448191-05:00","labels":["mcp","tome","tools"],"dependencies":[{"issue_id":"sensei-z1e","depends_on_id":"sensei-8qn","type":"blocks","created_at":"2025-12-01T08:38:58.836118-05:00","created_by":"daemon"},{"issue_id":"sensei-z1e","depends_on_id":"sensei-08s","type":"parent-child","created_at":"2025-12-01T08:44:08.767202-05:00","created_by":"daemon"}]}
{"id":"sensei-z8x","title":"Setup Neon Postgres database","description":"Create Neon project in us-east-1, get connection string for DATABASE_URL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T09:00:39.848645-05:00","updated_at":"2025-11-27T10:20:12.28555-05:00","closed_at":"2025-11-27T10:20:12.28555-05:00","dependencies":[{"issue_id":"sensei-z8x","depends_on_id":"sensei-4ok","type":"parent-child","created_at":"2025-11-27T09:00:39.849367-05:00","created_by":"daemon"}]}
{"id":"sensei-zii","title":"Implement TomeCrawler with Crawlee HttpCrawler","description":"Create TomeCrawler in sensei/tome/crawler.py. Subclass or use Crawlee HttpCrawler. Handle: fetch llms.txt, parse links, enqueue same-domain markdown URLs, respect depth limit, store via storage layer.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-27T11:16:04.783373-05:00","updated_at":"2025-11-27T11:27:39.631156-05:00","closed_at":"2025-11-27T11:27:39.631156-05:00","dependencies":[{"issue_id":"sensei-zii","depends_on_id":"sensei-nym","type":"parent-child","created_at":"2025-11-27T11:16:04.784816-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-au8","type":"blocks","created_at":"2025-11-27T11:16:18.364351-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-07q","type":"blocks","created_at":"2025-11-27T11:16:18.406751-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-qow","type":"blocks","created_at":"2025-11-27T11:16:18.449108-05:00","created_by":"daemon"},{"issue_id":"sensei-zii","depends_on_id":"sensei-9v0","type":"blocks","created_at":"2025-11-27T11:16:18.491868-05:00","created_by":"daemon"}]}
